% !TEX root = ../thesis.tex
\chapter{Computational Background}\label{chap:nlp}

\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

In this chapter I introduce related work in Natural Language Processing (NLP) and theoretical background on the machine learning methods that I use throughout this dissertation.

\zw{Add a few words about how word embeddings are trained.}
% TODO Introduce modelling
% TODO Introduce LIWC dictionary in depth
% TODO Introduce Linear models: SVM, LR
% TODO Neural Networks: Single-task + MTL
% TODO LSTM (and recurrence)
% TODO MLP
% TODO CNN and pooling
% TODO Introduce vectorisation & tensorisation
% TODO Introduce byte-pair encoding
% TODO Introduce Dropout
% TODO Introduce optimisaton:
% TODO L1, L2, and elasticnet
% TODO Softmax, Losses (NLLL), Optimisers (SGD, ADAM, ADAMW, ASGD)
% TODO Hyper parameter tuning
% TODO Metrics: precision, recall, F1, accuracy
% TODO Fairness work
% TODO Limitations

\section{Abusive Language Detection}

% Introduce the field of abusive language detection
In recent years, the computational study of online abuse has seen a rapid increase in the number of papers dedicated starting with a handful of papers prior to $2016$ to a thriving research field with numerous papers, shared tasks, and workshops \citep{Vidgen:2020}. In spite of the growth in research dedicated to the detection of online abuse, the research field is still in its infancy with a number of open questions, including questions around definition of the task, annotation guidelines, and modelling techniques.
The earliest work in the field sought to address questions of cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with sparing focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified abuse has surfaced as an independent task \citep{Tulkens:2015,Waseem:2016,Waseem-Hovy:2016,Park:2017,Samghabadi:2017,Karan:2018,Gorrell:2018,Stoop:2019,Meyer:2019,Palmer:2020,Vidgen:2020}. As a consequence of increased visibility of hate speech and abuse on online platforms, the academic inquiry into the computational detection has grown along with the regulatory responses \citep{Regulatory stuff: NetzDG, EUcommision on hate speech}.

Early, and contemporary computational work, has seen a great deal of focus to central questions around the task: how do we annotate and create datasets \citep{Waseem-Hovy:2016,Waseem:2017,Vidgen:2020} and understanding annotator interaction and performance \citep{Ross:2016,Waseem:2016,Vidgen:dynabench:2021}.
Early work focused on questions of marginalisation and oppression, for instance through the work of \citet{Waseem-Hovy:2016} who base their annotation on works in gender studies and critical race theory, and collect data based on gendered and racialised abuse; more recently data collection and annotation processes have moved towards a demographically blind process. Such early work was inspired by the marginalisation of certain bodies and the desire to develop computational tools to protect marginalised people \citep{Warner:2012}.marginalisation of certain bodies and the desire to develop computational tools to protect marginalised people \citep{Warner:2012}.
More recent work has instead directed its focus to demographically blind approaches to data collection and annotation, succumbing to ``colour-blind'' annotation processes and guidelines. Although processes that do not take marginalisation into account, but instead seek to treat every group equally provide an allure of fairness, they also encode dominant discourse on abuse with the subsequent result of the resistance to oppression and marginalisation being treated the same as marginalisation. In concert with the growing evidence of racially biased content moderation tools \citep{Waseem:2018,Davidson:2019}, demographically blind annotation criteria and data curation pose a threat to the goal of developing tools that aid in ensuring people from the right from persecution. One such example is presented by \citet{Salminen:2018} who develop a taxonomy that includes ``anti-white'' as a target of hate on par with anti-Black hate in spite of whiteness as a hegemonic entity that marginalises \citep{McIntosh:1988}.
A result of this are egregious annotation choice, such as ``The  white  will  always  steal;  FUCK  YOU  TO  ALL  WHITES  RACIST'' labelled as hate speech \citep{Salminen:2018}, in spite of the comment speaking to ongoing racism and the historical exploitation enacted by white societies (e.g. the theft of cultural artefacts from colonised territories \citep{Frost:2019}, the numerous genocides committed by imperialistic colonial states \citep{Weisbord:2003}, and the theft of bodies in the transatlantic slave trade). Moreover, and perhaps of even greater concern, the annotation and curation processes of \citet{Salminen:2018} result in data responding to the abuse of authority committed by police as hate, in one such example they identify the following comment as hate ``did to that poor guy. 10 s of pepper spray directly into the face, run over foot etc. equal it up a little bit, except for the detail of having a fucking stroke. So it still wouldnâ€™t be exactly what the guy went through. Fucking discusting. They get a hard on power tripping others. They are just fucking cowards'', in all likelihood due to the aggressive nature of the comment.

Through such demographically uninformed processes of curating and making data, a danger of erasure of past and ongoing marginalisation and responses to it as well as critical responses to the violent abuse of authority as ``hate speech'' that should be subject to content moderation. The question of automated hate speech detection thus, for works such as \citet{Salminen:2018} is no longer ensuring the right to not be persecuted but instead insuring that processes of marginalisation remain unchallenged.
For these reasons, I use the datasets released in early work, specifically I use the \textit{Offence} dataset \citep{Davidson:2017}, \textit{Toxicity} dataset \citep{Wulczyn:2017}, and \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} in all computational chapters. For \autoref{chap:liwc} which examines the influence and generalisability of vocabulary manipulation, I also use the \textit{Hate Expert} \citep{Waseem:2016} and the \textit{StormFront} \citep{Garcia:2019} datasets. Each of these datasets share the common attributes that they are collected either from spaces that are hateful towards marginalised groups or have considerations of marginalisation encoded into the annotation guidelines. In \autoref{chap:mtl} I also use three datasets that are labelled for abuse but instead to tasks that are seemingly related. First, I use the \textit{Argument Base} dataset \citep{Oraby_fact_feel:2017}, the second dataset (\textit{Sarcasm}) is developed for sarcasm detection \citep{Oraby_sarcasm:2017}, and the final dataset, \textit{Moral}, examines the moral sentiments
In an early effort to address issues of annotator biases and under-sampling of some forms of data in the data curation process, \citet{Waseem:2017} propose a typology of abuse that aims to categorise abuse by how it is characterised rather than determining the exact form of abuse. 
To this end, \citet{Waseem:2017} present a 2-dimensional typology of hate; the first dimension operates along implicit and explicitly expressed hate. Implicitly communicated hate, \citet{Waseem:2017} argue is hate that is communicated through subversive means by using code words and communicating implicit biases. Explicit abuse on the other hand is explicit in its intention to abuse, e.g. through the use of slurs. The second dimension concerns itself with the target of abuse that can either be a generalized other, or a specific group, the former category detailing abuse that is targeted towards small groups and individuals while the latter is aimed at generalised targets, e.g. larger demographics.
It's important to note that content may be simultaneously explicit and implicit, directed and generalised \citep{Waseem:2017}. For instance, content that implicitly targets Muslims, may simultaneously explicitly target a specific group of women.

\begin{table*}[ht]
\centering
\begin{tabular}{p{\textwidth/30}|p{0.45\textwidth}|p{0.45\textwidth}}
  & \textit{Explicit}    & \textit{Implicit} \\\hline
    \multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Directed}}}    &   {\scriptsize``Go kill yourself'',  ``You're a sad little f*ck'' \citep{Hee:2015a}}, \newline {\scriptsize ``@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga''  \citep{Davidson:2017}}, \newline {\scriptsize ``Youre one of the ugliest b*tches Ive ever fucking seen'' \citep{Kontostathis:2013}}. & {\scriptsize ``Hey Brendan, you look gorgeous today. What beauty salon did you visit?'' \citep{dinakar2012common}, \newline ``(((@User))) and what is your job?  Writing cuck articles and slurping Google balls?  \#Dumbgoogles'' \citep{Hine:2016},\newline  ``you're intelligence is so breathtaking!!!!!!'' \citep{dinakar2011modeling}}\\\hline
  \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{Generalized}}} & {\scriptsize``I am surprised they reported on this crap who cares about another dead n*gger?'', ``300 missiles are cool! Love to see um launched into Tel Aviv! Kill all the g*ys there!'' \citep{Nobata:2016}, \newline ``So an 11 year old n*gger girl killed herself over my tweets? \^ \_ \^\ thats another n*gger off the streets!!'' \citep{Kwok:2013}}. & {\scriptsize``Totally fed up with the way this country has turned into a haven for terrorists. Send them all back home.'' \citep{burnap2015cyber}, \newline ``Gas the skypes'' \citep{magu2017detecting}, \newline ``most of them come north and are good at just mowing lawns'' \citep{dinakar2011modeling}} \\
\end{tabular}
  \caption{Typology of abusive language presented by \citep{Waseem:2017}.}
\label{tab:typology}
\end{table*}

Modelling for automated hate speech detection has also undergone a development from early to contemporary work. Early work was primarily focused on feature-based modelling \citep[e.g.][]{Waseem-Hovy:2016,Waseem:2016,Davidson:2017} whereas subsequent work has directed a greater attention to neural network based approaches \citep[e.g.][]{Kolhatkar:2021,Waseem:2018,Gamback:2017,Badjatiya:2017}. In this dissertation I follow a similar pattern of developing baseline models from feature-based models and suggest neural network architectures as extensions and improvements on these. In early work, Logistic Regression (LR) and Support Vector Machines (SVM) were the most frequently used models. As the scholarship has developed, specific types of neural networks have come to dominate the modelling, namely Convolutional Neural Networks and Long-Short Term Memory networks. In each chapter, I perform the review of the models that are pertinent to the work in the chapter. Here instead I provide a theoretical overview of the models, their components (e.g. dropout and activation functions) and their intended functionalities (i.e. the kind of data that they are designed to operate on and which assumptions are built into the model architectures).

\subsection{Datasets}\label{sec:datasets}

Here I provide an overview of the different datasets that are used throughout this dissertation. For each dataset, I introduce the curation rationale, the source of the datasets, the annotation guidelines, annotator selection, and finally how each of these dimensions influence the resulting dataset.

% TODO Order:
% TODO Curation rationale
% TODO Source + Sampling method
% TODO Annotation guidelines
% TODO Annotator selection
% TODO How these influence
\subsubsection{Hate speech and abuse datasets}\label{sub:abuse_data}

\paragraph*{Hate Speech} Published as the first publicly available dataset for hate speech and abusive language detection, \citet{Waseem-Hovy:2016} developed a dataset for detecting abuse towards gendered and racialised minorities. In an interview in the Let's Chat Ethics Podcast, Zeerak Waseem shared that the initial motivation for developing the dataset was the somewhat na{\"i}ve hope to address online harassment as experienced by women during \#GamerGate, a harassment campaign against female game developers and games journalists \citep{Massanari:2015}. This aim of developing tools that can protect marginalised people is apparent in the data sampling and the source of data. As a large amount of the GamerGate abuse occurred on Twitter, \citet{Waseem-Hovy:2016} use Twitter as a source of their data, collecting $16,914$ tweets labelled as ``sexist'', ``racist'', and ``neither''.
In efforts to ensure that their collected and annotated sample contains gendered and racialised abuse, they bootstrap their corpus collection by first search for common slurs against women, ethnic minorities, religious minorities, and sexual minorities to identify the salient terms and users for scraping.
To annotate this dataset, with the target group in mind, \citet{Waseem-Hovy:2016} develop $11$ questions to test whether a comment is hateful or not.
This set of questions focuses on breadth in the types of hate expressed rather than depth in each type. This is apparent as the tests ranges from asking about explicit forms of hate, such as the use of slurs to implicit forms like questions around stereotyping and the use of straw man arguments in criticisms of minorities.
\citet{Waseem-Hovy:2016} annotate their dataset and have their annotations verified by an external annotator.
Collectively, these decisions are made to ensure that there was a diversity in the forms of hate in addition to the sources. However, as they note, the racist abuse only comes from $9$ different accounts. Moreover, as salient terms were sampled for annotation, some terms (i.e. the hashtag for the Australian TV show My Kitchen Rules) are over-represented in the data. In spite of these issues, the annotations in this dataset are \textit{embodied} within the context of critical race and gender studies perspectives on abuse.

\paragraph*{Hate Expert} In an extension of the dataset proposed by \citet{Waseem-Hovy:2016}, \citet{Waseem:2016} sample $6,909$ tweets from the original scrape and have it annotated by two groups, in efforts to understand the influence of annotator biases. The first group consisted of ``feminist and anti-racism activists'' \citep{Waseem:2016} who annotate the sample of the dataset with one of four labels ``racist'', ``sexist'', ``both'', and ``neither''. The second group of annotators were recruited from CrowdFlower to re-annotate the sample annotated by the first group.\footnote{CrowdFlower has since been renamed Appen.}
The label set was expanded by \citet{Waseem:2016} to include the category ``both'', in acknowledgement that marginalisation can be expressed across multiple dimensions, in an \textit{Intersectional} manner. Comparing models trained on each group of annotators, \citet{Waseem:2016} find that models that use the annotations of the first group consistently outperform models trained on the second. \citet{Waseem:2016} argue that the reason for this difference is that models trained on the former group benefit from similarities in the understanding of hate speech, whereas the distinct subjective positionalities of the latter group, that does not have a salient unifying characteristic beyond their work on a crowd-sourcing platform, produces internally inconsistencies in labelling that render it harder for models to consistently identify patterns that they can learn from.
In presenting this dataset, \citet{Waseem:2016} propose that ideologues can take similar positions on a topic, given their subjective positionalities. They argue that only through a principled understanding of hate speech is it possible to annotate reliably for hate speech and that crowd-sourced annotations for hate speech display inconsistencies that to some degree erases the meaning of the term.
In using this dataset for this dissertation, I use the annotations provided by the feminist and anti-racist activists.

\paragraph*{Offence}
\citep{Davidson:2017}

\paragraph*{Toxicity}
\citep{Wulczyn:2017}

\paragraph*{StormFront}
\citep{Garcia:2019}

\subsubsection{Non-abuse datasets}

\paragraph*{Sarcasm}
\citep{Oraby_sarcasm:2017}

\paragraph*{Argument Basis}
\citep{Oraby_fact_feel:2017}

\paragraph*{Moral Sentiment}
\citep{Hoover:2019}
% TODO Introduce datasets used in the dissertation (abusive and non-abusive)
% TODO Introduce annotation:
% TODO annotation guidelines, how they differ and how they influence and what that means
% TODO annotator selection and the influence

\subsection{Non-English datasets for abuse}
% TODO abusive language detection data in non-english languages
\zw{See hatespeechdatasets.com and ACL anthology}

In this dissertation, I focus my attention to detecting abuse in English language datasets as my methods do not map to other languages. However, an important growing body of research and resources are being developed for other languages such as Arabic \citep{Arabic abuse papers}, Chinese \citep{Chinese abuse papers}, Croatian \citep{Croatian papers}, Danish \citep{Danish abuse data}, French \citep{French papers}, German \citep{German papers}, Greek \citep{Greek papers}, Indonesian \citep{Indonesian papers}, Italian \citep{Italian papers}, Polish \citep{Polish papers}, Portuguese \citep{Portuguese papers}, Slovene \citep{Slovenian papers}, Spanish \citep{Spanish papers}, Turkish \citep{Turkish papers}, and Urdu \citep{Urdu papers}. Beyond these mono-lingual resources and approaches, there is also a body of work dedicated to abuse in code-switching contexts \citep{Code switching papers}.

Developing models for each individual language, and in particular resources that address abuse that code-switches, require an attention to the particularities of the different languages and cultures, just as model development for English requires researchers to be attuned to the particularities and cultures represented in English language use.



% \section{Natural Language Processing}
% In the recent years, there has been an increase in the amount of work focused on
% In the field of natural language processing (NLP), there has been a recent increase in work focused on abusive language and bias detection. Much of the research is still early-stage work, leaving much room for further inquiry, in particular on considering how biases are employed in written language. In this section, we will provide a detailed overview of recent work and trends in the topics.

% \subsection{Abusive Language Detection}
% % \zw{Update this}
% % Abusive language detection is a growing field of inquiry. Much off the early work focused on cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with little focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified has surfaced as an independent task \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Tulkens:2015,Agarwal:2016,Silva:2016,Park:2017,Samghabadi:2017}.
%
% A large part of the previous work on hate speech detection has primarily touched upon surface level analysis of abusive language, leaving much room for work to be done. A large effort has been expended in attempting to define annotation schemes. \cite{Waseem-Hovy:2016} proposed guidelines derived from gender studies \citep{McIntosh:1988} where a document is labeled as hate speech, if it fails any point in the guidelines.
%
% \cite{Waseem-Hovy:2016} released a data set for sexist and racist speech on social media which is annotated using their guidelines. In their paper, they investigate the impact of several features on detecting racism and sexism. They find that characters are more discriminative for hate speech detection in line with the findings of \cite{Mehdad:2016}. In addition, \cite{Waseem-Hovy:2016} find that information about a users gender can slightly improve classification performance, however they also find that adding location information slightly harms a classifiers performance. In addition, they find that information on length negatively impacts a classifiers performance.
%
% \cite{Ross:2016} investigate annotator agreement for anti-refugee sentiment. They instruct their annotators to follow the Twitter's guidelines for hateful content. They find that on a data set of 541 tweets, they achieve a very poor inter-annotator agreement, suggesting that it is necessary for clear and concise guidelines for annotation of abusive language.
%
% Building on the work of \cite{Waseem-Hovy:2016} and \cite{Ross:2016}, \cite{Waseem:2016} consider the impact of annotators' knowledge of hate speech for building models for hate speech detection; they find that employing feminist annotators for labeling data sets allows for more consistent annotations and models as compared to annotators that are not screened for political opinion. \cite{Waseem:2016} consider the application of features from sarcasm detection, using Author Historical Salient Terms (AHST) proposed by \cite{Bamman:2015}. The feature is generated by computing TF-IDF scores for each user and selecting the 100 highest weighted terms. If a term then occurs both in the document being analyzed and in the AHST. \cite{Waseem:2016} find that AHST performs extremely poorly, suggesting that hate speech may generally be a one off event, rather than a continuous stream of abuse. It is our contention that another reason AHST might not work is due to the data set employed being highly imbalanced. 
%
% \cite{Davidson:2017} seek to break down the task of hate speech detection into offensive language and hate speech and obtain labels for a Twitter data set using crowd sourced labor on CrowdFlower.
%
% More recently \cite{Badjatiya:2017} trained a deep convolutional neural network (CNN) on the data set annotated by \cite{Waseem-Hovy:2016}. By using a CNN on the data set \cite{Badjatiya:2017} obtain a significant improvement on Waseem and Hovy's (2016) scores improving the F1 score from  $73.89$ to $93.00$. Given the large increase in scores it is prudent to consider any potential errors. The data set \cite{Badjatiya:2017} employ, is highly imbalanced with the positive classes occupying a small minority of the labeled data, there is a risk that their model performs extremely well on the negative class but does not perform well on the positive classes. However, no error analysis is provided in the paper.
%
% In continuation of the results obtained by \cite{Badjatiya:2017}, \cite{Park:2017} compare using a two-step logistic regression classification, and a single step CNN approach to detecting hate speech. In the single step CNN, the specific form of hate speech is directly predicted, while in the two-step classification scenario, first a classifier is trained to identify whether a document contains abuse followed by predicting the specific type of abuse it is.
%
% A different approach is attempted by \cite{Waseem:2018}, in which they seek to combine three different datasets for abusive language detection using multi-task learning. With a hypothesis that abuse will differ between geographic and cultural locales, they seek to employ disjoint datasets and train two models, one for each data set that share parameters. We will seek to extend this work to employ more data sets of abusive language as well as related tasks, such as sentiment analysis.
%
% Finally, \cite{Jha:2017} break ``sexism'' down into benevolent and hostile sexism. They apply the ambivalent sexism theory as proposed by \cite{Glick:1996}. The ambivalent sexism theory suggests that there are two forms of sexism, benevolent sexism, which on a surface level speaks positively on women, but on a deeper level seeks to assert their inferiority, and hostile sexism, which expresses a strictly negative point of view on women. The following examples illustrate benevolent and hostile sexism respectively: ``Women are like flowers who need to be cherished.'' and ``Jus gonna say it..again..DUMB BITCH! \#MKR''.\vspace{5mm}
%
% As online platforms seek to remove abuse occurring on their platform, data sets that have been gathered and annotated may have the abusive documents removed, thus requiring several rounds of re-annotation of abusive language. In an attempt to deal with this, we will experiment with using documents that are assumed have a higher chance of being abusive as they are posted in forums that are known to abusive. Using these documents we will seek to build different forms of embeddings and evaluating on previously annotated data. Further, in an attempt to mitigate annotator needs, we will build an abusive language potential system which utilizes supervised methods for Named Entity Recognition (NER), Gender Identification \citep{Sap:2014}, and sentiment analysis amongst other methods. The goal of this is to identify the probability that a document has potential to contain abusive content, in efforts to exact greater control over what documents an annotator is faced with. Additionally, such a system will allow us to identify documents which are clearly abusive. Thus, we will be able to provide an automated method to create a seed set of positive documents for abusive language detection.
%
%\subsection{Datasets}
%
%
%\subsubsection{Datasets for abusive language detection}
%
% % TODO Rewrite these with more detail.
% % TODO Mention the aim for each dataset
% % TODO Mention data collection rationales for each dataset
% % TODO Mention the source of each dataset
% % TODO Mention annotation strategies for each dataset
% % TODO Tie together how these all build back into the stated aim for collection
% % TODO Mention high level dataset statistics
%
% Here, I provide only a brief overview of the datasets that are used for this task and address any processes on the datasets. Please see \autoref{sec:datasets} for a more complete introduction to each dataset.
%
% \zw{Davidson:2017}
% Our first dataset for training is the dataset developed by \citet{Davidson:2017} consists of $24,784$ tweets that are sampled from Twitter using keywords obtained from \citet{Hatebase}. The dataset is annotated for ``hate speech'', ``offensive language'' and ``neither''. The collection rationale was that not all content that immediately appears to be abusive is necessarily that, and that hate speech models must be able to distinguish between what is offensive and what is hateful \cite{Davidson:2017} (please see \autoref{chap:nlp} and \autoref{chap:filter} for more in-depth discussions on the implications of label categories, their overlap and differences). The dataset was annotated by crowd-workers on FigureEight\footnote{Previously known as CrowdFlower}. Unlike most datasets for abuse, this dataset consists primarily of positive instances, with $77$\% of the (binarised) dataset belonging to the positive class.
%
% \zw{Wulczyn:2017}
% Our second dataset used for training is the dataset presented by \citet{Wulczyn:2017}. This dataset consists of more then $100,000$ comments from Wikipedia talk pages that have been annotated for personal attacks and toxicity \cite{Wulczyn:2017}. The rationale of this dataset is that personal attacks are harmful to ongoing conversations, and that through the identification and removals of comments that poison, or toxify online conversations, more space will be left for healthy and constructive discussions (please see \autoref{chap:filter} for an in-depth consideration of the politics of what constitutes ``toxic'' and ``healthy''). The binarised distribution of documents tagged for toxicity aligns better with prior research, with the positive class consuming $\approx 9$\% of the dataset.
%
% \zw{Waseem-Hovy:2016}
% For our evaluation datasets, we use \citet{Waseem-Hovy:2016}, a dataset of $16,000$ documents that are sampled from Twitter and annotated for ``racism'', ``sexism'', and ``neither''. The dataset was annotated by two coders, who labelled $31$\% of the dataset containing as either ``sexist'' or ``racist'' content. This dataset was developed for an early exploration into automated content moderation of online hate speech.
%
% \zw{Waseem:2016}
% We also use the dataset by \citet{Waseem:2016} that followed this first exploration. Here the annotation guidelines remain the same while the label-set is expanded to include the intersection of racism and sexism, the ``both'' category. This dataset contains $6000$ documents, labelled by intersectional feminist activists, and another label-set annotated by crowd-workers from FigureEight. We choose the intersectional feminist tagged annotations, as \citet{Waseem:2016} show that simple computational models perform better using this tagset. The binarised positive labels occupy $15.19$\% of the dataset.
%
% \zw{Garcia:2019}
% Finally, we use the dataset on white supremacist speech developed by \citet{Garcia:2019}. This dataset, unlike the previous two evaluation datasets does not stem from Twitter, but instead the data is collected from StormFront\footnote{www.stormfront.net}, a web forum dedicated to the preservation and dissemination of white supremacist ideology. This dataset contains \zw{INSERT: Number of total document counts when using the entire dataset instead of the balanced one.} documents labelled for being hateful or not hateful, with \zw{INSERT: Percentage of positive class docs} in the positive class.
%
%
%
% \paragraph{Generalisable Machine Learning Models for Abusive Language Detection}
%
% A common critique of many current computational methods for abuse detection is that they have poor generalisability onto other datasets. Although this issue of non-generalisability poses a serious issue for the abuse community, it has received relatively little attention \citep{Waseem:2016,Waseem:2018,Karan:2018,Wiegand:2019,Swamy:2019,Fortuna:2021,Glavas:2020} in comparison to single-dataset classifier performance. While the primary goal of the experiments in this chapter is to consider how to both reduce the vocabulary space without loss, the concern of generalisation is important to understand the implications beyond within dataset performance. The task of generalising onto datasets collected using different methods and sampling strategies.
%
% In the pursuit of models that generalise well onto other datasets, researchers have proposed a variety of architectures. As an initial investigation into the question of generalisability, \citet{Waseem:2016} note that the best performing classifier on the dataset they propose does not generalise well onto the dataset published by \citet{Waseem-Hovy:2016}, noting that the performance of their classifier drops by more than a quarter. \citet{Waseem:2018} specifically address the issue of poor generalisability between the datasets poposed by \citet{Waseem:2016,Waseem-Hovy:2016} and the dataset published by \citet{Davidson:2017}. Turning towards multi-task learning as an avenue for
% their best performing classifier and apply it to the dataset proposed by \citet{Waseem-Hovy:2016}, finding that there is a significant performance drop of more than $20\%$. In subsequent work, \citet{Waseem:2018} c
%
% \zw{Note about different datasets collection rationales.}
% Although several works have attempted to predict abuse across datasets,
%
%\subsubsection{Datasets for Multi-Task Learning Models}
%\zw{Best described in MTL chapter?}
%
% For multi-task learning
%
\subsection{Terminology, discrepancies, and synergies}

\zw{Describe different annotation schemas, what they mean and how they influence}
\zw{Describe how the different annotation schemes overlap and differ from one another}

\paragraph{Generalisable Machine Learning Models for Abusive Language Detection}

%\section{Annotation}

%\subsection{Annotation Guidelines}
%\zw{Write about annotator guidelines}
%
%\subsection{Annotator Selection}
% \zw{Write about annotator influences}
% There are some interesting discrepancies in the selection of annotators for the datasets that we apply our models to. \citet{Waseem:2016} select their annotators based on socio-political positions, controlling for a specific interpretation of abuse. On the other hand \citet{Wulczyn:2017} select their annotators from the users of the Wikipedia Talk pages. However, the Wikipedia editor community has been accused of being a highly male space that is unwelcoming to women \cite{CITE: Cite article talking about anti-women culture on wikipedia}. This suggests that the influence of their selection of annotators, who are culturally situated in the norms and culture of the Wikipedia editor community, are also likely to be less attuned to content that may be offensive to women, but is accepted communicative practices within the Wikipedia editor community.
%
% On the other hand \citet{Garcia:2019} and \citet{Davidson:2017} select annotators that are removed from the context of the documents they are annotating. This suggests that global understandings of what constitutes abuse are possible, and that it is possible to annotate without a deep understanding of the issues and communicative practices of the specific communities that are being investigated.
%
% While we accept that the influence of annotation guidelines have strong influences on the subject that is being examined (e.g. \citet{Davidson:2017} examine the differences in what is merely offensive and what is hateful, \citet{Garcia:2019} examine what is hateful from a white supremacist community and what is not, and \citet{Wulczyn:2017} examine things that make conversations toxic and hostile), we assume that these different guidelines and questions highlight different aspects of abuse. Through our efforts to develop methods that can identify different forms of abuse across different datasets, we accept the assumption that there are some global understandings of abuse that can be learned by machine learning models. We revisit this assumption in \autoref{chap:disembodied}.
%
% \citet{Waseem:2016} argue that due to different backgrounds and political stances, the operationalisation of annotation guidelines differ from person to person. They show that this then influences the quality of annotations of datasets, as a lack of control of socio-political backgrounds also suggests a lack of control of what is being annotated. They argue that such differences in operationalisation across a dataset influences what machine learning models are able to predict. Conversely, \citet{Founta:2017} operate with the assumption that with a large numbers of annotators, such differences will even out, such that dominant discourse understanding of what constitutes abuse and hate will even out. However, considering the argument that \citet{Douglas:1966} makes about what constitutes dirt is both contextual and cultural, what remains through majority voting by a large number of annotators, will be few salient discourses of what constitutes abuse and hate speech. Scaling annotations across cultural divides then also suggests that only what is universally accepted as abuse should be considered as abusive.
%
% This notion of dominant discourse, and culturally agnostic definitions of abuse stand in stark contrast to the widespread calls for moderation of abuse to be culturally contingent \cite{CITE: Articles about how content moderation fails}. More importantly, as \citet{Waseem:2017,Davidson:2019} and \citet{Sap:2019} argue, such dominant discourse perspectives on acceptability are likely to reproduce cultural, and often racial, biases towards marginalised communities.
%
%
%\subsection{Platform Affordances}
%
% \zw{Write theoretically about platform affordances, base in STS litt.}
% As the datasets differ quite significantly in the sizes of the raw number of documents as well as the vocabulary sizes. Moreover, as the datasets are selected from different websites with different communities, purposes, and means of interaction; the data sampled from each platform may differ in content as well as style. Considering for instance \citet{Waseem:2016}, this dataset was collected on Twitter while the maximum length of a tweet was $140$ characters. Documents are thus short as they are given an upper limit on the number of characters. On the other hand, \citet{Wulczyn:2017} collected their data from the Wikipedia Editor Talk pages, where comments are not limited by length. Additionally, these two domains differ in that conversations on Twitter may have no particular topic, conversations on Wikipedia Talk pages always refer back to a specific topic and the conversation of how to address a particular edit to a page. Finally, given Wikipedia's ongoing issues with recruiting editors from a diverse set of backgrounds \cite{CITE: Wikipedia editors issue} and Twitter's comparatively broad user base \cite{CITE: Twitter userbase by demographic ref} may influence which dialects are represented on the platforms, which patterns of speech (e.g. sociolects, slang, and shorthand) occur, and the style of the discussions and conversations.
%
%
\section{Models}\label{sec:model_background}
%
% Throughout this thesis, we develop and train several different model types to test our hypotheses. We use a mixture between linear and non-linear models both to act as points of comparison with one another and to test how well both simple and more complex models perform given our tasks. In each chapter, we will go into greater detail with regard to the implementation details for each model while we provide a theoretical description of each of these models here.
%
\subsection{Byte-Pair Encoding and Linguistic Inquiry and Word Count}

\zw{Add info on LIWC dictionary size}

\subsection{Dropout and Early Stopping}\label{sec:dropoutearly}

\subsection{Input Encoding}

\zw{Explain the vectorisors used.}
\zw{Explain onehot and embedding layers}
%
% As embedding layers are layers that are optimised, it is unnecessary for the researcher to perform feature selection \cite{CITE: Papers that say no feature selection is needed}, although it can be a benefit \cite{CITE: Papers that take feature selection into account}. The process of selecting features for a model to consider implores the researcher to have a firm grasp of the concepts they are seeking to examine. This required intentionality of the researcher also comes with the risk that the researcher may, intentionally or unintentionally, omit features that illuminate a pattern in the data that the machine learning model can take advantage of.  Here optimisable representations excel if features are not pre-selected for them, as through multiple rounds of updating the layer, the resulting representation takes advantage of the patterns that emerge from the data.
%
\subsection{Softmax, Loss, and optimisers}

\subsubsection{Loss}

\paragraph{Negative Log Likelihood}

\paragraph{Cross Entropy}

\paragraph{L1 and L2}

\subsection{Activation Functions and Attention Layers}

\subsubsection{Sigmoid}

\subsubsection{Tanh}

\subsubsection{Rectified Linear Unit}

\subsection{Logistic Regression}

\subsection{Support Vector Machines}

\subsection{MLP}

\subsection{LSTM}
% TODO Start by explaining an RNN
% TODO Explain how LSTMs differ from RNNs

\subsection{RNN}

\subsection{CNN}

\subsubsection{Max Pooling}

\subsection{Multitask Learning}
%
% \zw{CITE: Add citations to the section below.}
% The Multi-task learning framework was initially proposed by \citet{Caruana:1997} as a way to train a model for a specific primary task by leveraging that (multiple) auxiliary tasks may be related and could thus provide inductive biases for the resulting model to take advantage of for its primary task. The multi-task learning framework can be trained in two different ways, through hard parameter sharing or soft parameter sharing. Training Multi-task learning models using hard parameter sharing is performed by having (some) hidden layers in the model that are shared by all tasks. When training each task, the model updates all layers of that task, including the shared hidden layers. Notably, while hard parameter sharing rely on (some) shared layers, the output layers are not shared between tasks \cite{CITE: Hard parameter sharing paper}. On the other hand, models that are trained using soft parameter sharing do not share any layers, instead the parameters of each task are regularised to be similar \cite{CITE: Soft parameter sharing paper - See Sebastian Ruder's blogpost on MTL for reference}. In \autoref{fig:mtl_types} we see the two different types of models. As seen in \autoref{fig:mtl_hard}, while each task have individual input and output layers, they all share a hidden layer. In \autoref{fig:mtl_soft}, we see that each task has its own model that would be unrelated to one another, if not for the fact that the parameters of each layer are regularised to be similar to each other. As researchers are frequently interested in one task over others, one can assign a primary task and a number of auxiliary task through a range of strategies, including weighting each task and the probability with which each task is selected for training.
%
%\begin{figure}
%  \centering
%  \begin{minipage}{0.5\linewidth}
%    \centering
%    \includegraphics[scale=0.75]{Figs/multitask_hard.jpg}
%    \caption{Depiction of Multi-task learning framework using hard parameter sharing.}
%    \label{fig:mtl_hard}
%  \end{minipage}
%  \begin{minipage}{0.5\linewidth}
%    \centering
%     \includegraphics[scale=0.75]{Figs/multitask_soft.jpg}
%    \caption{Depiction of Multi-task learning framework using soft parameter sharing.}
%    \label{fig:mtl_soft}
%  \end{minipage}
%  \caption{Parameter sharing strategies for Multi-task learning.}
%  \label{fig:mtl_types}
%\end{figure}
%
% While the idea of inductive biases from related tasks provides a compelling argument for examining multi-task learning for abuse and hate speech detection, there are some interesting attributes to the framework. First, as multi-task learning is compatible with neural networks, researchers can forego feature selection similar to other neural network approaches, with the same benefits and risks as described previously.
% Second, while the ensemble model training framework may appear very similar to multi-task learning framework, a key dissimilarity is apparent in that multi-task learning models seek to share information between the different tasks; for hard parameter sharing models this sharing occurs through shared layers \cite{CITE: Hard parameter sharing paper}, while for soft parameter sharing models this sharing happens through the regularisation that ensures similarity of the layers across the models for each task \cite{CITE: Soft parameter sharing paper}.
% Third, for hard-parameter sharing models the complexity of developing the model is reduced as the input and output layers are task independent, while at least one layer of the remainder of the model may be shared. Thus, only a single model is trained, where the researchers need only to concern themselves with the layers that are not shared, rather than concern themselves with full models and how to balance them.
% Fourth, as \cite{Caruana:1997} show, the framework allows for training for several distinct tasks while leveraging the similarities shared by each individual task.
% For hard parameter sharing models, this approach also introduces the risk (and opportunity) of a single task dominating the representation of the model, due to either more data being available or a task being selected with for training with greater probability than the remaining tasks. \citet{CITE: Weighting paper} argue, this risk can be mitigated by either weighting the loss function, such that loss of each task is controlled by the researcher and the importance they wish to provide each task \cite{CITE: Weighting paper}. However, as \cite{CITE: Weighting/aux task paper} show, there is also an opportunity in this risk. By assigning the task of primary interest a higher weight than all other tasks, the model can be guided towards prioritising what is learned from this task over all others \cite{CITE: Paper with auxiliary tasks}. Selecting such weighting of the different tasks, in a similar vein to feature selection assumes that the researcher has knowledge and a hypothesis about how the different tasks are likely to influence each other.
% Fifth, when working with different datasets for similar and distinct tasks alike, directly leveraging them outside of a multi-task learning model can be a cause for concern due to differences in collection rationales, data sources, or annotation strategies \cite{Waseem:2018}. However, through both weighting of the different tasks and the fact that each task has either its own input and output layers or its own model, such concerns can be alleviated due to either limited shared layers that are optimised or due to distinct models being trained that are regularised to minimise dissimilarity, depending on which parameter sharing strategy is used.
% Finally, in the event that an auxiliary task does not contribute to the primary task as the researcher had hypothesised, it may still contribute to the overall generalisability of the model as the offending task will act as regulariser for the primary task, as it introduces noise into shared layer according to \cite{CITE: Cite paper that argues the regularising effect of aux tasks}.
%
\zw{Add paragraph on how MTL works with mini batching, batch selection, etc.}
%
%
\subsection{Bayesian Hyper Parameter Tuning}\label{sub:bho}

\subsection{Metrics}

\zw{INSERT: Explanation of F1, macro-F1, Precision, Recall, Accuracy}

\section{Fairness}\label{sec:fairlitt}

Previous work on bias and fairness in machine learning pipelines operate within four overarching logics:

\begin{enumerate}
  \item{A descriptive strand which aims to map out models and datasets with their intended uses and limitations, }
  \item{the quantification and analysis of disparities in model performances,}
  \item{the mitigation of biases that are present in the models and datasets, and}
  \item{imaginaries of more equitable futures for AI.}
\end{enumerate}

\subsection{Mapping Limitations}

% Mapping
\cite{Mitchell:2019}
\cite{Bender-Friedman:2018}
\cite{Hovy-Spruit:2016}
\cite{Blodget:2020}
\cite{Holstein:2019}

\subsection{Quantifying harms}
\cite{Buolamwini:2018}
\cite{Kulynych:2020}
\cite{Shah:2020}
\cite{Vanmassenhove:2018}
\cite{Waseem:2016}
\cite{Derzynski:2016}
\cite{Birhane:2020}

\subsection{Quantification}
\cite{Agarwal:2018}
\cite{Romanov:2019}
Note that \cite{Sap:2019} misuse \cite{Blodgett:2016} to provide assumed demographic affiliation of the author, however author attributes are computationally estimated and \cite{Blodgett:2016}'s method does not afford such attribution.
\cite{Davidson:2019}
\cite{Zhao:2017}

\subsection{New Futures}

\cite{Yimam-Biemann:2018}
\cite{Bingel:2018}
\cite{Kalluri:2019}

\section{Summary}
% In this chapter, we have introduced the NLP work related to thesis and sought to show how we will build and expand on this work.
