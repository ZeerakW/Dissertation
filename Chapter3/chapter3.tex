% !TEX root = ../thesis.tex
\chapter{Computational Background}\label{chap:nlp}

\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi
In this chapter, we will introduce related work in Natural Language Processing. Additionally, we will introduce the methods which we will employ in this thesis.

\zw{Add more new papers and more papers that are relevant to the exact narrative of this thesis}
\zw{Add a few words about how word embeddings are trained.}

\section{Natural Language Processing}
In the field of natural language processing (NLP), there has been a recent increase in work focused on abusive language and bias detection. Much of the research is still early-stage work, leaving much room for further inquiry, in particular on considering how biases are employed in written language. In this section, we will provide a detailed overview of recent work and trends in the topics.

\subsection{Abusive Language Detection}
% \zw{Update this}
Abusive language detection is a growing field of inquiry. Much off the early work focused on cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with little focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified has surfaced as an independent task \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Tulkens:2015,Agarwal:2016,Silva:2016,Park:2017,Samghabadi:2017}.

A large part of the previous work on hate speech detection has primarily touched upon surface level analysis of abusive language, leaving much room for work to be done. A large effort has been expended in attempting to define annotation schemes. \cite{Waseem-Hovy:2016} proposed guidelines derived from gender studies \citep{McIntosh:1988} where a document is labeled as hate speech, if it fails any point in the guidelines. 

\cite{Waseem-Hovy:2016} released a data set for sexist and racist speech on social media which is annotated using their guidelines. In their paper, they investigate the impact of several features on detecting racism and sexism. They find that characters are more discriminative for hate speech detection in line with the findings of \cite{Mehdad:2016}. In addition, \cite{Waseem-Hovy:2016} find that information about a users gender can slightly improve classification performance, however they also find that adding location information slightly harms a classifiers performance. In addition, they find that information on length negatively impacts a classifiers performance.

\cite{Ross:2016} investigate annotator agreement for anti-refugee sentiment. They instruct their annotators to follow the Twitter's guidelines for hateful content. They find that on a data set of 541 tweets, they achieve a very poor inter-annotator agreement, suggesting that it is necessary for clear and concise guidelines for annotation of abusive language.

Building on the work of \cite{Waseem-Hovy:2016} and \cite{Ross:2016}, \cite{Waseem:2016} consider the impact of annotators' knowledge of hate speech for building models for hate speech detection; they find that employing feminist annotators for labeling data sets allows for more consistent annotations and models as compared to annotators that are not screened for political opinion. \cite{Waseem:2016} consider the application of features from sarcasm detection, using Author Historical Salient Terms (AHST) proposed by \cite{Bamman:2015}. The feature is generated by computing TF-IDF scores for each user and selecting the 100 highest weighted terms. If a term then occurs both in the document being analyzed and in the AHST. \cite{Waseem:2016} find that AHST performs extremely poorly, suggesting that hate speech may generally be a one off event, rather than a continuous stream of abuse. It is our contention that another reason AHST might not work is due to the data set employed being highly imbalanced. 

\cite{Davidson:2017} seek to break down the task of hate speech detection into offensive language and hate speech and obtain labels for a Twitter data set using crowd sourced labor on CrowdFlower.

More recently \cite{Badjatiya:2017} trained a deep convolutional neural network (CNN) on the data set annotated by \cite{Waseem-Hovy:2016}. By using a CNN on the data set \cite{Badjatiya:2017} obtain a significant improvement on Waseem and Hovy's (2016) scores improving the F1 score from  $73.89$ to $93.00$. Given the large increase in scores it is prudent to consider any potential errors. The data set \cite{Badjatiya:2017} employ, is highly imbalanced with the positive classes occupying a small minority of the labeled data, there is a risk that their model performs extremely well on the negative class but does not perform well on the positive classes. However, no error analysis is provided in the paper.

In continuation of the results obtained by \cite{Badjatiya:2017}, \cite{Park:2017} compare using a two-step logistic regression classification, and a single step CNN approach to detecting hate speech. In the single step CNN, the specific form of hate speech is directly predicted, while in the two-step classification scenario, first a classifier is trained to identify whether a document contains abuse followed by predicting the specific type of abuse it is.

A different approach is attempted by \cite{Waseem:2018}, in which they seek to combine three different datasets for abusive language detection using multi-task learning. With a hypothesis that abuse will differ between geographic and cultural locales, they seek to employ disjoint datasets and train two models, one for each data set that share parameters. We will seek to extend this work to employ more data sets of abusive language as well as related tasks, such as sentiment analysis.

Finally, \cite{Jha:2017} break ``sexism'' down into benevolent and hostile sexism. They apply the ambivalent sexism theory as proposed by \cite{Glick:1996}. The ambivalent sexism theory suggests that there are two forms of sexism, benevolent sexism, which on a surface level speaks positively on women, but on a deeper level seeks to assert their inferiority, and hostile sexism, which expresses a strictly negative point of view on women. The following examples illustrate benevolent and hostile sexism respectively: ``Women are like flowers who need to be cherished.'' and ``Jus gonna say it..again..DUMB BITCH! \#MKR''.\vspace{5mm}

As online platforms seek to remove abuse occurring on their platform, data sets that have been gathered and annotated may have the abusive documents removed, thus requiring several rounds of re-annotation of abusive language. In an attempt to deal with this, we will experiment with using documents that are assumed have a higher chance of being abusive as they are posted in forums that are known to abusive. Using these documents we will seek to build different forms of embeddings and evaluating on previously annotated data. Further, in an attempt to mitigate annotator needs, we will build an abusive language potential system which utilizes supervised methods for Named Entity Recognition (NER), Gender Identification \citep{Sap:2014}, and sentiment analysis amongst other methods. The goal of this is to identify the probability that a document has potential to contain abusive content, in efforts to exact greater control over what documents an annotator is faced with. Additionally, such a system will allow us to identify documents which are clearly abusive. Thus, we will be able to provide an automated method to create a seed set of positive documents for abusive language detection.

\subsection{Datasets}\label{sec:datasets}

\subsection{Terminology, discrepancies, and synergies}

\zw{Describe different annotation schemas, what they mean and how they influence}
\zw{Describe how the different annotation schemes overlap and differ from one another}


\section{Annotation}

\subsection{Annotation Guidelines}
\zw{Write about annotator guidelines}

\subsection{Annotator Selection}
\zw{Write about annotator influences}

\citet{Waseem:2016} argue that due to different backgrounds and political stances, the operationalisation of annotation guidelines differ from person to person. They show that this then influences the quality of annotations of datasets, as a lack of control of socio-political backgrounds also suggests a lack of control of what is being annotated. They argue that such differences in operationalisation across a dataset influences what machine learning models are able to predict. Conversely, \citet{Founta:2017} operate with the assumption that with a large numbers of annotators, such differences will even out, such that dominant discourse understanding of what constitutes abuse and hate will even out. However, considering the argument that \citet{Douglas:1966} makes about what constitutes dirt is both contextual and cultural, what remains through majority voting by a large number of annotators, will be few salient discourses of what constitutes abuse and hate speech. Scaling annotations across cultural divides then also suggests that only what is universally accepted as abuse should be considered as abusive.

This notion of dominant discourse, and culturally agnostic definitions of abuse stand in stark contrast to the widespread calls for moderation of abuse to be culturally contingent \cite{CITE: Articles about how content moderation fails}. More importantly, as \citet{Waseem:2017,Davidson:2019} and \citet{Sap:2019} argue, such dominant discourse perspectives on acceptability are likely to reproduce cultural, and often racial, biases towards marginalised communities.

\subsection{Platform Affordances}

\zw{Write theoretically about platform affordances, base in STS litt.}


\section{Models}\label{sec:model_background}

Throughout this thesis, we develop and train several different model types to test our hypotheses. We use a mixture between linear and non-linear models both to act as points of comparison with one another and to test how well both simple and more complex models perform given our tasks. In each chapter, we will go into greater detail with regard to the implementation details for each model while we provide a theoretical description of each of these models here.

\subsection{Byte-Pair Encoding and Linguistic Inquiry and Word Count}

\zw{Add info on LIWC dictionary size}

\subsection{Dropout and Early Stopping}\label{sec:dropoutearly}

\subsection{Input Encoding}

\zw{Explain the vectorisors used.}
\zw{Explain onehot and embedding layers}

As embedding layers are layers that are optimised, it is unnecessary for the researcher to perform feature selection \cite{CITE: Papers that say no feature selection is needed}, although it can be a benefit \cite{CITE: Papers that take feature selection into account}. The process of selecting features for a model to consider implores the researcher to have a firm grasp of the concepts they are seeking to examine. This required intentionality of the researcher also comes with the risk that the researcher may, intentionally or unintentionally, omit features that illuminate a pattern in the data that the machine learning model can take advantage of.  Here optimisable representations excel if features are not pre-selected for them, as through multiple rounds of updating the layer, the resulting representation takes advantage of the patterns that emerge from the data.

\subsection{Softmax, Loss, and optimisers}

\subsubsection{Loss}

\paragraph{Negative Log Likelihood}

\paragraph{Cross Entropy}

\paragraph{L1 and L2}

\subsection{Activation Functions and Attention Layers}

\subsubsection{Sigmoid}

\subsubsection{Tanh}

\subsubsection{Rectified Linear Unit}

\subsection{Logistic Regression}

\subsection{Support Vector Machines}

\subsection{MLP}

\subsection{RNN}

\subsection{LSTM}

\subsection{CNN}

\subsection{Multitask Learning}

\zw{CITE: Add citations to the section below.}
The Multi-task learning framework was initially proposed by \citet{Caruana:1997} as a way to train a model for a specific primary task by leveraging that (multiple) auxiliary tasks may be related and could thus provide inductive biases for the resulting model to take advantage of for its primary task. The multi-task learning framework can be trained in two different ways, through hard parameter sharing or soft parameter sharing. Training Multi-task learning models using hard parameter sharing is performed by having (some) hidden layers in the model that are shared by all tasks. When training each task, the model updates all layers of that task, including the shared hidden layers. Notably, while hard parameter sharing rely on (some) shared layers, the output layers are not shared between tasks \cite{CITE: Hard parameter sharing paper}. On the other hand, models that are trained using soft parameter sharing do not share any layers, instead the parameters of each task are regularised to be similar \cite{CITE: Soft parameter sharing paper - See Sebastian Ruder's blogpost on MTL for reference}. In \autoref{fig:mtl_types} we see the two different types of models. As seen in \autoref{fig:mtl_hard}, while each task have individual input and output layers, they all share a hidden layer. In \autoref{fig:mtl_soft}, we see that each task has its own model that would be unrelated to one another, if not for the fact that the parameters of each layer are regularised to be similar to each other. As researchers are frequently interested in one task over others, one can assign a primary task and a number of auxiliary task through a range of strategies, including weighting each task and the probability with which each task is selected for training.

%\begin{figure}
%  \centering
%  \begin{minipage}{0.5\linewidth}
%    \centering
%    \includegraphics[scale=0.75]{Figs/multitask_hard.jpg}
%    \caption{Depiction of Multi-task learning framework using hard parameter sharing.}
%    \label{fig:mtl_hard}
%  \end{minipage}
%  \begin{minipage}{0.5\linewidth}
%    \centering
%     \includegraphics[scale=0.75]{Figs/multitask_soft.jpg}
%    \caption{Depiction of Multi-task learning framework using soft parameter sharing.}
%    \label{fig:mtl_soft}
%  \end{minipage}
%  \caption{Parameter sharing strategies for Multi-task learning.}
%  \label{fig:mtl_types}
%\end{figure}

While the idea of inductive biases from related tasks provides a compelling argument for examining multi-task learning for abuse and hate speech detection, there are some interesting attributes to the framework. First, as multi-task learning is compatible with neural networks, researchers can forego feature selection similar to other neural network approaches, with the same benefits and risks as described previously.
Second, while the ensemble model training framework may appear very similar to multi-task learning framework, a key dissimilarity is apparent in that multi-task learning models seek to share information between the different tasks; for hard parameter sharing models this sharing occurs through shared layers \cite{CITE: Hard parameter sharing paper}, while for soft parameter sharing models this sharing happens through the regularisation that ensures similarity of the layers across the models for each task \cite{CITE: Soft parameter sharing paper}.
Third, for hard-parameter sharing models the complexity of developing the model is reduced as the input and output layers are task independent, while at least one layer of the remainder of the model may be shared. Thus, only a single model is trained, where the researchers need only to concern themselves with the layers that are not shared, rather than concern themselves with full models and how to balance them.
Fourth, as \cite{Caruana:1997} show, the framework allows for training for several distinct tasks while leveraging the similarities shared by each individual task.
For hard parameter sharing models, this approach also introduces the risk (and opportunity) of a single task dominating the representation of the model, due to either more data being available or a task being selected with for training with greater probability than the remaining tasks. \citet{CITE: Weighting paper} argue, this risk can be mitigated by either weighting the loss function, such that loss of each task is controlled by the researcher and the importance they wish to provide each task \cite{CITE: Weighting paper}. However, as \cite{CITE: Weighting/aux task paper} show, there is also an opportunity in this risk. By assigning the task of primary interest a higher weight than all other tasks, the model can be guided towards prioritising what is learned from this task over all others \cite{CITE: Paper with auxiliary tasks}. Selecting such weighting of the different tasks, in a similar vein to feature selection assumes that the researcher has knowledge and a hypothesis about how the different tasks are likely to influence each other.
Fifth, when working with different datasets for similar and distinct tasks alike, directly leveraging them outside of a multi-task learning model can be a cause for concern due to differences in collection rationales, data sources, or annotation strategies \cite{Waseem:2018}. However, through both weighting of the different tasks and the fact that each task has either its own input and output layers or its own model, such concerns can be alleviated due to either limited shared layers that are optimised or due to distinct models being trained that are regularised to minimise dissimilarity, depending on which parameter sharing strategy is used.
Finally, in the event that an auxiliary task does not contribute to the primary task as the researcher had hypothesised, it may still contribute to the overall generalisability of the model as the offending task will act as regulariser for the primary task, as it introduces noise into shared layer according to \cite{CITE: Cite paper that argues the regularising effect of aux tasks}.

\zw{Add paragraph on how MTL works with mini batching, batch selection, etc.}


\subsection{Bayesian Hyper Parameter Tuning}

\subsection{Metrics}

\zw{INSERT: Explanation of F1, macro-F1, Precision, Recall, Accuracy}

\section{Fairness}\label{sec:fairlitt}

Previous work on bias and fairness in machine learning pipelines operate within four overarching logics:

\begin{enumerate}
  \item{A descriptive strand which aims to map out models and datasets with their intended uses and limitations, }
  \item{the quantification and analysis of disparities in model performances,}
  \item{the mitigation of biases that are present in the models and datasets, and}
  \item{imaginaries of more equitable futures for AI.}
\end{enumerate}

\subsection{Mapping Limitations}

% Mapping
\cite{Mitchell:2019}
\cite{Bender-Friedman:2018}
\cite{Hovy-Spruit:2016}
\cite{Blodget:2020}
\cite{Holstein:2019}

\subsection{Quantifying harms}
\cite{Buolamwini:2018}
\cite{Kulynych:2020}
\cite{Shah:2020}
\cite{Vanmassenhove:2018}
\cite{Waseem:2016}
\cite{Derzynski:2016}
\cite{Birhane:2020}

\subsection{Quantification}
\cite{Agarwal:2018}
\cite{Romanov:2019}
Note that \cite{Sap:2019} misuse \cite{Blodgett:2016} to provide assumed demographic affiliation of the author, however author attributes are computationally estimated and \cite{Blodgett:2016}'s method does not afford such attribution.
\cite{Davidson:2019}
\cite{Zhao:2017}

\subsection{New Futures}

\cite{Yimam-Biemann:2018}
\cite{Bingel:2018}
\cite{Kalluri:2019}

\section{Summary}
In this chapter, we have introduced the NLP work related to thesis and sought to show how we will build and expand on this work.
