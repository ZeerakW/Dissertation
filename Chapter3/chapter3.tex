% !TEX root = ../thesis.tex
\chapter{Computational Background}\label{chap:nlp}

\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

In this chapter I introduce related work in Natural Language Processing (NLP) and theoretical background on the machine learning methods that I use throughout this dissertation.

% \subsection{Abusive Language Detection}
% % \zw{Update this}
% % Abusive language detection is a growing field of inquiry. Much off the early work focused on cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with little focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified has surfaced as an independent task \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Tulkens:2015,Agarwal:2016,Silva:2016,Park:2017,Samghabadi:2017}.
%
% A large part of the previous work on hate speech detection has primarily touched upon surface level analysis of abusive language, leaving much room for work to be done. A large effort has been expended in attempting to define annotation schemes. \cite{Waseem-Hovy:2016} proposed guidelines derived from gender studies \citep{McIntosh:1988} where a document is labeled as hate speech, if it fails any point in the guidelines.
%
% \cite{Waseem-Hovy:2016} released a data set for sexist and racist speech on social media which is annotated using their guidelines. In their paper, they investigate the impact of several features on detecting racism and sexism. They find that characters are more discriminative for hate speech detection in line with the findings of \cite{Mehdad:2016}. In addition, \cite{Waseem-Hovy:2016} find that information about a users gender can slightly improve classification performance, however they also find that adding location information slightly harms a classifiers performance. In addition, they find that information on length negatively impacts a classifiers performance.
%
% \cite{Ross:2016} investigate annotator agreement for anti-refugee sentiment. They instruct their annotators to follow the Twitter's guidelines for hateful content. They find that on a data set of 541 tweets, they achieve a very poor inter-annotator agreement, suggesting that it is necessary for clear and concise guidelines for annotation of abusive language.
%
% Building on the work of \cite{Waseem-Hovy:2016} and \cite{Ross:2016}, \cite{Waseem:2016} consider the impact of annotators' knowledge of hate speech for building models for hate speech detection; they find that employing feminist annotators for labeling data sets allows for more consistent annotations and models as compared to annotators that are not screened for political opinion. \cite{Waseem:2016} consider the application of features from sarcasm detection, using Author Historical Salient Terms (AHST) proposed by \cite{Bamman:2015}. The feature is generated by computing TF-IDF scores for each user and selecting the 100 highest weighted terms. If a term then occurs both in the document being analyzed and in the AHST. \cite{Waseem:2016} find that AHST performs extremely poorly, suggesting that hate speech may generally be a one off event, rather than a continuous stream of abuse. It is our contention that another reason AHST might not work is due to the data set employed being highly imbalanced.
%
% \cite{Davidson:2017} seek to break down the task of hate speech detection into offensive language and hate speech and obtain labels for a Twitter data set using crowd sourced labor on CrowdFlower.
%
% More recently \cite{Badjatiya:2017} trained a deep convolutional neural network (CNN) on the data set annotated by \cite{Waseem-Hovy:2016}. By using a CNN on the data set \cite{Badjatiya:2017} obtain a significant improvement on Waseem and Hovy's (2016) scores improving the F1 score from  $73.89$ to $93.00$. Given the large increase in scores it is prudent to consider any potential errors. The data set \cite{Badjatiya:2017} employ, is highly imbalanced with the positive classes occupying a small minority of the labeled data, there is a risk that their model performs extremely well on the negative class but does not perform well on the positive classes. However, no error analysis is provided in the paper.
%
% In continuation of the results obtained by \cite{Badjatiya:2017}, \cite{Park:2017} compare using a two-step logistic regression classification, and a single step CNN approach to detecting hate speech. In the single step CNN, the specific form of hate speech is directly predicted, while in the two-step classification scenario, first a classifier is trained to identify whether a document contains abuse followed by predicting the specific type of abuse it is.
%
% A different approach is attempted by \cite{Waseem:2018}, in which they seek to combine three different datasets for abusive language detection using multi-task learning. With a hypothesis that abuse will differ between geographic and cultural locales, they seek to employ disjoint datasets and train two models, one for each data set that share parameters. We will seek to extend this work to employ more data sets of abusive language as well as related tasks, such as sentiment analysis.
%
% Finally, \cite{Jha:2017} break ``sexism'' down into benevolent and hostile sexism. They apply the ambivalent sexism theory as proposed by \cite{Glick:1996}. The ambivalent sexism theory suggests that there are two forms of sexism, benevolent sexism, which on a surface level speaks positively on women, but on a deeper level seeks to assert their inferiority, and hostile sexism, which expresses a strictly negative point of view on women. The following examples illustrate benevolent and hostile sexism respectively: ``Women are like flowers who need to be cherished.'' and ``Jus gonna say it..again..DUMB BITCH! \#MKR''.\vspace{5mm}
%
% As online platforms seek to remove abuse occurring on their platform, data sets that have been gathered and annotated may have the abusive documents removed, thus requiring several rounds of re-annotation of abusive language. In an attempt to deal with this, we will experiment with using documents that are assumed have a higher chance of being abusive as they are posted in forums that are known to abusive. Using these documents we will seek to build different forms of embeddings and evaluating on previously annotated data. Further, in an attempt to mitigate annotator needs, we will build an abusive language potential system which utilizes supervised methods for Named Entity Recognition (NER), Gender Identification \citep{Sap:2014}, and sentiment analysis amongst other methods. The goal of this is to identify the probability that a document has potential to contain abusive content, in efforts to exact greater control over what documents an annotator is faced with. Additionally, such a system will allow us to identify documents which are clearly abusive. Thus, we will be able to provide an automated method to create a seed set of positive documents for abusive language detection.
\section{Abusive Language Detection}

% Introduce the field of abusive language detection
In recent years, the computational study of online abuse has seen a rapid increase in the number of papers dedicated starting with a handful of papers prior to $2016$ to a thriving research field with numerous papers, shared tasks, and workshops \citep{Vidgen:2020}. In spite of the growth in research dedicated to the detection of online abuse, the research field is still in its infancy with a number of open questions, including questions around definition of the task, annotation guidelines, and modelling techniques.
The earliest work in the field sought to address questions of cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with sparing focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified abuse has surfaced as an independent task \citep{Tulkens:2015,Waseem:2016,Waseem-Hovy:2016,Park:2017,Samghabadi:2017,Karan:2018,Gorrell:2018,Stoop:2019,Meyer:2019,Palmer:2020,Vidgen:2020}. As a consequence of increased visibility of hate speech and abuse on online platforms, the academic inquiry into the computational detection has grown along with the regulatory responses \citep{Regulatory stuff: NetzDG, EUcommision on hate speech}.

Early, and contemporary computational work, has seen a great deal of focus to central questions around the task: how do we annotate and create datasets \citep{Waseem-Hovy:2016,Waseem:2017,Vidgen:2020} and understanding annotator interaction and performance \citep{Ross:2016,Waseem:2016,Vidgen:dynabench:2021}.
Early work focused on questions of marginalisation and oppression, for instance through the work of \citet{Waseem-Hovy:2016} who base their annotation on works in gender studies and critical race theory, and collect data based on gendered and racialised abuse; more recently data collection and annotation processes have moved towards a demographically blind process. Such early work was inspired by the marginalisation of certain bodies and the desire to develop computational tools to protect marginalised people \citep{Warner:2012}.marginalisation of certain bodies and the desire to develop computational tools to protect marginalised people \citep{Warner:2012}.
More recent work has instead directed its focus to demographically blind approaches to data collection and annotation, succumbing to ``marginalisation-blind'' annotation processes and guidelines. Although processes that do not take marginalisation into account, but instead seek to treat every group equally provide an allure of fairness, they also encode dominant discourse on abuse with the subsequent result of the resistance to oppression and marginalisation being treated the same as marginalisation. In concert with the growing evidence of racially biased content moderation tools \citep{Waseem:2018,Davidson:2019}, demographically blind annotation criteria and data curation pose a threat to the goal of developing tools that aid in ensuring people from the right from persecution. One such example is presented by \citet{Salminen:2018} who develop a taxonomy that includes ``anti-white'' as a target of hate on par with anti-Black hate in spite of whiteness as a hegemonic entity that marginalises \citep{McIntosh:1988}.
A result of this are egregious annotation choice, such as ``The  white  will  always  steal;  FUCK  YOU  TO  ALL  WHITES  RACIST'' labelled as hate speech \citep{Salminen:2018}, in spite of the comment speaking to ongoing racism and the historical exploitation enacted by white societies (e.g. the theft of cultural artefacts from colonised territories \citep{Frost:2019}, the numerous genocides committed by imperialistic colonial states \citep{Weisbord:2003}, and the theft of bodies in the transatlantic slave trade). Moreover, and perhaps of even greater concern, the annotation and curation processes of \citet{Salminen:2018} result in data responding to the abuse of authority committed by police as hate, in one such example they identify the following comment as hate ``did to that poor guy. 10 s of pepper spray directly into the face, run over foot etc. equal it up a little bit, except for the detail of having a fucking stroke. So it still wouldn’t be exactly what the guy went through. Fucking discusting. They get a hard on power tripping others. They are just fucking cowards'', in all likelihood due to the aggressive nature of the comment.

Through such demographically uninformed processes of curating and making data, a danger of erasure of past and ongoing marginalisation and responses to it as well as critical responses to the violent abuse of authority as ``hate speech'' that should be subject to content moderation. The question of automated hate speech detection thus, for works such as \citet{Salminen:2018} is no longer ensuring the right to not be persecuted but instead insuring that processes of marginalisation remain unchallenged.
For these reasons, I use the datasets released in early work, specifically I use the \textit{Offence} dataset \citep{Davidson:2017}, \textit{Toxicity} dataset \citep{Wulczyn:2017}, and \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} in all computational chapters. For \autoref{chap:liwc} which examines the influence and generalisability of vocabulary manipulation, I also use the \textit{Hate Expert} \citep{Waseem:2016} and the \textit{StormFront} \citep{Garcia:2019} datasets. Each of these datasets share the common attributes that they are collected either from spaces that are hateful towards marginalised groups or have considerations of marginalisation encoded into the annotation guidelines. In \autoref{chap:mtl} I also use three datasets that are labelled for abuse but instead to tasks that are seemingly related. First, I use the \textit{Argument Base} dataset \citep{Oraby_factfeel:2015}, the second dataset (\textit{Sarcasm}) is developed for sarcasm detection \citep{Oraby_sarcasm:2016}, and the final dataset, \textit{Moral}, examines the moral sentiments
In an early effort to address issues of annotator biases and under-sampling of some forms of data in the data curation process, \citet{Waseem:2017} propose a typology of abuse that aims to categorise abuse by how it is characterised rather than determining the exact form of abuse.
To this end, \citet{Waseem:2017} present a 2-dimensional typology of hate; the first dimension operates along implicit and explicitly expressed hate. Implicitly communicated hate, \citet{Waseem:2017} argue is hate that is communicated through subversive means by using code words and communicating implicit biases. Explicit abuse on the other hand is explicit in its intention to abuse, e.g. through the use of slurs. The second dimension concerns itself with the target of abuse that can either be a generalized other, or a specific group, the former category detailing abuse that is targeted towards small groups and individuals while the latter is aimed at generalised targets, e.g. larger demographics.
It's important to note that content may be simultaneously explicit and implicit, directed and generalised \citep{Waseem:2017}. For instance, content that implicitly targets Muslims, may simultaneously explicitly target a specific group of women.

\begin{table*}[ht]
\centering
\begin{tabular}{p{\textwidth/30}|p{0.45\textwidth}|p{0.45\textwidth}}
  & \textit{Explicit}    & \textit{Implicit} \\\hline
    \multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Directed}}}    &   {\scriptsize``Go kill yourself'',  ``You're a sad little f*ck'' \citep{Hee:2015a}}, \newline {\scriptsize ``@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga''  \citep{Davidson:2017}}, \newline {\scriptsize ``Youre one of the ugliest b*tches Ive ever fucking seen'' \citep{Kontostathis:2013}}. & {\scriptsize ``Hey Brendan, you look gorgeous today. What beauty salon did you visit?'' \citep{dinakar2012common}, \newline ``(((@User))) and what is your job?  Writing cuck articles and slurping Google balls?  \#Dumbgoogles'' \citep{Hine:2016},\newline  ``you're intelligence is so breathtaking!!!!!!'' \citep{dinakar2011modeling}}\\\hline
  \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{Generalized}}} & {\scriptsize``I am surprised they reported on this crap who cares about another dead n*gger?'', ``300 missiles are cool! Love to see um launched into Tel Aviv! Kill all the g*ys there!'' \citep{Nobata:2016}, \newline ``So an 11 year old n*gger girl killed herself over my tweets? \^ \_ \^\ thats another n*gger off the streets!!'' \citep{Kwok:2013}}. & {\scriptsize``Totally fed up with the way this country has turned into a haven for terrorists. Send them all back home.'' \citep{burnap2015cyber}, \newline ``Gas the skypes'' \citep{magu2017detecting}, \newline ``most of them come north and are good at just mowing lawns'' \citep{dinakar2011modeling}} \\
\end{tabular}
  \caption{Typology of abusive language presented by \citep{Waseem:2017}.}
\label{tab:typology}
\end{table*}

Modelling for automated hate speech detection has also undergone a development from early to contemporary work. Early work was primarily focused on feature-based modelling \citep[e.g.][]{Waseem-Hovy:2016,Waseem:2016,Davidson:2017,Sahlgren:2018} whereas subsequent work has directed a greater attention to neural network based approaches \citep[e.g.][]{Kolhatkar:2021,Waseem:2018,Gamback:2017,Badjatiya:2017}. In this dissertation I follow a similar pattern of developing baseline models from feature-based models and suggest neural network architectures as extensions and improvements on these. In early work, Logistic Regression (LR) and Support Vector Machines (SVM) were the most frequently used models. As the scholarship has developed, specific types of neural networks have come to dominate the modelling, namely Convolutional Neural Networks and Long-Short Term Memory networks. In each chapter, I perform the review of the models that are pertinent to the work in the chapter. Here instead I provide a theoretical overview of the models, their components (e.g. dropout and activation functions) and their intended functionalities (i.e. the kind of data that they are designed to operate on and which assumptions are built into the model architectures).

\subsection{Datasets}\label{sec:datasets}

Here I provide an overview of the different datasets that are used throughout this dissertation. For each dataset, I introduce the curation rationale, the source of the datasets, the annotation guidelines, annotator selection, and finally how each of these dimensions influence the resulting dataset. In \autoref{sub:abuse_data} I describe the datasets annotated for hate speech and abuse and then in \autoref{sec:mtl_data}, I turn to the datasets used for the auxiliary tasks for \autoref{chap:mtl}.

\subsubsection{Hate speech and abuse datasets}\label{sub:abuse_data}

\paragraph*{Hate Speech} Published as the first publicly available dataset for hate speech and abusive language detection, \citet{Waseem-Hovy:2016} developed a dataset for detecting abuse towards gendered and racialised minorities. In an interview in the Let's Chat Ethics Podcast, Zeerak Waseem shared that the initial motivation for developing the dataset was the somewhat na{\"i}ve hope to address online harassment as experienced by women during \#GamerGate, a harassment campaign against female game developers and games journalists \citep{Massanari:2015}. This aim of developing tools that can protect marginalised people is apparent in the data sampling and the source of data. As a large amount of the GamerGate abuse occurred on Twitter, \citet{Waseem-Hovy:2016} use Twitter as a source of their data, collecting $16,914$ tweets labelled as ``sexist'', ``racist'', and ``neither''.
In efforts to ensure that their collected and annotated sample contains gendered and racialised abuse, they bootstrap their corpus collection by first search for common slurs against women, ethnic minorities, religious minorities, and sexual minorities to identify the salient terms and users for scraping.
To annotate this dataset, with the target group in mind, \citet{Waseem-Hovy:2016} develop $11$ questions to test whether a comment is hateful or not.
This set of questions focuses on breadth in the types of hate expressed rather than depth in each type. This is apparent as the tests ranges from asking about explicit forms of hate, such as the use of slurs to implicit forms like questions around stereotyping and the use of straw man arguments in criticisms of minorities.
\citet{Waseem-Hovy:2016} annotate their dataset and have their annotations verified by an external annotator.
Collectively, these decisions are made to ensure that there was a diversity in the forms of hate in addition to the sources. However, as they note, the racist abuse only comes from $9$ different accounts. Moreover, as salient terms were sampled for annotation, some terms (i.e. the hashtag for the Australian TV show My Kitchen Rules) are over-represented in the data. In spite of these issues, the annotations in this dataset are \textit{embodied} within the context of critical race and gender studies perspectives on abuse.

\paragraph*{Hate Expert} In an extension of the dataset proposed by \citet{Waseem-Hovy:2016}, \citet{Waseem:2016} sample $6,909$ tweets from the original scrape and have it annotated by two groups, in efforts to understand the influence of annotator biases. The first group consisted of ``feminist and anti-racism activists'' \citep{Waseem:2016} who annotate the sample of the dataset with one of four labels ``racist'', ``sexist'', ``both'', and ``neither''. The second group of annotators were recruited from CrowdFlower to re-annotate the sample annotated by the first group.\footnote{CrowdFlower has since been renamed Appen.}
The label set was expanded by \citet{Waseem:2016} to include the category ``both'', in acknowledgement that marginalisation can be expressed across multiple dimensions, in an \textit{Intersectional} manner. Comparing models trained on each group of annotators, \citet{Waseem:2016} find that models that use the annotations of the first group consistently outperform models trained on the second. \citet{Waseem:2016} argue that the reason for this difference is that models trained on the former group benefit from similarities in the understanding of hate speech, whereas the distinct subjective positionalities of the latter group, that does not have a salient unifying characteristic beyond their work on a crowd-sourcing platform, produces internally inconsistencies in labelling that render it harder for models to consistently identify patterns that they can learn from.
In presenting this dataset, \citet{Waseem:2016} propose that ideologues can take similar positions on a topic, given their subjective positionalities. They argue that only through a principled understanding of hate speech is it possible to annotate reliably for hate speech and that crowd-sourced annotations for hate speech display inconsistencies that to some degree erases the meaning of the term.
In using this dataset for this dissertation, I use the annotations provided by the feminist and anti-racist activists.

\paragraph*{Offence} Departing from the question of forms of hate speech and gender studies and critical race theory based annotation guidelines, \citet{Davidson:2017} turn instead to ask where the distinction between simply ``offensive'' speech and ``hate speech''.
Using a list of terms from Hatebase\footnote{Hatebase.org is a website that crowd-sources slurs and insulting turns of phrase. Due to marginalised people being disproportionately targeted, there is a distributional skew towards terms that target marginalised people in the number of terms.} to identify $33,458$ users whose tweets they sample.
From these users, they randomly sample $25,000$ tweets for annotation by CrowdFlower workers, resulting in $24,802$ annotated tweets. The crowd-workers were given guidelines to aid them in distinguishing between ``offensive content'', ``hateful content'' or ``neither offensive or hateful'', selecting only one for each tweet. \citet{Davidson:2017} instruct their annotators that hate speech is speech that ``is used in reference to certain groups that expresses hatred towards the group or is intended to be derogatory, to humiliate, or to insult the members of the group''. Moreover, they provide examples of such content which includes the straightforward ``Need to send these w******s back to their country''\footnote{Censoring of the slur is mine.} and the more conflicted ``I hate white trash''. The conflict in the latter stems from it being unclear whether the emphasis is on class-based hate or if it is targeting white people. While the former is less contentious, the latter would imply that white people too are targets of marginalisation on the basis of their race. However, as numerous scholars have argued, whiteness is the hegemonic force that marginalises \citep{CRT scholars}.
``Offensive'' content is provided as an alternative, less serious form of potentially unwanted content. This group is defined in contrast to the hateful class ``[o]ffensive content might use some of the same words we associate with hate speech but do not \textit{necessarily} constitute hate speech because the words are not used in the same context as ``hate speech````.\footnote{Emphasis added.}
From this definition it's clear that in spite of the instruction to select only one category, \citet{Davidson:2017} acknowledge that there is a potential overlap between offensive language and hate speech. 
Moreover, unlike the annotation guidelines proposed by \citet{Waseem-Hovy:2016}, the definition of offensive draws in the question of context. Illustrating this point, \citet{Davidson:2017} provide the following example ``Oh shush you know I love you f****t''. This use of context, provides space for inoffensive uses of slurs and insulting terms e.g. for reclaimed and in-group uses, a space that the annotation guidelines of \citet{Waseem-Hovy:2016} does not afford.
With the annotators being selected from CrowdFlower, the issues of multiple distinct ideologues in the annotator pool raised by \citet{Waseem:2016} are likely also manifest in this dataset.
However, as the dataset offers a space within which one can utter offensive but not hateful messages, it also offers the space to live spaces that dominant discourse on acceptability of language use would deem as unacceptable.\footnote{By dominant discourses on acceptability, I refer to what mainstream discourses deem as acceptable and unacceptable manners of speaking. However, such a discourses are internally inconsistent, as \citet{Oliva:2020} show, acceptable speech can come to include neo-nazi and white supremacist speech that threaten social cohesion while deeming speech by queer communities as toxic and inherently holding greater threat to the boundaries within which society should operate.}
In consideration of the marginalisation of queer people and people of colour, this dataset thus offers space for their uninterrupted existence. However, as the dataset is labelled for a multi-class classification problem, where a single label is assigned to each document, the dataset does not afford space to be exposed to hate speech in such contested spaces.

\paragraph*{Toxicity} Starting from a similar point as \citet{Davidson:2017}, \citet{Wulczyn:2017} develop a dataset  of $115,737$ comments to understand which types of conversations are likely to make users depart from the conversation. Departing from the early tradition of using Twitter as a source of data, \citet{Wulczyn:2017} consider the Wikipedia editor discussion pages. Taking a narrow view of behaviours that inhibits participation in conversations, \citet{Wulczyn:2017} focus on personal attacks and harassment, specifically asking their annotators whether which entity (the participant or a third party) is the subject of the attack. As a last positive category, they include whether it is ``[a]nother kind of attack or harassment'', thus relegating all forms of harassment that are not directed at specific individuals to a residual category. The dataset thus is comprised of ``personal attacks'' and ``other forms of harassment''. As the study is specifically grounded in identifying personal attacks, this categorisation of various forms of personal attacks and a residual category as positive instances is in line with the aims of the data, if not the description.
Using this definition, \citet{Wulczyn:2017} select a random sample of $37,611$ and have it refereed by $10$ annotators for personal attacks, resulting in only $0.9\%$ of the labelled data being assigned the positive label. Subsequently, they identify $78,126$ comments that had been made by users whose comments were moderated from the discussion pages, for each user taking $5$ comments that they made around the moderated comment, and similarly subject them to annotation this time resulting in the positive class occupying $16.9\%$ of the labelled data.
Similarly to \citet{Waseem:2016} and \citet{Davidson:2017}, \citet{Wulczyn:2017} use CrowdFlower to obtain their annotations and subsequently are prone to similar issues in their data. However, to curb such issues they obtain $10$ annotations for each comment, allowing to compute a majority vote that takes a broader perspective on the comment into account. In spite of this approach, where those annotators are from and what their position on personal attacks are, and their ability to identify subtle attacks, still remain uncertain resulting in a dataset that may take a global position or a culturally grounded position on identifying personal attacks, e.g. if a large subset of annotators live in India, a subset of the data may very well reflect Indian perceptions of personal attacks.
The resulting dataset has been constructed to understand which comments are likely to turn discussions ``toxic'' as a result of personal attacks. Through the use of $10$ annotators for each comment, \citet{Wulczyn:2017} aim for a global understanding of toxicity derived, in part, from personal attacks.
Similarly to \citet{Davidson:2017}, there appear to be no consideration of the experiences of abuse against marginalised communities.
Considering Wikipedia's well documented issues with being a hostile space to women \citep{Torres:2016} and the distribution of gender crowd-workers often veering towards a greater representation of men than women \citep{Posch:2018}, the lack of such a consideration may further entrench subjective positions that are hostile towards women into the datasets and subsequently into the models.

\paragraph*{StormFront} Focusing on the white supremacist web-forum StormFront, \citet{Garcia:2019} collect a dataset of $10,568$ sentences annotated by three of the authors for containing hateful utterances. Similarly to \citet{Davidson:2017} and \citet{Wulczyn:2016}, \citet{Garcia:2019} employ a marginalisation-blind definition and understanding of hate speech. In the case of a white supremacist web-forum, employing a marginalisation-blind definition is unlikely to be challenged as the participants are unlikely to engage in derogation against white, straight, cisgender men.
The decision for using StormFront as a source of data was motivated by the prevalence of ``pseudo-rational discussions of race''.
Moreover, this dataset further distinguishes itself from prior datasets by annotating on a sentence level.
The authors argue that annotating on a sentence level can reduce the confounding factors by only addressing content which is explicitly hateful.
While this may, in some instances have little effect as the surrounding sentences bear no impact on whether a sentence is hateful. This particularly holds for explicit hate speech.
However, for subtle forms of hate speech, conducting sentence level annotation may obscure hate that is only apparent when considering a post in its entirety rather than its sentence level components. In order to address this issue, \citet{Garcia:2019} introduce a ``related'' tag which is to be used when individual sentences do not convey hate but the combination of several sentences in sequence do convey hate.
This method for mitigation does not account for longer sequences of sentences that convey hate, as is often the case for subtle forms of hate speech and dog whistles.
Moreover, as \citet{Garcia:2019} take a very conservative position on what constitutes hate, for instance, the use of a derogatory term, on a white supremacist web-forum, ``cannot be said to be a deliberate attack, taken without any more context, despite it likely being offensive.'' For this reason, \citet{Garcia:2019} argue that simply the occurrence of slurs weaponised against marginalised communities cannot be said to be hateful.
Thus, while initially side-stepping the issue of marginalisation-blind definitions by sourcing data from a white supremacist web-forum, it is softly reintroduced by taking a conservative stance on what constitutes hate.

\subsubsection{Non-abuse datasets}\label{sec:mtl_data}

\paragraph*{Sarcasm} \citet{Oraby_sarcasm:2016} develop a dataset for sarcasm detection in dialogues. The dataset was developed in order to address the lack annotation for subtypes of sarcasm, i.e. rhetorical questions and hyperbole, at scale in previous datasets.
Sourcing their data from the Internet Argument Corpus (IAC) \citep{Abbott:2016}, \citet{Oraby_sarcasm:2016} annotate their data for ``generic sarcasm, rhetorical questions, and hyperbole''.
In order to generate a dataset from the IAC, \citet{Oraby_sarcasm:2016} train a ``weakly-supervised pattern learner'' \citep{Oraby_sarcasm:2016} to identify a set of $30,000$ posts, filtering two thirds of the posts that don't contain any ``not-sarcastic'' cues and annotate the remaining $11,040$ posts in quote-response pairs for annotation on Amazon Mechanical Turk.
Similarly to the abusive language datasets annotated on CrowdFlower, this choice of annotators can introduce biases stemming from the subjective embodiments of the human annotators and the geo-cultural contexts in which they exist.
Following the annotation process a dataset of $6,520$ posts (with a $50\%$ split of sarcastic and not-sarcastic posts) is obtaned and released.
Examining the dataset for suitability for machine learning experiments, \citet{Oraby_sarcasm:2016} train a linear SVM with Stochastic Gradient Descent (SGD) training and L2 regularisation obtaining F1-score of $0.74$ using features derived from Word2Vec \citep{Mikolov:2013}.

\paragraph*{Argument Basis} Investigating the characteristics of factual and emotional argumentation styles, \citet{Oraby_factfeel:2015} also draw on the IAC as the source of data. Considering quote-response pairs, each response is annotated for whether the argument presented in the response based primarily in fact or feeling.
\citet{Oraby_factfeel:2015} present $10,003$ from the IAC for annotation by $5-7$ crowd-workers on Amazon Mechanical Turk for annotation selecting a value ranging from $-5$ to $5$ to indicate whether the response is a feeling or fact-based argument, where negative values indicate that the argument basis is dominated by an emotional argumentation style and positive values indicate a fact-based argument.
Each document is then given a binary label indicating its argument basis, where all texts with a score greater than $1$ are assigned as fact-based, all texts with a score lower than $-1$ are assigned to the feeling-based class, and all scores $[-1, 1]$ are discarded.
This annotation process results in $3,466$ fact-based and $2,382$ feeling-based documents.
Similarly to the previously examined datasets that utilise crowd-workers, this dataset is also subject to the contexts which the individual annotators exist within. For instance if an annotator is from a culture where feeling-based argumentation is not experienced as impassioned but instead supportive of facts, they may be likely to rate some documents as more fact-based than annotators who hail from cultures that emphasise fact-based argumentation would deem as relying on an emotional argumentation style.
The subjectivity of the annotation task may provide an explanation for why $4,155$ or more than $41\%$ of the documents are discarded due to being rated, in aggregate, as dominated by neither fact or emotion.

\paragraph*{Moral Sentiment} The final dataset used in this dissertation is the Moral Foundations Twitter Corpus \citep{Hoover:2019}. This dataset provides $35,108$ tweets annotated for $10$ different categories of moral sentiment, introducing the task of moral sentiment prediction.
A task, and dataset designed to allow psychology researchers to investigate the relationship between comments made around events and the moral foundations found in such comments made on Twitter.
% This task was introduced to explore the utility of NLP modelling techniques for psychology research, namely measuring psychologically relevant psychological constructs in tweets.
\citet{Hoover:2019} draw from research in psychology around human morality using a five-factor taxonomy that reveals insights into the moral foundations \citep{Graham:2009,Graham:2013} that underlie comments about and attitudes towards topics.
Each of the five factors are represented through a binary, where one end of the binary represents a virtue and is contrarian to the other, representing a vice.
\citet{Hoover:2019} argue that the human expression of vice and virtue are distinguishable from one another through distinct language use for each.
The five factors introduced are \texttt{care}, ``concerns related to caring for others'' and \texttt{harm}, ``concerns related to not harming others''; \texttt{fairness}, ``concerns related to fairness and equality'' and \texttt{cheating}, ``concerns related not not cheating or exploiting others''; \texttt{loyalty}, ``concerns related to prioritising one's ingroup'' and \texttt{betrayal}, ``concerns related to not betraying or abandoning one's ingroup''; \texttt{authority}, ``concerns related to submitting to authority and tradition'' and \texttt{subversion}, ``concerns related to not subverting authority or tradition''; \texttt{purity}, ``concerns related to maintaining the purity of sacred entities, such as the body or a relic'' and \texttt{degradation}, ``concerns focused on the contamination of such [sacred] entities.''
Noting that there is low occurrence of moral sentiments expressed in a random sample of tweets, \citet{Hoover:2019} collect tweets related seven different discourse domains where the occurrence of moral sentiment is likely to at a high rate: Black Lives Matter, All Lives Matter, Baltimore protests following the death of Freddie Gray, the 2016 presidential elections in the United States of America, hurricane Sandy, the \#MeToo movement, and offensive language, re-annotating a sample of \citet{Davidson:2017} for the moral sentiments.
For annotation, \citet{Hoover:2019} train $8$ undergraduate research assistants to an expert-level familiarity with the moral foundations taxonomy, annotating $4,000 - 6,000$ for each discourse domain. The annotators are trained through a training sessions and, in early stages, discussion surrounding annotator disagreement.  
The annotator selection procedure here thus develops on the suggestion of \citet{Waseem:2016} to use expert annotators to describing a means of training expert annotators for a highly subjective task. Interestingly, as the annotation process continues past early stages, annotator disagreements are not resolved, instead the authors opt for expressing the inherent subjectivity of the human annotation task.

\subsubsection{Non-English datasets for abuse}
\zw{See hatespeechdatasets.com and ACL anthology}

In this dissertation, I focus my attention to detecting abuse in English language datasets as my methods do not map to other languages. However, an important growing body of research and resources are being developed for other languages such as Arabic \citep{Arabic abuse papers}, Chinese \citep{Chinese abuse papers}, Croatian \citep{Croatian papers}, Danish \citep{Danish abuse data}, French \citep{French papers}, German \citep{German papers}, Greek \citep{Greek papers}, Indonesian \citep{Indonesian papers}, Italian \citep{Italian papers}, Polish \citep{Polish papers}, Portuguese \citep{Portuguese papers}, Slovene \citep{Slovenian papers}, Spanish \citep{Spanish papers}, Turkish \citep{Turkish papers}, and Urdu \citep{Urdu papers}. Beyond these mono-lingual resources and approaches, there is also a body of work dedicated to abuse in code-switching contexts \citep{Code switching papers}.

Developing models for each individual language, and in particular resources that address abuse that code-switches, require an attention to the particularities of the different languages and cultures, just as model development for English requires researchers to be attuned to the particularities and cultures represented in English language use.


\subsection{Generalisable Machine Learning Models for Abusive Language Detection}
A common criticism of many current computational methods for abuse detection is that they have poor generalisability onto other datasets. Although this issue of non-generalisability poses a serious issue for the abuse community, it has received relatively little attention \citep{Waseem:2016,Waseem:2018,Karan:2018,Wiegand:2019,Swamy:2019,Fortuna:2021,Glavas:2020} in comparison to single-dataset classifier performance. In each of the computational chapters (see \cref{chap:liwc,chap:mtl}), I also provide consideration of how well the trained models perform on out-of-domain datasets.
In the pursuit of models that generalise well onto other datasets, researchers have proposed a variety of architectures. As an initial investigation into the question of generalisability, \citet{Waseem:2016} note that the best performing classifier on the dataset they propose does not generalise well onto the \textit{Hate Speech} dataset, noting that the performance of their classifier drops by more than a $25\%$.
Using Multi-Task Learning (MTL),\footnote{Multi-task learning allows for training models using multiple different datasets, for distinct machine learning tasks, where one (main) task is given higher priority and all other tasks are treated as auxiliary tasks.} \citet{Waseem:2018} address the issue of poor generalisability between the \textit{Hate Speech} and \textit{Hate Expert} (combined into a single dataset) and the \textit{Offence} dataset, showing that a MTL framework can be used for training models that can generalise onto from one cultural context onto another. Moreover, considering the results posted by \citet{Waseem:2018}, it appears that there is a trade-off between well-performing in-domain models and well-performing cross-domain models, where cross-domain improvements appear to come at the cost of in-domain performance, where out-of-domain performance is computed by mapping the classes in the in-domain datasets to the out-of-domain dataset.

\citet{Karan:2018} further explore the question of cross-dataset generalisability using a linear SVM model. \citet{Karan:2018} approach the task of out-of-dataset performance as a classical domain adaptation task and use the FEDA framework \citep{Doume:2007}, finding that without significant procedures for domain adaptation, there is poor generalisability. Similarly to \citet{Waseem:2018}, \citet{Karan:2018} find that cross-domain performance comes at the cost of in-domain performances but with large out-of-domain improvements. One difference between \citet{Karan:2018} and the previously described studies is that \citet{Karan:2018} reduce the learning task to a binary classification task of ``abusive'' and ``non-abusive'' documents.

The last approach to generalisation I consider is the work by \citet{Fortuna:2021}. In this paper, the authors compare four different models for out-of-domain classification: a Bag-of-Words SVM model, a Continuous Bag-of-Words FastText model, a BERT model \citep{Devlin:2019}, and an ALBERT model \cite{Lan:2020}. The latter two being transformer-based language-models that are fined-tuned to the task of predicting abuse.
\citet{Fortuna:2021} propose a different class organisation to past studies, first they propose as generalised class organisation that collapse classes across datasets into a smaller, generalised subset that maps across datasets. For instance, the ``sexist'' class provided by \citet{Waseem-Hovy:2016} and the ``misogyny'' class provided by \citet{Fersini:2018} into a ``misogyny-sexism'' class. Each of the generalised classes are binarised to allow models trained with other standardised labels to predict on them.
Using these generalised classes, \citet{Fortuna:2021} show that by using methods that capture more complex word-interactions, out-of-domain performance generally improves within and out of domain, subject to the classification task.
Specifically, they find that when classes have significant overlaps across datasets in their rationalisation of what the are to represent then models trained on those classes will map well onto the rest.
Conversely, when the classes have a little overlap, the models will generalise poorly onto the new dataset.
Moreover, \citet{Fortuna:2021} identify that some dataset combinations produce poor generalisation between each other regardless of the models used.
This, in concert with their conclusion that dataset overlap and out-of-domain similarities are drivers of model generalisation has two implications.
First, current computational models can, to some degree, adapt onto new distributions and samples but models using words as input are poorly suited for learning general trends of a wide variety of abuse, including closely related concepts such as ``toxicity'' and ``severe toxicity'' \citep{Fortuna:2021}.
Second, as models do not generalise onto other concepts, even if closely related, research in the detection of online abuse must either develop methods that can generalise onto studying different objects and perspectives of online abuse, or datasets must be annotated following highly similar annotation guidelines at the cost of the depth and breadth of concepts that can be explored.

\section{Models}\label{sec:model_background}
In this section I provide an introduction to the different modelling techniques that I use throughout the dissertation.

\subsection{Data Encoding}

In order for models to read the data, it is necessary to provide the models with machine readable representations of the data. The first step to creating such machine readable representations is to provide each unique token with a numerical index.
The numerical index, and what it represents is a matter of how the data is pre-processed. For instance, in \cref{chap:liwc}, I represent tokens in three different ways.
First, I represent tokens using their surface form, that is each word is represented in its entirety following a tokenisation process where all words are lower-cased and punctuation markers are split from the word (see \cref{chap:liwc} for further pre-processing steps). Second, I take the surface forms of tokens computed and represent them as the categories of the Linguistic Inquiry and Word Count (LIWC) categories each token induces (see \cref{chap:liwc} for further detail). Finally, in \cref{chap:liwc,chap:mtl} I represent tokens as the subwords that they consist of. 
In this section, the sub-word forms while omitting the surface-token and LIWC-token forms as these rely on simple pre-processing and mapping steps that are described in more detail in \cref{chap:liwc}.

\subsubsection{Byte-Pair Embeddings}

Byte-Pair Encodings were introduced to the NLP comunity by \cite{Sennrich:2016} for the task of Neural Machine Translation to address the issue of out-of-vocabulary tokens. In this paper, the authors argue that for word-level machine translation there is not always a one-to-one relationship between a word in the source language and its translation into a target language. \citet{Sennrich:2016} illustrate this point through compound words, where a compound word represents a specific entity that is represented through multiple words in the source language, e.g. the German \textit{Abwasser\textbf{|}behandlungs\textbf{|}anlange} and its English translation \textit{sewage water treatment plant} \citep{Sennrich:2016}.
\citet{Sennrich:2016} propose to compute sub-words using the byte-pair encodings algorithm proposed by \citet{Gage:1994}. While the algorithm proposed by \citet{Gage:1994} operates on bytes and seeks to develop a new representation of bytes that can compress their representation, \citet{Sennrich:2016} seek to operate on the sub-units of words, that is a sequence of characters. In both cases, the algorithm operates by considering the input and identifying frequently occurring patterns that can be represented in terms of a single unit.
In efforts to obtain a sub-word representation, \citet{Sennrich:2016} initialise their algorithm initially with a vocabulary consisting of each unique character token in the dataset and then count all symbol pairs (e.g. character co-occurrences) and merge the most frequently occurring pairs and adding it to the vocabulary. This merging process is repeated a number of times, where the total number of merge operations is a hyper-parameter set by the designer of the sub-word representation. The size of the vocabulary following this process will be the size of the original vocabulary plus the number of merge operations that are set by the designers \citep{Sennrich:2016}
In terms of language, using sub-words to represent documents can minimise the number of out-of-vocabulary tokens in the validation and evaluation sets of a dataset, as the likelihood of a word not being represented decreases as it is broken down into its subwords.

In this dissertation, I use the pre-trained Byte-Pair Embeddings (BPE) developed by \citet{Heinzerling:2018}. These embeddings were trained for $275$ languages using the Wikipedia pages in each language as the source of data. \citet{Heinzerling:2018} provide embeddings for $1,000,\, 3,000,\, 5,000,\, 10,000,\, 25,000,\, 50,000,\, 100,000$, and $200,000$ merge operations with dimensions $25,\, 50,\, 100,\, 200$, and $300$. For all chapters in this dissertation, I choose the $300$ dimensional embeddings that have been subject to $200,000$ merge operations

\subsection{Strategies against over-fitting}\label{sec:anti-overfit}

Machine learning models are prone to identify salient patterns in the training data with the result that they perform poorly on evaluation sets and out-of-domain data. In order to address this issue, I use a number of different techniques depending on the type of model used.
For linear models, I experiment with three different regularisers: L1 regularisation, L2 regularisation, and Elasticnet.
For neural networks, that by virtue of their ability to identify and represent complex interaction patterns are prone to overfit, I use two different techniques, namely early stopping and dropout.

\paragraph*{L1 Regularisation} L1 regularisation operates by iteratively zeroing out uninformative features in order to produce a more sparse representation of the data while minimising loss the performance of a given model \citep{L1 regularisation paper}. For instance, if there are two features $x_1$ and $x_2$ that both carry an equal weight towards the same class, one of the features will be zeroed out while the other will retain its weight. While this can be helpful in an in-domain setting, it may not be quite as useful when the model is used on new data, in cases where $x_1$ exists in the document to be classified but is zeroed out $x_2$ does not occur in the document.

\paragraph*{L2 Regularisation} To address this short-coming, $L2$ regularisation is proposed \citep{L2 regularisation paper}. $L2$ regularisation seeks to penalise weights of features, making the weights smaller, rather than altogether zeroing out any weights. This penalisation and reduction of weights by $L2$ regularisation seeks to minimise across all features. Thus $L2$ regularisation does not necessarily zero out any individual feature but instead reduce the weight of all features to prevent over-fitting to any particular set of features.

\paragraph*{Elastic Net} Elastic Net seeks to combine $L1$ and $L2$ regularisation into a single regularisation function \citep{Elastic net paper}. Thus, elastic net seeks to both zero out uninformative features and minimise the weight of all features to reduce variance between them. Elastic Net is particularly fitting in modelling contexts where there is a high dimensionality in the data, for which reason it is desirable to reduce the size of the feature space while retaining a maximum number of features that are informative towards the prediction task.

\paragraph*{Dropout} In order to prevent neural networks from over-fitting on training data, \citet{Dropout paper} introduce the notion of dropout. Dropout refers to randomly zeroing out some values of a model's internal representation between different layers. The idea behind dropout is that models may over-fit to individual tokens or interaction patterns between tokens, thus to prevent the model from learning such patterns, one can randomly zero out values in the internal representations of a document as it is passed through the layers of the model.
Such zeroing out forces the model to adapt to different representations for a given document each time it is passed through the model and, hopefully, learn general patterns rather than ones that occur from spurious correlations in the data.

\paragraph*{Early Stopping} A second method for addressing over-fitting in neural network models is the idea of early stopping \citep{Early stopping papers}. Early stopping, in terms of neural networks, means to end a training cycle before it passed over the data for the number of epochs specified by the researcher. The idea behind early stopping is that a model may identify an optimal representation before it the maximum number of epochs has been reached. Any further optimisation processes on the model representation are thus likely to have a detrimental effect to a model's performance on the evaluation set.
While a number of different approaches to identifying when to trigger early stopping have been proposed \citep{early stopping papers}, in this dissertation I trigger early stopping by considering the development of model loss. Specifically, if the model loss monotonically increases for a set number of epochs, I trigger early stopping as this indicates that the model has already identified a representation that minimises the loss.

%
% As embedding layers are layers that are optimised, it is unnecessary for the researcher to perform feature selection \cite{CITE: Papers that say no feature selection is needed}, although it can be a benefit \cite{CITE: Papers that take feature selection into account}. The process of selecting features for a model to consider implores the researcher to have a firm grasp of the concepts they are seeking to examine. This required intentionality of the researcher also comes with the risk that the researcher may, intentionally or unintentionally, omit features that illuminate a pattern in the data that the machine learning model can take advantage of.  Here optimisable representations excel if features are not pre-selected for them, as through multiple rounds of updating the layer, the resulting representation takes advantage of the patterns that emerge from the data.
%
\subsection{Optimisation Techniques}

Training a neural network requires a host of different techniques for optimisation, such that the model can identify optimal minima. Among these are the loss function, the activation function and pooling functions for Convolutional Neural Networks (CNN). Further, in order to identify optimal minima, it may be necessary to train the model with a number of different values for the hyper-parameters (i.e. model parameters such as embedding sizes and parameters for the optimisation functions such as the learning rate) which is also a process that can be subject to optimisation itself. Here, I describe the different optimisation techniques that I use in this dissertation.

Across all neural network models trained for the experiments in this dissertation, I use a \texttt{softmax} function to produce output values representing the likelihood for each class based on the model representations. The \texttt{softmax} function operates by producing taking a vector and producing a value of $[0,1]$ of the vector by computing the normalised exponential function of all the units in the layers (see \cref{eq:softmax} for a mathematical definition for \texttt{softmax}).

\begin{figure}[h]
  \begin{equation}\label{eq:softmax}
    S(f_{y_i}) = \dfrac{e^{f_{y_{i}}}}{\Sigma_j e^{f_j}}
  \end{equation}
  \caption{Equation for the \texttt{softmax} function.}
\end{figure}

Moreover, as the resulting vector sums to $1$ we can treat the values in the vector as a probability distribution where the largest value represents the most likely class.

\subsubsection{Loss}

Broadly, two different types of neural networks exist: feed-forward networks and networks that use back-propagation. Feed-forward networks chronologically update the model on the basis of the data it is provided without concern for how each update to the model's parameters impact the model's ability to perform the classification task. 
Back-propagation \citep{Backprop paper} was introduced as a method with which model parameters could be updated after the forward step of the model had completed and an evaluation of the model's performance with its most recent parameter weights.
By obtaining the model's loss, or model error given by a loss function, one can back-propagate the loss through the model to perform an update to the model's parameters after the completion of the forward step.
In this dissertation, I only make use of back-propagated models with \texttt{Negative Log Likelihood} loss.

\paragraph{Negative Log Likelihood} I choose \texttt{Negative Log Likelihood (NLL)} as a loss function as it is particularly well-suited for use with the \texttt{softmax} function. I provide the definition of \texttt{NLL} as provided by the PyTorch library \citep{Paszke:2019} (see \cref{eq:nll}), where $x$ is the input, $y$ is the class label, $w$ is the weight tensor, $l_n$ is defined as $-w_{y_{n}} x_{n,y_{n}}$ and $N$ is the batch size.\footnote{See \url{https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html} for the implementation details for \texttt{Negative Log Likelihood}.}
Negative log-likelihood operates by by assigning a higher loss to for the correct class for each document on the basis of the probability estimates (obtained through the \texttt{softmax} function) for the class. The higher the probability estimation for the correct class is, the lower the loss is and on the other hand the smaller the probability estimate for the correct class is, the higher the loss is.
By focusing on the probability estimate for the correct, in terms of ground truth, label, \texttt{NLL} avoids the potential issue of assigning all predictions with a correct or incorrect label a specific value. Thus, \texttt{NLL} addresses the model's certainty rather than the prediction itself.

\begin{figure}[h]
  \begin{equation}\label{eq:nll}
    \mathit{l}(x, y) = \sum_{n=1}^{N} \dfrac{1}{\Sigma_{n=1}^{N} w_{y_{n}}}l_n
  \end{equation}
  \caption{Equation for \texttt{Negative Log Likelihood} loss.}
\end{figure}

\subsubsection{Non-linearities}

In the experiments conducted in this dissertation I use two different non-linear functions that I subject various layers in the neural network models to. The role of non-linearities in neural networks is to allow for models to learn non-linear functions rather than linear ones. The two non-linearities that I experiment with are \texttt{Tanh} and \texttt{ReLU}.

The \texttt{hyperbolic tangent}, or \texttt{Tanh}, function (see \cref{eq:tanh} for its mathematical definition) is a non-linear activation function that element wise transforms the values of the tensor representation of the model into a real valued space between $[-1, 1]$. \texttt{Tanh} is a monotonically increasing function that is symmetrical around $0$ due to which there is a risk of the issue of vanishing gradients for the model \citep{Teuwen:2020}.
Vanishing gradients refers to the issue where the gradients of the models become increasingly small, to the point of no longer having an effect on the parameter updates, due to being centred around $0$.

\begin{figure}[h]
  \begin{equation}\label{eq:tanh}
    \tanh{x} = \dfrac{e^x - e^{-x}}{e^x + e^{-x}} 
  \end{equation}
  \caption{Equation for \texttt{Tanh}.}
\end{figure}

One way to address the potential issue of vanishing gradients is to use a \texttt{Rectified Linear Unit} (\texttt{ReLU}) as the activation function (see \cref{eq:relu} for the mathematical definition of \texttt{ReLU}). Unlike the \texttt{tanh} function, \texttt{ReLU} is not a symmetrical function, but instead relies on a binary evaluation of each element in a vector. If the weight of the element under consideration $w_x < 0$, then the value computed is $\mathit{ReLU}(x) = 0$. On the other hand, when the value $w_x > 0$, then the value computed is $\mathit{ReLU}(x) = 1 \cdot w_x$ \citep{Teuwen:2020}.

\begin{figure}[h]
  \begin{equation}\label{eq:relu}
    \mathit{ReLU (x)} = \max{0, x}
  \end{equation}
  \caption{Equation for \texttt{ReLU}.}
\end{figure}

\subsubsection{Pooling layers}\label{sub:pooling}
For CNN models it is necessary to use either average pooling or maximum pooling to summarise the features under a filter. As a summary, pooling operations act as a method for downsampling the feature representation obtained after convolutional layers. Two common kinds of pooling operations are average pooling and maximum pooling.
Average pooling computes the mean value of the pooling features under the filter while maximum pooling extracts the largest value. In my experiments with CNN models, I use maximum pooling exclusively due to its dominance in the CNN models developed for abuse detection \citep{CNN with max pooling papers}.

\subsubsection{Optimisation algorithms}

At the heart of neural networks lie the optimisation algorithms that control the rate and manner in which model weights are updated. A number of different optimisation algorithms have been proposed for neural networks, but in my experiments I focus on two algorithms, \texttt{Stochastic Gradient Descent (SGD)} and \texttt{Adam}. For each of these algorithms, I use the originally proposed algorithms, \texttt{SGD} and \texttt{Adam}, and a variant that address specific short-comings of each algorithms, \texttt{Averaged Stochastic Gradient Descent (ASGD)} and \texttt{Adam with decoupled weight decay (AdamW)}. For all algorithms, we use the implementations used in \citet{Pazske:2019} and refer readers to the PyTorch documentation for further details.\footnote{The API reference can be found at \url{https://pytorch.org/docs/stable/optim.html}.}

\paragraph{Stochastic Gradient Descent}
\texttt{Stochastic Gradient Descent}~\citep{Sutskever:2013} is a popular optimisation algorithm used for neural networks and has been used by a large number of researchers for a diverse set of tasks across various machine learning research areas and tasks, including online abuse detection \citep{Singh:2018,Bodapati:2019}. \texttt{SGD} relies on gradient descent, which is an algorithm that computes the gradients of points on a function until it reaches a minima. This process can be computationally expensive as gradient descent requires a computation on the entire dataset. This approach has two issues: First it is computationally expensive as the computation is performed on the entire dataset; second, gradient descent requires a learning rate being given, which determines the position for the next point at which to compute the gradient. If the learning rate is sufficiently small, and the function under optimisation is not a convex function, gradient descent may identify and settle at a local minima rather than the desired global minima.

\begin{figure}[h]
  \begin{align}\label{eq:sgd}
    v_{t+1} &= \mu \nu_t - \epsilon \nabla f(\theta_t + \mu \nu_t)\\
    \theta_{t+1} &= \theta_t + \nu_{t+1}
  \end{align}
  \caption[Equation for \texttt{Stochastic Gradient Descent}]{Equation for \texttt{Stochastic Gradient Descent} \citep{Sutskever:2014}, where $\epsilon$ is the learning rate, $\nabla f(\theta_t + \mu\nu_t)$ is the gradient, and $\mu$ is the momentum.}
\end{figure}

\texttt{SGD} similarly computes the gradients of points on a function, but rather than computing the gradient descent on the entire dataset, a single example is selected and the gradient descent is computed for that data point, thus minimising the computation time, even when more iterations are necessary to identify the minima. The second issue of local minima is in part addressed by the random nature of selecting a data point to compute the gradient from. This randomness results in greater fluctuations in the development of the gradient, however, this exact fluctuation and variance may allow the algorithm to identify a better minima.

\paragraph{Averaged Stochastic Gradient Descent}
Another approach to addressing the issue of identifying optimal minima is \texttt{Averaged Stochastic Gradient Descent}~\citet{Polyak:1992}. \texttt{ASGD} operates similarly to \texttt{SGD} but considers averaged trajectories in order to accelerate the identification of the optimal minima. The acceleration is obtained through a reduction of noise from the stochastic nature of the selection of data point for consideration. \texttt{ASGD} takes the standard \texttt{SGD} algorithm and recursively computes the average $\bar{w_t} = \tfrac{1}{t}\Sigma_{i=1}^t w_t$ \citep{Bottou:2010}.

\paragraph{Adam}
The \texttt{Adam} algorithm~\citep{Kingma:2015} is also frequently used in abuse detection classification research \citep{Meyer:2019,Zimmerman:2018,Kolhatkar:2021}.
The algorithm seeks to further push the goal of faster convergence onto optimal minima, \citet{Kingma:2015} propose the \texttt{Adam} algorithm.
The \texttt{Adam} algorithm is also a stochastic optimisation algorithm, however it only requires computing the first-order gradients.
The algorithm seeks to compute the value of parameters $\theta$ at time-step $t$ that achieves convergence.
However, rather than updating all parameters with with the same learning rate, \texttt{Adam} maintains a learning rate for each parameter which is adapted as the training of the network proceeds.

\texttt{Adam} achieves this by first computing the gradients with regard to the stochastic objective at time-step $t$, then updating the biased mean estimate and the biased uncentred variance estimate.
This is followed by computing the bias-corrected mean and uncentred variance estimates, respectively which are computed by factoring in exponential decay rates for the moment (mean and uncentred variances) estimates.
Finally, the value of $\theta_t$ is updated and the process is repeated if $\theta_t$ has not converged.

\paragraph{Adam with decoupled weight decay} \texttt{Adam with decoupled weight decay (AdamW)} was proposed by \citet{Loshchilov:2019} as a result of examining the implementation of Adam in many libraries for neural network training and finding that many had incorrectly implemented \texttt{Adam} using $L2$ regularisation rather than weight decay.

Since this correction, papers on abuse detection have started to use this algorithm \citep{Rottger:2021,Vidgen:2020} over the initial \texttt{Adam} implementation that used $L2$ regularisation rather than weight decay.

\subsection{Bayesian Hyper Parameter Tuning}\label{sub:bho}
The performance of neural network architectures rely on a range of hyper-parameters that control their behaviour from a number of different positions in the model. For instance, the size of the layers in the network can be treated as a hyper-parameter, the learning rate for the optimisation algorithms, and the rate with which to apply dropout in the model.
As a result of the many different potential hyper-parameters that can be tuned, the complete search space for all hyper-parameters grows exponentially for each new hyper-parameter under consideration. 
While the same holds true for linear models, the number of parameters to explore often figure in much smaller ranges, for instance for the linear baseline models that I use in this dissertation, only hyper-parameters are used resulting in a search space that can be fully explored in the matter of minutes.
For neural network models, a full search of the hyper-parameter search space however quickly becomes infeasible and thus introduces the question of how a hyper-parameter search space can be adequately explored without the need for a complete search. 

One way to perform a hyper-parameter space search, without searching the complete space of every combination, is through Bayesian Optimisation for hyper-parameter identification \citep{Snoek:2012}. Through the use of Gaussian Processes (GP), the selection of hyper-parameters for trial can be cast as an optimisation problem, where the hyper-parameters serve as a feature space and the performance obtained with each parameter serves as the label.
The aim of the GP model is to estimate how each hyper-parameter contributes to the final classification performance of the model and provide suggestions for the next set of hyper-parameters to trial.
I use \citet{Wandb}, which implements \citet{Snoek:2012}, for all hyper-parameter searches for neural network-based experiments.

\subsection{Metrics}

Model performances can be evaluated in a number of different ways, from qualitative analyses of the model outputs to quantitative analyses. Within the bracket of quantitative analysis, further subdivisions exist including the one I will use in this dissertation, namely the use metrics computed using model predictions and the ground truth.
For my evaluation, I use the \texttt{F1-score}, \texttt{precision}, \texttt{recall}, and \texttt{accuracy}. As many of the datasets that are used for training and evaluation have heavily imbalanced class distributions, each of these scores provide different aspects into model performances and different levels of insight into the models.
The metrics all require insights into the agreements between the ground truth and a model's predictions. These agreements can be categorised into four different groups:
\texttt{True Positives (TP)}, where the model's prediction and the ground truth label agree and the label belongs to the positive class; \texttt{True Negatives (TN)}, similarly where model prediction and ground truth agree and the label belongs to the negative class; \texttt{False Positive (FP)}, where the model predicts the label for the positive class but the ground truth is in the negative class; and \texttt{False Negative (FN)}, which is the inverse of \texttt{True Positive}, i.e. the model predicts the negative class but the ground truth is in the positive class.
\footnote{I do not use the \texttt{Area Under the receiver operating characteristic Curve} as a metric as this metric does assumes a similarity between all underlying samples \citep{Stevenson:2021}. A guarantee that cannot be made when datasets with distinct annotation strategies and sampling are involved .}

\paragraph{Accuracy}
\texttt{Accuracy} is the simplest metrics among those I use, and it's subsequently also highly volatile to class imbalances.
The score (see \cref{eq:acc}) computes the number of correct predictions out of all predictions made. For balanced datasets, this metric provides a good insight into a model's overall performance, however for imbalanced data, it is susceptible to providing a distorted view of a model's performance. For instance, if a dataset has a heavy class imbalance, a model that only predicts the majority class will have a deceivingly high \texttt{accuracy} score.

\begin{figure}[h]
  \begin{equation}\label{eq:acc}
    accuracy(Y,\hat{Y}) = \frac{TP + TN}{TP + TN + FP + FN}
  \end{equation}
  \caption{Equation for the \textit{accuracy score}.}
\end{figure}

\paragraph{Precision}
\texttt{Precision} provides an estimate of how well a model predicts into the positive class.
Specifically precision asks to which degree classifications into positive class are correct classifications into the class (see \cref{eq:prec}). Thus, one can ascertain to which degree a model can be trusted when it predicts a positive label.

\begin{figure}[h]
  \begin{equation}\label{eq:prec}
    precision(Y,\hat{Y}) = \frac{TP}{TP + FP}
  \end{equation}
  \caption{Equation for the \textit{precision score}.}
\end{figure}

\paragraph{Recall}
\texttt{Recall} on the other hand, provides insight into the ability of a model to retrieve correct instances of the positive class.
By computing the fraction of data predicted correctly into the positive class and the union of data correctly predicted into the positive class or incorrectly predicted into the negative class, \texttt{recall} can allow for an intuition into how trust-worthy a model is when it predicts that data is not in the positive class.

\begin{figure}[h]
  \begin{equation}\label{eq:rec}
    recall(Y,\hat{Y}) = \frac{TP}{TP + FN}
  \end{equation}
  \caption{Equation for the \textit{recall score}.}
\end{figure}

\paragraph{F1-score}
In practice it is often desirable to balance \texttt{precision} and \texttt{recall} as as they allow for intuitions into two crucial aspects of model performance, it's ability to correctly retrieve data into and exclude data from the positive class. The \texttt{F1-score} provides exactly such a balancing by computing the harmonic mean of the \texttt{precision} and \texttt{recall} scores (see \cref{eq:f1score}).

\begin{figure}[h]
  \begin{equation}\label{eq:f1score}
    \mathit{F1}\text{-}score(Y,\hat{Y}) = 2\cdot\frac{Precision \cdot Recall}{Precision + Recall}
  \end{equation}
  \caption{Equation for the \textit{F1-score}.}
\end{figure}

For abuse detection, the macro average of the F1-score is often used. The macro averaged F1-score, or macro F1-score, sum the F1-score for each class and computes their mean, thus providing insight into the performance of models across the different classes.

\subsection{Machine Learning Models}

As I explore different experimental research questions, I train different machine learning algorithm for detecting abusive language. Each of the model types that I use rely on different methods of operationalising data to obtain internal representations of the different classes. Here $X$ is to mean the processed input to the model, $Y$ is to denote the corresponding ground truth labels, and $\hat{Y}$ denote the set of model predictions. For all models, the aim is to learn a function $f(X|Y)$ that can delineate between each class $y_i\in Y$.

\paragraph{Logistic Regression}
The first linear model that I use in this dissertation is Logistic Regression (LR), which has previously been used widely in NLP tasks, and abusive language detection in particular \citep{LR papers}. Logistic Regression is a model that carries certain assumptions about the data that is represented, in particular, I call to attention its assumption of feature independence. The assumption of feature independence presumes that each individual feature, or word token in the case of language, contributes to the classes in isolation of all other features. Trivially, this assumption does not hold for language.

In terms of mathematical modelling, LR relies on the \texttt{Sigmoid} function (see \autoref{eq:sigmoid}) which calculates the probability of a data point, or document, belonging to a class. In \autoref{eq:sigmoid}, $w_0, w_1, \ldots, w_n$ denote model coefficients that are obtained through maximum likelihood estimation and $x_1, \ldots, x_n$ represent the features that are treated independently.

\begin{figure}[h]
  \begin{align}\label{eq:sigmoid}
    F(z) &= \frac{1}{1-e^{-z}}\\
    z &= w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + \cdots + w_n \cdot x_n
  \end{align}
  \caption{The \texttt{sigmoid} function.}
\end{figure}

\paragraph{Support Vector Machines}
The Support Vector Machine (SVM) algorithm seeks to identify a hyper-plane where the data can be mapped to and classes $y_i \in Y$ are linearly separable. Such mappings can be computed using different kernels. Beyond identifying a hyper-plane where the classes are linearly separable, SVMs also have the additional aim of identifying a hyper-plane that maximises the margins, that is the distance between the linear separation and the closest data points for each class. Specifically, SVMs seeks to maximise the prediction given by $sign(w^T\phi(x)+b)$ where $\phi$ is the identity function and $b$ is an independent value.
Although many different kernels exist for SVM classifiers, I use a linear kernel (see \cref{eq:svm} for the mathematical formula used by \citet{Pedregosa:2011}) as this provides weights for each feature that can be analysed to understand which patterns the model is learning.

% See for SVM documentation: https://scikit-learn.org/stable/modules/svm.html#id15

\begin{figure}[h]
  \begin{equation}\label{eq:svm}
    \min_{w,b} \frac{1}{2}w^T w + C\cdot \Sigma_{i=1}\max(0, y_i(w^T\phi(x_i)+b))
  \end{equation}
  \caption{Formulation of the Linear Support Vector Machine provided by \citet{Pedregosa:2011}, where $\phi$ is the identity function and $C$ is the regularisation strength.}
\end{figure}

\paragraph{Multi-Layered Perceptron}
The Multi-Layered Perceptron (MLP) is perhaps the simplest form of neural network. This neural network is an extension of the Perceptron algorithm by chaining several Perceptron units into a single layer, A Perceptron is updated given the update rule in \autoref{eq:perceptron_update}. Moreover, rather than consisting of a single Perceptron that learns weights of the training data, MLPs are formed of multiple layers of chained Perceptron units. An MLP requires at least three layers, an input layer, at least one hidden layer, and an output layer.
Similar to the linear models described in the past sections, MLPs also have an independence assumption coded in, as they assume that each input token is independent from each other.

\begin{figure}[h]
  \begin{equation}\label{eq:perceptron_update}
    w_{i+1} = w_i(t) + \epsilon (y_i - \hat{y_i}(t))x_{i})
  \end{equation}
  \caption[The Perceptron weight update function.]{The Perceptron weight update function for binary classification. Where $t$ is the time-step, $\epsilon$ is the learning rate, $x_i$ is the training example, $y_i$ and $\hat{y_i}(t)$ are the ground truth and the model prediction at time-step $t$, respectively.}
\end{figure}

Finally, the Perceptron, similarly SVM and LR classifiers is a classifier operates on the data in a one-directional, that is a feed-forward manner. MLPs on the other hand can either be developed as feed-forward networks or networks with back-propagation.
A network that uses back-propagation updates the model representation first in the same manner as a feed-forward network in its forward pass, second by computing the loss and propagating it backwards through the model, updating the representation as the loss is propagated through each layer of the model.
In this dissertation, all MLPs are trained with back-propagation.

\paragraph{Long-Short Term Memory Networks}
The idea of recurrence for neural networks stems from the realisation that MLPs are poorly suited to address sequences that move through some conceptualisation of time.
\footnote{The conceptualisation of time can vary depending on the data and task at hand. For structured prediction, time can be the sequence of tokens while for stock price prediction it can be the traditional understanding of time as a linear construct.}
To address this short-coming, Recurrent Neural Networks (RNNs) were proposed. RNNs introduce a loop, or recurrence, in the neural network by iterating over the components of the input, linking each iteration (cell) of the loops to all prior iterations.
By linking into past iterations of the within-model loop, RNNs can model developments of data through a linear conceptualisation of time, by predicating the performance of the loop at time-step $t$ on the representation of the network at time-step $t-1$.
For instance, when passing a document through a RNN, the model will iterate over the document, treating each token as a time-step.
The representation derived for the token at time-step $t$ will then be predicated by all preceding tokens.
In this way, RNN models can encode dependencies to understand complex interactions of tokens through time.

In practice however, RNNs aren't well suited for long-range dependencies, as all preceding time-steps are treated with equal value, leading to a decay over time.
Additionally, some information occurring at an earlier time-step may not be relevant to all subsequent time-steps.
To address this issue, \citet{Schmidthuber:1997} propose the Long-Short Term Memory (LSTM) network, which is a special form of RNNs.
LSTMs differ from vanilla RNNs by introducing the concepts of gates. Namely, \citet{Schmidthuber:1997} introduce a ``forget'' gate and an ``input'' gate in each cell of the LSTM. Each gate in the LSTM cell can modify the cell state.
First, the input at time-step $h_t$ receives the output from the cell at state $h_{t-1}$ and a sigmoid function that determines what information from the cell state at $h_{t-1}$ is retained, given $x_t$.
Next, the ``input'' gate decides which values will be stored in the cell state. This decision is made by first selecting the values that are to be updated and how much they are to be updated, and then creating a vector of candidate values to be added.
Having computed which values to forget, store, and update, it is now a simple matter of performing the updates to the cell state at $h_{t-1}$. First modifying the cell-state to only retain the values that are to be remembered. Then, the values selected for updating and their candidate values are added to the cell-state, thus producing a new cell-state.
Finally, a version of the cell-state, filtered by a sigmoid function to control what is passed on, is output to the next cell $h_{t+1}$.

For my experiments using LSTMs, I use the implementation offered by \citet{Paszke:2019} which uses the variation of LSTMs proposed by \citet{Sak:2014}.

\paragraph{Convolutional Neural Network}
Convolutional Neural Networks (CNN) are a type of neural network that were initially proposed for computer vision tasks.
Like all other forms of neural networks, CNNs have an input, an output layer, and some hidden layers.
The hidden layers of CNNs contain \textit{convolutional layers}. These layers apply a series of convolutions, or sliding windows over a matrix of features and compute a summary of those features.
Where other networks, MLPs for instance, often contain just a single hidden layer, CNNs often contain multiple hidden layers in the form of convolutional layers.
After the data has been processed by each convolutional layer, a non-linearity is applied to the resulting representation.
Once all convolutions have been completed, a pooling operation (see \cref{sub:pooling} for more detail on types of pooling) is performed as the final step that is unique to CNNs.
The operation of using convolutions that consider multiple features can be likened to the use of n-grams where $n>1$. However, unlike traditional n-grams that are processed directly, convolutional layers are subject to the non-linearity and the pooling operation, the latter of which summarises the identified features and creates a modified representation of the learned feature maps.

\subsection{Multi-Task Learning}\label{sub:mtl}
The Multi-task Learning (MTL) framework was initially proposed by \citet{Caruana:1993} as a way to train a model for a specific primary task by leveraging that (multiple) tasks may be related.
Choosing a primary task, one or more tasks can be chosen as auxiliary tasks that can provide inductive biases for the model to take advantage of to perform better performance for the main task.
MTL models can be trained in two different ways, through hard parameter sharing or soft parameter sharing.
Hard parameter sharing models are trained by having (some) hidden layers that are shared by all tasks and some layers that are individual to each task.
When training each task, the model updates all layers of that task, including the shared hidden layers.

On the other hand, models that are trained using soft parameter sharing do not share any layers, instead the parameters of each task are regularised to be similar \citep{Duong:2015}.
In \autoref{fig:mtl_types} we see the two different types of models.
As seen in \autoref{fig:mtl_hard}, while each task have individual input and output layers, they all share a hidden layer.
In \autoref{fig:mtl_soft}, we see that each task has its own model that would be unrelated to one another if not for the fact that the parameters of each layer are regularised to be similar to each other.

\begin{figure}
 \centering
 \begin{minipage}{0.5\linewidth}
   \centering
   \includegraphics[scale=0.75]{Figs/multitask_hard.jpg}
   \caption{Depiction of Multi-task learning framework using hard parameter sharing.}
   \label{fig:mtl_hard}
 \end{minipage}
 \begin{minipage}{0.5\linewidth}
   \centering
    \includegraphics[scale=0.75]{Figs/multitask_soft.jpg}
   \caption{Depiction of Multi-task learning framework using soft parameter sharing.}
   \label{fig:mtl_soft}
 \end{minipage}
 \caption{Parameter sharing strategies for Multi-task learning.}
 \label{fig:mtl_types}
\end{figure}

While the idea of inductive biases from related tasks provides a compelling argument for examining MTL for abuse and hate speech detection, there are some interesting attributes to the framework.
First, as MTL is compatible with neural networks, researchers can forego feature selection similar to other neural network approaches. This automated feature selection process carries some benefits and risks.
One benefit of automated feature selection performed by neural networks is that designers of models aren't required to identify potentially suboptimal features. 
On the other hand, such automated feature identification risks that models identify spurious patterns in the data to exploit without easy ability to easily identify such spurious patterns.
Moreover, manual feature creation relies on designers of systems to interrogate the data to create features, resulting in research hypothesis being directly embedded in the systems designed to answer the research questions.

Second, while an ensemble model training framework may appear very similar to the MTL framework, a key dissimilarity is that MTL models share information between the different tasks; for hard parameter sharing models this sharing occurs through shared layers \citep{Caruana:1993}, while for soft parameter sharing model information is shared through the similarity of of layers across models for each task \citep{Duong:2015}.

Third, for hard-parameter sharing models, the complexity of developing a model is reduced as information is directly shared between the models through the shared layer, while at least two layers (input and output layers) are individual to each task.
Thus, only a single model is trained, where the designers need only to concern themselves with the layers that are not shared, rather than concern themselves with full models and how to balance them.

Fourth, as \cite{Caruana:1997} show, the framework allows for training for several distinct tasks while leveraging the similarities shared by each individual task.
For hard parameter sharing models, this approach also introduces the risk (and opportunity) of a single task dominating the representation of the model, due to either more data being available or a task being selected with for training with greater probability than the remaining tasks.
% \citet{CITE: Weighting paper} argue, this risk can be mitigated by either weighting the loss function, such that loss of each task is controlled by the researcher and the importance they wish to provide each task \citep{CITE: Weighting paper}. However, as \cite{CITE: Weighting/aux task paper} show, there is also an opportunity in this risk. By assigning the task of primary interest a higher weight than all other tasks, the model can be guided towards prioritising what is learned from this task over all others \cite{CITE: Paper with auxiliary tasks}. Selecting such weighting of the different tasks, in a similar vein to feature selection assumes that the researcher has knowledge and a hypothesis about how the different tasks are likely to influence each other.

Fifth, when working with different datasets for similar and distinct tasks alike, directly leveraging them outside of a MTL model can be a cause for concern due to differences in collection rationales, data sources, or annotation strategies \cite{Waseem:2018}. 
However, through both weighting of the different tasks and the fact that each task has either its own input and output layers or its own model, such concerns can be alleviated due to either limited shared layers that are optimised or due to distinct models being trained that are regularised to minimise dissimilarity, depending on which parameter sharing strategy is used.

Finally, in the event that an auxiliary task does not contribute to the primary task as the researcher had hypothesised, it may still contribute to the overall generalisability of the model as the offending task will act as regulariser for the primary task, as it introduces noise into shared layer \citep{Bingel:2018}.

For hard parameter sharing MTL models, the selection of batches for training the model requires significant consideration as the batch determines which task is being trained. Thus, if one task is selected more than others, the resulting model will be tuned towards that model.
For this reason, there are two ways to control which task acts as the primary task, 1) through the main task being selected most frequently or 2) through weighting the different tasks according to their importance.
The latter method controls the influence of each task by multiplying the weight of each task with the loss produced following each epoch.

\section{Fairness}\label{sec:fairlitt}

Bias and fairness in machine learning and the corresponding field for NLP are growing fields that seeks to describe and address how machine learning systems have disparate impacts on different groups, leading to downstream marginalisation of some bodies.
The field addresses the question of marginalisation using statistically based measures to quantify and redress the harms enacted by optimisation technologies \citep{Kulynych:2020}.
In other words, the field attempts to address issues of marginalisation by using the very abstractions that cause the exacerbation of harms by computational tools.
In general, work in the field operates along three different strands

\begin{enumerate}
  \item{A descriptive strand which aims to map out models and datasets with their intended uses and limitations,}
  \item{a quantitative strand, which seeks the quantification and automated analysis of the quantification and analysis of disparate outcomes of model prediction, and}
  \item{a mitigation strand focusing on how biases that are present in models and datasets can be addressed.}
\end{enumerate}

\subsection{Mapping Uses and Limitations}

A number papers have sought to map limitations in prior work and proposed methods for future works to document ethical risks and ramifications.
In early work, \citet{Hovy-Spruit:2016} design a taxonomy of ethical risks of NLP systems from over generalisation to dual use of models and from exclusion of demographies of people in datasets to over- and under-exposure of topics to a model.
Following with considerations of datasets, \citet{Bender-Friedman:2018} and \citet{Gebru:2018} propose ``data statements'' and ``data sheets'', respectively, to documenting the processes with which datasets for machine learning experiments are created and the logics that they draw on for their creation, and shortly thereafter \citet{Mitchell:2019} propose an analogous ``model card'' framework for describing the design rationales for machine learning models.
More recently, \citet{Blodgett:2020} surveyed $146$ papers addressing questions of bias in NLP, and identify that in spite of the large body of work, the notion of ``bias'' is often under-specified to a point that ``techniques [for addressing bias] are poorly matched to their motivations, and are not comparable to one another'' \citep[p. 5455]{Blodgett:2020}.

\subsection{Quantifying harms}

\citet{Shah:2020} propose a mathematical framework for quantifying biases that arise in different steps of the NLP pipeline with a basis in the taxonomy proposed by \citet{Hovy-Spruit:2016}. Here, the authors develop a method to quantify biases that may stem from the data and models trained on it, aiming to provide designers of NLP pipelines with a method to zoom away from the details of how data and models may be biased and instead obtain an abstraction that provides a guide to where human attention may be needed.
Moving away from a laboratory setting, \citet{Buolamwini:2018} identify how commercial facial recognition systems perform and fail for people. They find that there is a correlation between a facial recognition system's ability to identify faces and the gender and skin-tone of the subject. They find that, in general the systems surveyed tend to perform worse on darker skin-tones and women, with the ability to detect dark-skinned women.
Turning to language, \citet{Gonen:2019} highlight that many methods for addressing bias in word embeddings leave traces of stereotypes that allow for reconstruction of gendered spaces in word embeddings that have been treated for gender bias.\footnote{Although bias treatment is often termed ``debiasing'', I resist convention as the term ``debias'' is a red-herring for ``acceptable bias''. As I address in greater detail in \cref{chap:disembodied}, such language obscure how methods treated for bias exist and are politicised.}
In a different conceptualisation and operationalisation of bias, \citet{Waseem:2016} examine how different annotator groups label hate speech. While many of the previous methods seek to eliminate, document, or redress biases in datasets and models, \citet{Waseem:2016} proposes to instead accept that social biases are an inevitable force that cannot simply be removed. Instead, they propose that one can lean into this issue by specifically biasing data towards a specific position. \citet{Waseem:2016} argue that by such deviation from requiring a ``debiased'' or ``global'' position, it is possible to train models that outperform systems that are based on data that reflects the quest for a global consensus.

\subsection{Harm Reduction}
At least two broad conceptualisations of bias can be found in the large body of work dedicated to this question \citep[e.g.][]{Agarwal:2018,Romanov:2019,Kulynych:2020,Bolukbasi:2016,Zhao:2017}.
In the first conceptualisation, bias can be imagined as a finite and countable quantity in a model. Being a countable quantity, it can also be minimised and reduced out of the model or data representation.
The aims of this work, is not only to minimise the discriminatory social biases that exist in the models but also maintain ``good'' performance on the primary task.
Thus, this line of work accepts a premise that models and data representations that have been treated for bias must still be useful for their intended purpose instead of proposing that models that cannot function without encoding social biases cease to have a valid justification for their existence.
The second conceptualisation of harm reduction accepts that machine learning models, and optimisation systems more generally, are subject to social biases and instead of direct reductions to the model, seek to identify methods that can externally counteract marginalisation.

Working within the first conceptualisation, \citet{Agarwal:2018} propose a method to modify the weights of trained models such that they satisfy a given criteria for fairness. In this work, there is a reliance on the knowledge of who, in the case of language data, the speaker is and what demographics they belong to.
Contrary to this requirement, \citet{Romanov:2019} propose a method that does not have this requirement. Instead, they propose developing an auxiliary machine learning system for the expressed purposed of identifying the demographic belongs of a person given text that they have authored. The predictions of this machine learning system is tehn encoded into the loss function of the task they seek to train a model that has been treated for bias, letting the loss be subject to the identities that the author has.

Using the second conceptualisation as their basis \citet{Kulynych:2020} propose a class of Protective Optimisation Technologies (POTs) that use the logics of optimisation to counteract marginalisation demographic groups experience as the result of being direct or indirect subjects of optimisation technologies.
Notably, this class of systems deviates from all other systems in that it does not necessitate developing computational models but rather seek to interact antagonistically with the optimisation technologies that people are subject to.
Such systems can be computational in nature, for instance \citet{Kulynych:2020} show how an automated system can address disparities in loan applications by identifying which features can be modified by a demographically dissimilar collective that have similar loan applications to reduce the number of false negatives, that is people who are incorrectly predicted to default on loans, in part as a basis of their demographic belonging.
In an example of a non-computational POTs, \citet{Kulynych:2020} describe how people who see large amounts of traffic being redirected through residential neighbourhoods by route-planning applications report road works and other obstructions, to avoid traffic from being directed through their residential neighbourhoods. Thus, while the residents that resist the optimisation of route-planning applications are not the primary users of the application, they become externalities of those applications and antagonistically use the technologies to address the harms that are inflicted upon them by the optimisation technologies.

% Note that \cite{Sap:2019} misuse \cite{Blodgett:2016} to provide assumed demographic affiliation of the author, however author attributes are computationally estimated and \cite{Blodgett:2016}'s method does not afford such attribution.

\section{Summary}

In this chapter, I have provided an introduction to the computational methods and logics that the work in this dissertation rely on. First, I introduced the task of abusive language detection; second, I provided an overview of the datasets that I used in the subsequent chapters of this dissertation; third, I detail the different parts of the modelling process that the machine learning systems developed in this dissertation rely on; and finally, I gave a brief overview of different strands of thinking for work on bias and fairness in the machine learning literature.
