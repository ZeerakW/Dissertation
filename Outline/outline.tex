\chapter{Thesis Outline}
\label{chap:ddp}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Outline/Figs/Raster/}{Outline/Figs/PDF/}{Outline/Figs/}}
\else
    \graphicspath{{Outline/Figs/Vector/}{Outline/Figs/}}
\fi

In this chapter we provide the thesis outline, and note which chapters are yet to be undertaken.
\section{Changelog}
The primary changes in this chapter are:
\begin{enumerate}
    \item Re-ordering and moving social scientific work and ethics in machine learning work to the end to immediately precede the discussion chapter
    \item Drop temporal analysis work
    \item Update timelines and expected completions
    \item Add disembodied machine learning chapter
    \item Modify Content moderation as care to politics of content moderation
\end{enumerate}
{\color{red}
\section{Introduction}

Here we provide introducing text stemming from each of the different areas and provide an overview of the thesis aims.

Timeline: Expected completion late August
\section{Literature Review}

\subsection{NLP and Computer Science}

Here we present the previous works in NLP and computer science on fairness, abusive language detection, computational social science, and fake news detection. While a great deal of this has been written, an updated version is warranted to address the latest literature and the added areas from the draft version below. 

Timeline: Expected completion late August
\subsection{Social Science and Policy}
In this section we present the social scientific work on fairness, intersectional feminism and oppression, notions of acceptability of dialects, law, policy and platform policy, ethics of research into abuse, and moderation practices. A large part of this has been written but further detail for many of the topics covered in this draft would benefit from further attention.

Timeline: Expected completion late August
%\subsection{That is the Law - Developments of legal realities for content moderation}
%Here we detail the current state of platform policy and legal developments. We examine the state of content moderation technology for abusive langauge and address the ways in which these may interact with proposed and implemented European regulation.\footnote{Status: In initial draft stages.}

\section{Towards Fairer NLP}

This chapter will consist of work on extending fairness methods to NLP and allowing for fair fake news and abusive language detection. Additionally, we will incorporate our considerations of how natural language processing can incorporate considerations of critical studies in a practical sense, allowing for an examination of how systems are built. This will be apparent from the other works in this chapter. Methods found in this chapter will be used in the subsequent chapters, where appropriate.

\subsection{Fairness in NLP Classification}

In this section we will introduce our methods for building fair NLP models. We extend the method for fair classification proposed by \citep{Agarwal:2018} to Natural Language Processing. We apply the method to two problems: Fake news detection and hate speech detection.

\subsubsection{Fairly Fake: Investigating the Influence of Party Affiliation on Fake News Classification}
Here we consider the influence of political affiliation on fake news detection models. We use this task to show the viability of the model by showing that unrestricted, a model for fake news detection learns a bias towards a political direction.\footnote{Status: Running final experiments.}

Timeline: Expected completion Early August
\subsubsection{Fairly Reliable: Investigating the influence of political ideology on predicting reliability of political articles}
Following the methodology from the previous section, we show how measuring classifiers learn confounding factors in predicting the reliability of articles across the political spectrum.\footnote{Status: Running final experiments.}

Timeline: Expected completion Early August

%\subsubsection{''Walking Contradiction``: De-marginalizing AAVE in Abusive Language Detection}
%In this section we describe our annotation experiment for labelling African-American Vernacular English (AAVE), and the subsequent experiments to build a classifier to distinguish between AAVE and Standard American English. The resulting classifier will then be used to augment datasets labeled for abuse with dialectical annotations. Following the augmentation of abusive language datasets, we seek to identify bias against AAVE speakers and investigate methods to minimise the influence of bias introduced by disproportionate representation of AAVE in the positive classes.\footnote{Status: Recruiting annotators.}

%\section{''In the Beginning there was Jack!`` The Temporal Developments of Abuse}

%In this chapter, we investigate the temporal patterns and influences on the production of abuse towards British Members of Parliament.\footnote{Status of experiments in this chapter: Planning - application for Ethical Approval Submitted}

%\subsection{About Time: Analyzing the Temporal Patterns in Abusive Responses to British Members of Parliament}
%In this project, we aim to analyse the temporal patterns of abuse against British Members of Parliament. We describe here how abuse develops temporaly in its intensity and volume - and seek to understand the linguistic developments of abuse as function of the identified temporal patterns.

%\subsection{Timing is Everything: Using Temporal Patterns of Abuse for Early Detection of Abusive Moments}
%Here, we describe how we apply the temporal patterns in aims to predict moments of high-volume abusive responses. Further, we analyze tweets and their volume of abusive responses in efforts to predict and analyse which tweets are likely to receive a high volume of abusive responses.

%\subsection{Out of Time: Identifying External Tensions through Abuse Detection}

%Here we seek to use the spikes of abuse identified to identify candidates for examination of external influences such as elections, public debates, and violence on the volume of abuse.


\section{Generalizing Abusive Language Detection}
In this chapter, we will seek to address the heavy reliance on words for detection of abuse, seeking to allow for the use of meta-information and linguistic patterns and using multi-task learning for abusive language detection.

\subsection{Multi-task learning for hate speech detection}
Here we seek to map out which auxiliary tasks are conducive to identifying abuse. Specifically, we aim to use tasks which have large scale resources available to allow for higher generalizability of the resulting model than many approaches to identifying abuse allow for.\footnote{Status: Code is written and tested. Experiments need to be set up.}

Timeline: Final results expected by the end of July.

\subsection{Classifying Abusive Documents using Linguistic Information and Meta data}
Here we aim to provide some critical consideration into which kind of metadata to use for building models for detecting abuse. Additionally, we aim for building a more generalizable model by using linguistic information rather than the words themselves.\footnote{Status: Verifying experimental results}

Timeline: Final results expected by the end of July.

\section{The Socio-technical realities of abuse}

In this chapter, we consider the formal and informal social systems and processes which govern the production and responses to abuse.

\subsection{Filtering Harm: The Politics of Toxicity in Content Moderation Infrastructures}
Here we detail our work on how notions of abuse and toxicity are used in content moderation literature and the implications of such meanings. Through engaging with theories on pollution and discard, we ask how and who content moderation technologies are for and how they are employed.\footnote{Status: In review; chapter roughly drafted, missing parts of previous work.}
%Here we detail our work on framing content moderation in the light of systems of care. By viewing content moderation in this framing, it is then possible to consider the work of the content moderator in the tradition of intersectional feminist scholarship, giving access to considerations of how the burden of moderating harmful content - and in fact the outcomes of said moderation - is placed disproportionately on marginalised bodies.\footnote{In review for a special issue in Big Data and Society.}

Timeline: Expected completion in Late July

\subsection{Disembodied Machine Learning}
In this chapter, we examine how data and machine learning models predicate on an illusion of objectivity and discuss how the acceptance of such objectivity hinder the possibility of reaching the goals of fair machine learning.\footnote{Status: In review; chapter roughly drafted, missing previous work.}

Timeline: Expected completion in Late July

\subsubsection{Critical NLP for Abusive Language Detection/ Discussion Chapter}
Here we discuss how content moderation systems, and widely NLP systems, may influence the people they are meant to serve. Here we aim to investigate how choices throughout the process of building NLP systems and tasks can allow for marginalisation and how, by borrowing from critical studies, practitioners and researchers can divulge and analyse their choices and the potential impacts of them. Further, we here ask if the directions in the field of abusive language detection are inherently harmful to the populations it seeks to protect.\footnote{Status: Waiting for the rest of the content to be written.}

Timeline: Expected completion Mid August.

\section{Conclusion}
In this chapter, we will conclude the findings of the thesis with a specific focus on how the methods described in the thesis may interact with both regulation and platform policies.

Timeline: Expected completion late August
}