\ifpdf
    \graphicspath{{Chapter8/Figs/Raster/}{Chapter8/Figs/PDF/}{Chapter8/Figs/}}
\else
    \graphicspath{{Chapter8/Figs/Vector/}{Chapter8/Figs/}}
\fi

\chapter{Conclusion}\label{chap:conclusion}
%- - Chapter arguments:
%- - - Chap 4 (Dirt): Abuse detection is broken because it operates with trying to classify uncertain boundaries
%- - - Chap 5 (LIWC): If the boundary is porous then maybe we're looking at the wrong problem, maybe the right problem is what people mean
%- - - Chap 6 (MTL): If we must look at dirt, then maybe we need to look more broadly to understand how people are trying to communicate
%- - - Chap 7 (Disembodied): But maybe this is all wrong if what we want to do is protect people but we're actually harming them.
%- - - Chap 8 (Conclusion): Well, maybe we need to rethink this all, but some components of rethinking it are in thinking about the things around the abuse, not the abuse itself.

In this thesis I have sought to explore the content moderation infrastructures that are built for classifying textual abuse in online spaces.
The contributions of the thesis are structured around four central themes:
How the notions of `healthy' and `toxic' content are operationalised, the implications of such operationalisation and how these come to embody hegemonic imaginaries on respectability;
how large vocabulary reductions, that represent the mental and emotional states of speakers rather than their words influence the ability of models to classify in-domain and out-of-domain data;
how different, apparently related, tasks can be used to jointly optimise model representations to gain models that more closely come to reflect the contexts a given speaker is operating in when speaking;
and finally, how the subjective embodiments of data subjects and modellers alike are embodied in the machine learning pipeline and how these collectively come to privilege hegemonic discourses.
These four distinct themes are connected through two over-arching \ZTedit{research} questions: \ZTdelete{How does the human fit into abusive language technologies and how can machine learning models for content moderation come to more closely respect and represent their humanity?}
\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \roman*}}]
      \item\textit{\ZTedit{What technical and social factors are present in the socially discriminatory predictions of content moderation systems?}}
      \item\textit{\ZTedit{In which ways can computational methods be used to address limitations that are influential in discriminatory outputs from computational modelling?}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

To adequately answer these questions, I address each theme in turn through \ZTedit{through multiple disciplines.
This approach affords insights into the technical, social, and political dimensions of content moderation infrastructures for abusive texts.}
\ZTdelete{a multi-disciplinary perspective that affords insights into the technical, social, and political dimensions of content moderation infrastructures for abusive texts.}
The contributions in this thesis are thus in part \ZTdelete{of theoretical nature and in part of experimental nature}\ZTedit{theoretical and in part experimental in nature}.
By examining the questions through theoretical and experimental lenses, I \ZTedit{can} begin to uncover the political and technical complexities of content moderation infrastructures
\ZTedit{Furthermore, this multi-disciplinary approach affords}\ZTdelete{and obtain} insights that are opaque when the questions are addressed purely theoretically or purely experimentally \ZTedit{approach}.
The thesis has been structured such that I start and finish with primarily theoretical contributions while the primarily experimental contributions constitute the middle of the thesis.
I choose this structure to remain faithful to the machine learning pipeline for content moderation, addressing first definitional questions and then questions of modelling.
Finally, I take a step back and reflect on the machine learning pipeline from start to an end.

In efforts to answer \textit{RQ I} and \textit{RQ II}, I formulate sub-questions that ask the following directed research questions:
\ZTdelete{The research questions I address in this thesis are:}

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{\ZTedit{How are notions of `toxicity' operationalised and modelled, and what are their socio-political implications for content moderation systems?}}}
        \item{\textit{\ZTedit{What are the modelling implications of using LIWC to substitute the use of words and sub-words as input tokens?}}}
        \item{\textit{\ZTedit{How do the individual and combinatory use of abuse classification and non-abusive tasks impact classification of specific forms of abuse?}}}
        \item{\textit{\ZTedit{How are the subjective embodiments embedded in the machine learning pipelines?}}}
        \item{\textit{\ZTedit{What are the implications of such subjective embodiments with regard to developing machine learning models?}}}
        \item{\textit{\ZTdelete{How are notions of ``toxicity'' operationalised in content moderation infrastructures and what are the socio-political implications of such operationalisations?}}}
        \item{\textit{\ZTdelete{How do modelling choices impact how computational models for content moderation learn and represent hegemonic notions of respectability?}}}
        \item{\textit{\ZTdelete{Can LIWC provide a meaningful substitute to using words or sub-word tokens as input tokens and how is model performance affected by such a substitution?}}}
        \item{\textit{\ZTdelete{What are the implications of using LIWC as input on model development in terms of optimisation time?}}}
        \item{\textit{\ZTdelete{What are the implications on generalisability of LIWC-based models?}}}
        \item{\textit{\ZTdelete{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}}
        \item{\textit{\ZTdelete{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}}
        \item{\textit{\ZTdelete{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}}
        \item{\textit{\ZTdelete{How are the subjective embodiments embedded in the machine learning pipelines?}}}
        \item{\textit{\ZTdelete{Which processes in the machine learning pipeline are influenced by the subjective embodiments of the designers and people they rely on and how are they expressed?}}}
        \item{\textit{\ZTdelete{What are the implications of such subjective embodiments with regard to developing machine learning models?}}}
        \item{\textit{\ZTdelete{What are the implications of subjective embodiments on the goal of developing fair and equitable machine learning models?}}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

In \cref{chap:filter}, I address research question $1$.
In addressing \textit{RQ 1}, I critique of how notions of `toxicity' are operationalised and employed through a consideration of of Mary \citet{Douglas:1966} work on social pollution.
I argue that notions of `toxic' and `healthy' are operationalised within several socio-cultural contexts: \ZTdelete{that}\ZTedit{the context} of the designers of the task, the contexts of the annotators, and the contexts of distinct modelling techniques.
These contexts, however are not reflected on in the process of developing automated content moderation tools, instead they are assumed to have little influence, resulting in models that collapse each of these contexts into a single entity that embodies them.
Such embodiment is predicated on efforts towards obtaining a global understanding of `toxic' and `healthy' content, thus reproducing racialised and gendered positions on respectability.
As such, the content moderation systems that I examine, engage in toxic slippage, where they simultaneously over-police the content produced by people inhabiting marginalised positions while also failing to protect these groups from `toxic' content.

\ZTdelete{I address RQ 2 by examining the model development process and the choices that are made in this.}
\ZTedit{Moreover,} I argue that the model development process exhibits three prominent avenues that lead to models that maintain pre-existing, hegemonic power structures.
First, through the optimisation data, which is likely to more frequently have occurrences of some identity terms in the positive class than in the negative class, e.g. `Black', `gay', and `woman'.
Thus, a model is likely to embody that identity terms have a greater association to `toxic' content.
\ZTdelete{Moreover}\ZTedit{Further}, I argue that even when several datasets with competing definitions of abuse are used, \ZTedit{the optimisation procedures result in models better representing the overlaps of data and labels, at the expense of where the datasets or labels diverge from one another.}
\ZTdelete{models will come to better represent the overlaps in the datasets and labels than the data which is separate.}
Second, \ZTedit{models come to embody discriminatory norms} through the use of \ZTdelete{pre-trained}\ZTedit{pre-optimised} embeddings that take a distributional perspective on language production and are known to harbour harmful social biases against minoritised groups \citep{Speer:2017}.
Lastly, I argue that the models themselves are likely to exacerbate hegemonic positions, given that machine learning models have been shown to amplify social biases that exist in datasets \citep{Zhao:2017}.
These findings have an impact on future work for computational modellers as they provide theoretical scaffolding for why and how marginalised communities come to suffer under abuse classification models.
Thus, the contributions here begin to forge a path towards abuse detection models that are centred around the experiences of those who are most likely to suffer harms from misclassifications.

\ZTedit{One challenge that is raised in \cref{chap:filter} is the notion of the pluralist model that Opt Out develop.
While a pluralist model does allow for people embed their own subjectivities into the modelling process, they are not exempt from the critiques made in \cref{chap:filter}.
In fact, as we see in the chapter, Opt Out incorrectly moderates \cref{fig:flash_hoe} due to the uses of the \textit{b-word} and references to sexual promiscuity.
Moreover, as pluralist models also need some centralisation for optimising the machine learning model, they are similarly subject to the risks of optimising towards hegemonic positions.
This risk may be slightly decreased as the pluralist model is only be applicable to a single person at a time. 
Thus the hegemonic positions that they may come to embody might only affect singular individuals who specifically provide data to the model that would encode such hegemonic positions.
This further raises an issue for consideration, namely that of the harms individuals may enact on a larger community.
Specifically, if a person's model comes to embody positions that are tailored to the individual, are there any limitations that should be set for minimal notions of acceptability, that are applied for everyone using such pluralist models?
For instance, should people who believe in the genocide of other people be afforded the ability to determine that calls for genocide is acceptable content while resistance to such calls is deemed as unacceptable?
If such minimal notions of acceptability are to be set, further questions around who is to determine them and what exceptions should be made to this remain as vital questions.}

In \cref{chap:liwc}, I turn towards research questions $2$\ZTdelete{-$5$}.
Starting with \textit{RQ \ZTdelete{3}\ZTedit{2}}, I optimise three different neural network models and two baselines for five different datasets, each experimenting with 3 different data representations: Word tokens, sub-words, and LIWC encoded documents.
I find that using LIWC encoded documents to optimise machine learning models can obtain competitive results with linear models and neural networks \ZTedit{that are} optimised on word tokens and sub-word representations alike.
Specifically, I find that for linear models, LIWC-based models can obtain classification performances that are as good or better than models optimised on the other two representations, though for some dataset and model combinations there is a significant drop in performance.
For neural network models, I similarly find that LIWC models can provide for in-domain classification improvements in some cases, and in most cases provide with a competitive performance.
I find that all three neural model types are well suited for the use of LIWC, with the best performances achieved by the CNN models.
Thus, I conclude that while there is space for the improvements in model performances, LIWC based modelling can provide for an alternative to using word-token or sub-word document representations.

\ZTdelete{To address RQ 4, I monitor the models for how long they require for the optimisation procedure.}
\ZTedit{I further observe how machine learning models are affected by the vocabulary change in terms of the time required to fully optimise them.}
I find that in many cases, LIWC-based modelling for neural networks show a reduction in the time it takes for a model to finish the optimisation procedure.
\ZTedit{However,} I also observer that in some cases, the LIWC-based models take as long, or longer, than models optimised on other document representations.
In particular, I find \ZTdelete{that there is} a relationship between the complexity of the model and the model optimisation time, where the more complex a model is, the longer it takes to optimise.
That is, I find that all but one LIWC MLPs take less time to complete the optimisation procedure than their word-token and sub-word counterparts.
For CNNs, LIWC models tend to finish optimising close to as close as word-token models, and faster than sub-word based models, with a single exception where the LIWC model takes almost twice as long as its closest competitor.
Finally, for LSTM models, I similarly find that in most configurations of datasets, LIWC-based models tend to finish optimising quicker than the other models.
Here too there is a single outlier in which the LIWC LSTM model takes longer to finish optimising than all other models.
Thus, I find that in terms of speed in optimising the models, LIWC-based modelling in most cases provides for as fast or faster model optimisation procedures.

\ZTdelete{I address RQ 5 by applying all optimised models on all datasets, including those that the model has not been optimised on.}
\ZTedit{I also examine how the LIWC-based models perform when evaluated on out-of-domain data by applying on all models across all datasets, including those that the model has not been optimised for.}
Using this method, I find that LIWC-based models often provide for out-of-domain performances that out-perform all other out-of-domain models.
Particularly LIWC-based models optimised on the \textit{Toxicity} dataset performs well on out-of-domain data.
Finally, I observe that all models tend to perform better on out-of-domain datasets where the goal of the dataset resembles the goal of the dataset the model is optimised on.

As the results of my experiments show, there is space for improving in-domain and out-of-domain classification performances by thinking carefully about how data is represented.
Research question\ZTdelete{s 3-5}\ZTedit{2} invites researchers to think carefully about data representations.
Moreover, in specific next steps, there is space to investigate what the impact on modelling is when \ZTdelete{combining} LIWC\ZTedit{-based document representations are combined with} and word-token or sub-word token \ZTedit{representations}.
Such combinations would \ZTdelete{also} allow for using models that have \ZTdelete{pre-trained}\ZTedit{pre-optimised} embedding layers for the word and sub-word parts of the data representations.
The use of \ZTdelete{which}\ZTedit{pre-optimised layers} is likely to provide for additional improvements on both in-domain and out-of-domain performances.

Turning questions of context, I address research questions \ZTdelete{$6$-$8$}\ZTedit{$3$} in \cref{chap:mtl}.
In this chapter, I experiment with Multi-Task Learning with three main tasks for distinct forms of abuse, the \textit{Offence} detection task, the \textit{Hate Speech} detection task, and the \textit{Toxicity} detection task.
Each of these is also used as an auxiliary task when it is not the primary task.
For auxiliary tasks I use the three main task datasets in turn, and the \textit{Hate Expert} dataset, as my abusive auxiliary tasks.
For my non-abusive auxiliary tasks I use the \textit{Sarcasm} detection task, the \textit{Moral Sentiment} prediction task, and the \textit{Argument Basis} task.
\ZTdelete{Addressing RQ 6 and 7 collectively,}\ZTedit{Through my experiments,} I find that there is a positive impact on model performances, in terms of improvements over a single task MLP baseline when using almost any dataset as and auxiliary task.
Specifically\ZTdelete{ to RQ 6}, I observe that the three main task models \ZTedit{that I optimise}\ZTdelete{that I use} all benefit from using sarcasm detection as an auxiliary task.
Moreover, two out of three auxiliary tasks also benefit from using the remaining two non-abusive auxiliary tasks.
Using combinations of only non-abusive tasks also improves modelling performances, though none of these combinations achieve the best-performing auxiliary task combination.

\ZTdelete{Turning to RQ 7,}In fact, I find that using abusive tasks as auxiliary tasks has a positive impact on the model performances.
Unlike the non-abusive tasks there is one auxiliary task setting for one dataset where the highest performance is achieved by a combination of only abusive auxiliary tasks.
The use of abusive tasks in particular has a beneficial impact on performances in terms of precision score.
In the abusive tasks however, I also find that not all tasks are equally suited.
In particular, I observe a relationship between abusive datasets that share similarities in either data source or in annotation goals.
For instance, the \textit{Hate Expert} dataset only performs well enough as an auxiliary tasks to be selected as for further when the main task is the \textit{Hate Speech} task which was sampled from the same collection of data and annotated using the same guidelines.
Moreover, I find that the \textit{Offence} auxiliary task is useful for both main tasks, where it shares the dataset source with the \textit{Hate Speech} task and shares annotation goal with the \textit{Toxicity} task.

\ZTdelete{As I address RQ 8,}\ZTedit{When considering combinations of abusive and non-abusive auxiliary tasks,} I find that combining abusive and non-abusive auxiliary tasks provides for some of the best model improvements.
Specifically, I find that using the \textit{Sarcasm} task in conjunction with one or more abusive tasks provides for high performing models.
This suggests that encoding representations for sarcasm detection in combination with an auxiliary abuse dataset can have benefits on the main task performance.
It is worth noting that all three non-abusive auxiliary tasks appear in the best-performing auxiliary task configurations \ZTdelete{, showing}\ZTedit{and} the best performances are obtained when abusive and non-abusive auxiliary tasks are used together.

The findings in this chapter, provides several paths for future work.
For instance, one avenue \ZTdelete{of}\ZTedit{for} future work is to explore more non-abusive auxiliary tasks such as sentiment analysis.
Additionally, future research can address improvements in modelling e.g. by using more complex modelling architectures or by optimising the weight each auxiliary task is given.

Finally, in \cref{chap:disembodied}, I address the final \ZTdelete{three}\ZTedit{two} research questions: \textit{RQ 4 \& 5}\ZTdelete{9-12}.
In this chapter, I \ZTdelete{critically analyse}\ZTedit{read against the grain in my considerations of} the \ZTdelete{entire} machine learning pipeline for NLP.
\ZTedit{I further} apply my insights to models developed in this thesis for abuse detection.

Through my \ZTdelete{critical analysis}\ZTedit{reading}, I find that subjectivity is embedded into machine learning in all processes \ZTdelete{in which humans with their subjective experiences are involved}\ZTedit{that humans with their subjective experiences are involved in}.
The subjective experiences of people involved in the modelling pipeline express their subjective experiences through the data collection, annotation, and model building processes.
Thus, I argue that to address issues of bias, fairness, and representing the users of \ZTedit{machine learning} models, it is necessary for awareness of the subjective experiences that modellers want to represent and develop processes which foster the human and computational expression of these.
\ZTedit{One avenue for such development processes is through participatory design with a focus on notions of design justice, as outlined by \citet{Costanza-Chock_2018}.}
Moreover, to develop which are fair an equitable, it is necessary to start with the development process of machine learning models with those whose experiences are not embodied in machine learning.
Thus, rather than attempting to force models that have \ZTdelete{learned}\ZTedit{been optimised} to reproduce oppressive structures, this research calls for modelling to be centred around the subjective, lived experiences of people and their needs.

In future work, there is more space for practitioners and researchers to engage in identifying how their own subjective experiences influence their design decisions.
Moreover, an implication of this work is that models that embody desired subjective experiences can be developed, given that the resources \ZTedit{for this} are developed.

\ZTedit{Returning to the guiding questions of how machine learning systems for content moderation come to produce socially discriminatory outputs and the ways in which computational methods can come to address some of these concerns, my findings in this thesis are that machine learning systems very poorly represent the subjective experiences of large groups of people, and the computational approaches that I developed to more faithfully represent these people make positive steps but still fall shy of making truly faithful representations.}
\ZTdelete{Returning to the guiding questions of how the human fits into abusive language technologies and how machine learning models for moderating online abuse expressed through text, the findings in this thesis suggest that at the current stage, people are not well represented by the models that the field have produced and is on track to produce.}
Moreover, content moderation technologies for online abuse expressed through text at present have largely not sought \ZTdelete{to model }to closely represent the subjective experiences of users.
Particularly, they have failed to represent the perspectives and experiences of those who stand to be harmed most by content moderation technologies, instead focusing on goals such as having models that take a global perspective on abuse.
These issues are the result of a computing culture that seeks to abstract away subjectivity in search of, if not `objective truths', global consensus on inherently subjective questions.
However, as I show in this thesis, there is vast, and largely unexplored, space for developing models that more closely seeks to represent the people and thus better make space for peoples subjective experiences.
For instance, in \cref{chap:liwc}, we see how using modelling that seeks to encode the mental and emotional state of the author yields \ZTedit{for improved performances on out-of-domain data, when the annotation goals of the in-domain and out-of-domain data align, thus providing space for generalising specific perspectives.}
Moreover, as observed in \cref{chap:mtl} the use of Multi-Task Learning can allow for models to optimise representations of related auxiliary tasks, such as whether a comment is sarcastic or the expressed moral sentiments, can allow for deeper engagements with the intentions of the speaker, thus moving modelling to more closely represent the speakers and their intentions.

\ZTedit{In seeking answers to the guiding questions, my work in this thesis both fails to achieve the research goals and manages to fulfil them.
My work fails to achieve these goals by not fully engaging with the questions in more depth and in the same vein achieves these research goals by providing a step into these questions.
Moreover, my work achieves the research goals by providing alternative readings and methods for machine learning that can afford more faithful representations of people.
On the other hand, the work fails to achieve its goals by not providing definitive answers, but instead provides suggestions for research directions.
While I construct the successes and failures in absolute terms here, my position is more nuanced.
The work that I have performed provides for some beginnings of research directions and for some further steps for pre-existing directions.
The failures of not achieving definitive answers are offset by having identified new questions to ask while the successes of providing steps for new directions also make space for analyses of the limitations of the directions that I have investigated.}

In this thesis, I have sought to identify challenges and opportunities for developing models for the content moderation of online abuse. This is scaffolding for a hopeful path forward - one that centres the subjective experiences and humanity of those impacted by content moderation technologies,  and that brings back considerations of the social hierarchies that make them most vulnerable to abuse.
I hope to bring the experiences of these people back into the heart of the task.

\ZTedit{
\section{Limitations and Future Work}
The work in this thesis has a number of limitations, both computationally and theoretically.
A core limitation that applies to all aspects of this thesis is that the work is primarily investigating content moderation as it applies to the global economic north, specifically for English speaking nations, with a particular focus on the United States of America.
For all aspects of this thesis, future work would be well suited to develop on the work presented here by centring specific contexts in the global economic south, where content moderation, or the lack thereof has had disastrous consequences.
}
\ZTedit{ % Dirt
  Moreover, my chapter on content moderation and social pollution (see \cref{chap:filter}) provides a next step, expanding on \citet{Liboiron:2018}.
  While this chapter extends a general theoretical framework, it is centred around how content moderations deals with race and gender, and to a lesser degree sexuality in the global economic north.
  A limitation here is then that the considerations and how they apply to other contexts, i.e. the global economic south are not included.
  Future work could then think more deeply about the specificities with which our framework requires extension for specific contexts and that of content moderation in the global economic south.
}

\ZTedit{ % LIWC
  In terms of computational limitations, the work in \cref{chap:liwc} is limited to English as the LIWC dictionary is only defined for English.
  Moreover, using LIWC is most appropriate for content written in mainstream American English, as this is the only variant of English that it is defined for.
  Finally, the use of psychometrics has deep-rooted issues, a reliance on any psychometrics carries a risk of reproducing harmful and hegemonic reductions of psychological constructs.
  Future work could then consider other forms of low-vocabulary representations that are not rooted in such problematic histories.
  Furthermore, future work could consider how low-vocabulary representations can be used in conjunction with pre-optimised technologies such as large language models and word embeddings.
  Centring other languages than English would also have the direct benefit of requiring a different vocabulary reduction technique as LIWC only exists for mainstream American English.
}

\ZTedit{ % MTL
  Similarly to the limitations of \cref{chap:liwc}, the work I have performed on Multi-Task Learning would benefit greatly from centring other languages than English and other populations and nations beyond that of the United States of America.
  Moreover, although there are improvements on the baselines in this chapter, there is ample space for considering how Multi-Task Learning could be used to improve results further.
  This space also includes considerations of wider computational architectures, e.g. the inclusion of pre-optimised technologies as internal layers of the model, or directly experiment with Multi-Task Learning using solely large language models.
  Although I perform an extensive investigation of tasks, several more tasks could be considered for exploration including rumour detection and sentiment analysis.}

\ZTedit{
  Both computational chapters could be extended along the lines of directly including considerations of demographic belonging.
  That is, the loss functions for the neural networks used in both chapters could take into account the demographic information that is available about the speakers.
  By considering demographic information, machine learning models could come to start to encode pre-existing power structures and account for these in their functions.
}
\ZTedit{ % Disembodied
  The final contribution of my thesis is largely theoretical and functionally translation work between fields.
  For this reason, introducing the notion of disembodiment to the machine learning and NLP communities, starts at the beginning, and thus the chapter seeks to provide a foundation for future thought.
  The limitations of this work then is that there is far more depth to consider in how each of the aspects highlighted contribute to disembodying machine learning technologies from human experiences.
  Moreover, the pipeline that I have sought to read and analyse is a research pipeline.
  In a production pipeline for a commercial entity, machine learning technologies are often embedded in deeper technical structures.
  As machine learning technologies are increasingly being deployed into such production pipelines, it is prudent with a consideration of how embodiment and disembodiment happens in a commercial, for-profit entity.
}

\ZTedit{
  Finally, with the work in this thesis I have sought to lay the foundations for new directions for abusive language detection and machine learning.
  The scaffolding that I provide directly invites and welcomes work to build around the scaffolding and develop the structures further, such that we can emphasise justice in the processes we create for developing new technologies for people and communities.
}
