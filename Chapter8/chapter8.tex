\ifpdf
    \graphicspath{{Chapter8/Figs/Raster/}{Chapter8/Figs/PDF/}{Chapter8/Figs/}}
\else
    \graphicspath{{Chapter8/Figs/Vector/}{Chapter8/Figs/}}
\fi


\chapter{The Disembodied and Conservative Natures of Machine Learning}\label{chap:haraway-foucault}

In each of the previous chapters we have identified different areas of concern for the use of our models and data. From the reductive perspectives on data from linear models in \autoref{chap:detection}, over the constraints of document transformation in \autoref{chap:liwc}, to the influence of multiple data sources and prediction tasks in \autoref{chap:mtl}, and finally the constraints of fairness methods for machine learning in \autoref{chap:fair}. In this chapter, we theorize over the core sources of these issues: the models and the data. Through a consideration of both data generation processes and the modelling stages of the machine learning pipeline.

\section{Disembodied Machine Learning - Under Review}\label{sec:disembodied}
\zw{Write disembodied ML paper here}

Machine learning is a practice that is concerned with making decisions based on machine-discernible patterns in observed data. Often, the data upon which machine learning methods are trained, and later applied to, are ``extracted'' from the context within which they are created. Through this process of separation of context and datum, an notion of ``objectivity'' is imposed upon the data and the subsequent operations on the data and their results. Through the repeated separation of datum from context bodies of data, or datasets, are created. These amalgamated bodies of data exist only by virtue of their strict separation from the material bodies that they derive from are then used to train machine learning methods. Machine learning method come in two different forms: Supervised learning methods which seek to learn to distinguish between set of bodies of data, and unsupervised models which seek to identify discernible limbs of data within a single body of data. For both supervised and unsupervised models both the underlying data and the model applied to them have strong influences as to what bodies are discovered and what may be discovered within them.

In the advent of machine learning, these models were hailed as objective, unimpeded by subjective human biases, and by extension social marginalisation \cite{Oneill:2016}. However, an increasing amount of research suggests that social biases are common in machine learning models \cite{Shah:2020,Buolamwini:2018,Agarwal:2018}, and that biases in the underlying data may be exacerbated by the machine learning models \cite{Zhao:2017,Jia:2020}. As a result of this growing awareness, a number of research directions seek to identify \cite{Shah:2020,Bender-Friedman:2018,Mitchell:2019,Buolamwini:2018} aim to identify, reduce or remove social biases \cite{Zhao:2017,Agarwal:2018,Romanov:2019,Jia:2020} from machine learning models to prevent further marginalisation. Such work assumes that social biases operate within a positivist logic in which the removal social bias is cast as an optimisation problem, i.e. that there is a finite and quantifiable source of bias that can be disentangled, isolated, and mathematically reduced out of the body of data or mathematical model from which the designer of both models and data are removed.

Here, we provide a challenge to such a positivist logic drawing on work from feminist Science and Technology Studies and examples from Natural Language Processing we argue that bias and subjectivity in machine learning pipelines are inescapable and can therefore not be simply be reduced or removed, for which reason we hold that an ongoing recognition and reflection on our own positions and the imaginary of objectivity found in subjective realities reflect political choices throughout the machine learning pipeline. Through a contextualisation of bias in these terms, we aim to shift the surrounding discourse away from bias an its elimination to subjective positionality and its implications on the machine learning pipeline from data generation to trained model.

\subsection{Embodiment in the Machine Learning Pipeline}
Through Haraway's \citet{Haraway:1988} critique of objectivity it is possible to understand subjectivity, or bias in machine learning in a way that recognises its potential to create social marginalisation without casting the problem in a positivist, optimisational logic. We argue that the disembodied or objective position exists within the machine learning pipeline at multiple junctions:
\begin{enumerate}
  \item{In the data which is often removed from context and potentially adjudicated by externalised others,}
  \item{in the person designing the experiment and pipeline, and}
  \item{in the model trained on the disembodied data stemming from embodied data subjects.}
\end{enumerate}
First, as data collection processes require decisions to be made to delineate individual pieces of datum that are relevant to the task from those that are deemed irrelevant, it is often necessary to also create a separation between the person who created the datum and the datum in and of itself. Thus, a datum is removed from the context of its creator. Further, datum is often disassociated with the time and the social and political contexts within which it is created. In the case of supervised machine learning, the datum is then provided to a number of annotators, who frequently are neither the creator of the datum nor necessarily situated with contexts of the creator or the datum. These annotators then determine which limb of data, within the larger body of data, the datum belongs to given a set of criteria for such judgement.
Second, as the designer of the model themselves are frequently removed from the contexts within which the body of data are created, they determine how the data are to be represented through a choice of features, setting limits on vocabularies (see \autoref{chap:intro}), transforming the input (as we see in \autoref{chap:liwc}), and the choice of models with their requirements of representing data.
Third, as machine learning models step through disembodied data and embody it within itself, the model itself manipulates the representations of data to identify discernible boundaries between the limbs of data. In this process, the models further disembody the data from the data itself, operating within an assumption that datum consist of the sum of its parts.

In each of these situations lay value judgements on which perspectives of the data are relevant and which are irrelevant. We observe here a peculiarity of machine learning, as data which is disembodied from its creator then becomes the body of knowledge upon which the machine learning model draws on, implicitly transforming all positions that exist outside of the model's internal body then becomes disembodied from the model. This transformation from disembodied to embodied then can serve as an explanation for calls for ``more'' and ``more diverse'' data \cite{Holstein:2019}.
It is worth noting here that the model-embodiment is tacitly acknowledged in the research fields of domain adaptation \cite{Daume:2007} and transfer learning. Through these fields' acknowledgement that to the knowledge held in machine learning models are limited to the domains that are present in the datasets the models are trained on and that even small perturbations in the input to the model may drastically degrade its performance~\cite{Szegedy:2014,Daume:2007}. These acknowledgements of embodiment exist in a self-contradictory tension with the position of objectivity within which these transfer-learning and domain adaptation methods operate within.

\paragraph{The Embodied Designer}
Often a lack of diversity in machine learning development teams is cited as a source of socially biased technologies along with corresponding calls for an increase in designers embodying diverse experiences \cite{West:2019}. Similar to our argument, such calls argue that the embodied designers project an embodiment of self into the technologies they develop through data and modelling choices. This argument, in line with \citet{Haraway:1988} suggests that it is only through the recognition and promoting different embodiments that certain perspectives, understandings, and uses can be achieved.

\subsection{Embodiment in Data}
As \citet{Gitelman:2013} argues, datasets do not exist naturally but must be produced. Considering this production of data through \citet{Haraway:1988}, datasets can be understood as a form of knowledge that is produced through disembodying embodied experiences. Subjectivity can thus stem from a number of sources including the source of the data \cite{Gitelman-Jackson:2013}, the data sampling method \cite{Shah:2020}, and the selection of annotators \cite{Waseem:2016,Derczynski:2016}.

Grounding our discussion in Natural Language Processing, we show how subjectivity manifests itself in machine learning models through a number of meaning-making processes, modelling choices, and data idiosyncrasies. We seek here to highlight the subjective and embodied nature of of data and classifications and that by taking a position of objectivity, we cannot do justice to the needs and wants of individuals or communities.

\subsubsection{Natural Language Processing Tasks}

A range of, if not all, Natural Language Processing tasks are highly sensitive to the subjective values encoded in data. While such issues have frequently been studied in the context of high-level tasks, such as machine translation and abusive language detection, less attention has been given to core Natural Language Processing tasks. Notably, the primary object of study of biases in core Natural Language Processing has been the Part of Speech tagging task \cite{Blodgett:2016,Jorgensen:2016} for which reason we also investigate the task.
Generally, we argue that by removing the adjudication of content, be it abusive language, translations; text simplification; part of speech tagging; or any of the many other tasks natural language processing as a field has and is addressing, from the user experiencing the phenomenon, we delegitimise the very tools that are built through a cloak of presumed objectivity which is neither truly neutral nor objective.

\paragraph{High-level tasks} High-level tasks that require semantic and pragmatic understanding, e.g. machine translation, dialogue systems, metaphor detection, sarcasm detection, and abusive language detection are all highly sensitive to subjective values encoded in the data. In machine translation, research has identified a range of issues including stylistic \cite{Hovy:2020} and gender biases \cite{Vanmassenhove:2018}. Particularly issues that pertain to the reinforcement of sexist stereotypes have been the object of academic \cite{Escuda:2019} and public \cite{Locklear:2018} scrutiny. A classic example of stereotypical translation are the translations stereotyped occupations from a language that does not contain grammatical gender to a language that does, e.g. the translations of \textit{doctor} from English (unmarked for gender) to the German Arzt (marked for masculine) and \textit{nurse} from English (unmarked for gender) to Krankenschwester (marked for feminine). Here we see that the ``objective'', yet stereotypified translations are embodied in a patriarchal context which delegates high prestige to men and low prestige to women. While the translations may be correct in individual cases, they are not always correct. Moreover, assigning a single gold label to a given translation in itself provides an issue as an input text may have several distinct and correct translations. However, most training training and evaluation algorithms assume that there exists a single correct translation, and that is the one the model is provided for training and evaluation. We note that word embeddings similarly harbour stereotypical associations \cite{Bolukbasi:2016}.

%similarly to the example of translations of \textit{doctor} and \textit{nurse} in machine translation, word embeddings have been shown to harbour similar stereotyped associations, e.g. that the feminine equivalent of masculine-coded \textit{programmer} is to \textit{homemaker} \citet{Bolukbasi:2016}.

The issue of highly subjective `truths' and gold labels for data extends to several other tasks such as text simplification and abusive language detection. In text simplification, numerous datasets make the claim that some words, sentences, or texts are difficult to read while others are easy. These labels are typically provided by human annotators who may agree on some labels and this agreement may aid in the ability of models trained on such data to generalise to other datasets. However, the process of externalising the labelling process disembodies the data and subsequent models from the embodiments of the diverse set of users of simplification technology and how text difficulty manifests for those users \cite{Bingel:2018}.

Further, as we see in abusive language detection, if the positionality of the adjudicators deviate within the group of adjudicators less consistent annotations can be derived \cite{Waseem:2016}, which harm both the model and the supposed users of it. Many other causes and effects of disembodiment have been considered in the task of identifying abusive language. For instance, \citet{Waseem:2018} argue that datasets for abusive language embody a white perspective on respectability, finding that almost all uses of the \textit{n-word} are tagged in the positive class in the dataset released by \citet{Davidson:2017}. This in itself does not necessarily embody a white perspective on respectability, as the word does have many pejorative uses \cite{Croom:2013}. However, the word is also frequently used in the Black diaspora for a multitude a reasons \cite{Croom:2013,Rahman:2012}. Indeed \citet{Waseem:2018} find that a large subset of the documents that contain the \textit{n-word} in \citet{Davidson:2017} that are labelled as hate speech and offensive language are likely to be in-group uses. In an effort to address such disparities, \citet{Sap:2019} propose providing annotators with an \textit{projected} racial\zw{Raciolinguistic?} embodiment of the speaker (see \autoref{sec:fairlitt} for more detail). They find annotators, when provided projected racial attribution are less likely to label projected racialized speech as hateful or offensive.

\paragraph{Core Natural Language Processing tasks}
Beyond these issues existing in high-level tasks which may require a certain level of cognitive abstraction, they also exist in lower level, core Natural Language Processing tasks such as Part of Speech tagging and dependency parsing. While we restrict our example here to part of speech tagging, we contend that precisely the same arguments apply to dependency parsing.

Considering part of speech tagging, we find multiple junctions at which theory and data influence the process of developing tagsets. First, the tagset is developed based on a subjective linguistic theory that licenses some tags while rejects other. This linguistic theory is typically informed by observations on specific types language in the data it is developed to describe. Second, in the choice of sources of data. If the observed language production is a forum dedicated to computer games, the linguistic theory used to develop the part of speech tagset, the linguistic theories that form the basis of the tagset are likely to focus on informal, and perhaps adolescent communication patterns. If on the other hand, the source of data primarily consists of newswire documents, the linguistic theory is likely to specifically address language production in formal settings. Third, in the development of a dataset of part of speech tags, we see similar issues of adjudication as for the high level tasks.\footnote{Although this may be mitigated by using trained linguists to label the dataset.}
Thus, the development of part of speech tagsets and datasets it is applied on is a practice in developing descriptors and data which are mired in the context of the language production they seek to describe.

An example of one such tagset is the Penn Treebank tagset \cite{Marcus:1993}, the \textit{de-facto} standard for describing English word classes in Natural Language Processing. The Penn Treebank tagset was developed on primarily financial newswire text and published works in the U.S. in 1961 \cite{Francis:1982}. The tagset was primarily motivated by economic factors, such as there being several word classes that were ambiguous or word classes which occurred with such low frequency that they might only describe a single word. The Penn Treebank tagset was thus developed with formal Standard American English in mind and is thus better suited to describe language which conforms to the English the underlying theory the tagset describes than other varieties, sociolects, or slang \cite{Jorgensen:2016}.
This issue becomes even more drastically apparent when a tagset developed for English is forced upon some other language, which it is far removed from being able to describe.

\subsection{Embodiment in Modelling}

While datasets are an important source of how a model may be embodied, machine learning methods themselves encode which embodiments are highlighted and which are subjugated. We primarily focus on supervised machine learning in our exploration of how models exacerbate disembodied positions, as unsupervised methods are more directly volatile to subjective choices of the researcher, e.g. how the data is represented and which parameters the model is subject to.

As we seek to distinguish distinct model behaviours, we offer that models act on a spectrum from \textit{localized} to \textit{global} behaviour. In this conceptualisation, localized behaviour refers to when a model seeks to ground the datum within the context it is derived from, often using knowledge external to the training data, e.g. context-aware models \cite{Garcia:2019,Devlin:2019}. Conversely, global modal behaviour instead operates only within the realm of the training data it is trained on, i.e. models that compound multiple senses of a word with little or no regard to their local contexts. Although language production is situated within a wider socio-political context of society, we limit our use of ``context'' to the entirety of the sentence provided to the model.

By virtue of the subjective nature of embodying a datum within its context, there is large variation in how locally acting models may be developed. One tactic to situate datum within its context is through the use of transfer learning which allows for knowledge produced outside of the training data to alter what the model embodies. For instance, should a dataset be embody the language production within multiple sociolects, through the use of pre-trained language models \cite{Devlin:2019} or mixed-member language models \cite{Blodgett:2016} deeper information about the sociolects in question can be provided by using the context of the sentence to identify how to situate the representation of a document.\footnote{As different language and dialecal varities may not be equally distributed in training data for contextual models \cite{Dunn:2020}, similar issues of which bodies are given the privilege space palgue such models \cite{Tan-Celis:2019}.} The Multi-task learning paradigm also offers an avenue for embodying data in their contexts through their ability to encode information about the creator of the datum \cite{Benton:2017,Garcia:2019}. Transfer learning can similarly be applied to direct the model to embody the context a datum is derived from. For instance, \citet{Romanov:2019} encode demographic information of the datum's creator into the model in efforts to deter models from learning stereotyped representations of marginalised speakers and communities.

Globally acting models on the other hand do not afford such embodiment. Through their reduction of a features in a model to a single sense, they are inherently unable to take into account the embodiment of the author, even if they are provided signals for how to embody a document at training and inference time due to the fact that such models remake meaning according to the distribution of features. Any step taken towards embodying datum in its original context move globally acting models along the spectrum towards being locally acting models. An example of such a step are word embeddings. Through their representation of words by the words that co-occur with the word's neighbouring words, thus assuming a similarity between the word and other words. While they provide a slight shift towards locally acting models, the frequency-based nature of how closely associated a word is, they fail to take a meaningful step away from being globally acting models, as all instances of a token occurring in the dataset will be reduced to a singular representation that does not take the surrounding context, i.e. the sentence, into account.

\subsection{Discussion}

Given that subjective choices and biases masquerading as disembodied ``objective'' positions permeate through the machine learning pipeline, the quest for objectivity and bias-free machine learning becomes redundant. In fact, the search for objectivity in the pipeline creates a veneer of social progress that may cause further harm already marginalised communities by obscuring and entrenching the dominance of certain bodies over others. Without taking the unique embodiments of all data subjects into account, this imaginary of fair then only serves as a justification of maintaining oppressive structures that are inherently harmful and reductive. 

Only by recognising the positionality of the designers of machine learning models can one account for what (and whom) ones own position and the models derived from it privilege and sanction, and the political consequences of these. As data permeates through the machine learning pipeline, a consideration of how data is embodied can empower the designer to answer specific questions that are embodied and mired in context. Such considerations allows designers to interrogate the contexts within which data are created and meaning is made at each step in the dataset creation process. It is through such recognition of context and embodiment that we can realise that as context change, so does the applicability data. Further, only by such recognition of the deeply complex nature of embodiment and data can we hope to ask and ascertain which views the models privilege and which are subjugated.

Although there are methods with which we can move towards more localized machine learning models, what positions are given space remains a political question. It is only through wholly representing the context and embodiments of the data creator and the datum that we can hope to arrive at sufficiently localized models.

Thus, rather than asking how to eliminate bias and subjective experiences from machine learning in the pursuit of objectivity, shifting our question to consider embodiments would ask us to reflect on the subjective experiences that are given voice. And such a shift would require us to ask and reflect upon which bodies' subjective experiences we need to account for to give voice to socially, and computationally, marginalised groups.

\section{Machine Learning as a Conservative Practice}
\zw{Write about dominant and subjugated discourses for machine learning here; bring in that ML is a conservative practice}

\zw{Citations needed: Foucault on dominant and subjugated discourses, Fraser on subaltern publics, some archival theory on marginalising effects of dominant discourses.}
