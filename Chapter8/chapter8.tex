\ifpdf
    \graphicspath{{Chapter8/Figs/Raster/}{Chapter8/Figs/PDF/}{Chapter8/Figs/}}
\else
    \graphicspath{{Chapter8/Figs/Vector/}{Chapter8/Figs/}}
\fi

\chapter{Conclusion}
%- - Chapter arguments:
%- - - Chap 4 (Dirt): Abuse detection is broken because it operates with trying to classify uncertain boundaries
%- - - Chap 5 (LIWC): If the boundary is porous then maybe we're looking at the wrong problem, maybe the right problem is what people mean
%- - - Chap 6 (MTL): If we must look at dirt, then maybe we need to look more broadly to understand how people are trying to communicate
%- - - Chap 7 (Disembodied): But maybe this is all wrong if what we want to do is protect people but we're actually harming them.
%- - - Chap 8 (Conclusion): Well, maybe we need to rethink this all, but some components of rethinking it are in thinking about the things around the abuse, not the abuse itself.

In this dissertation I have sought to explore the content moderation infrastructures that are built for classifying textual abuse in online spaces.
The contributions of the thesis are structured around four central themes:
How the notions of ``healthy'' and ``toxic'' content are operationalised, the implications of such operationalisation and how these come to embody hegemonic imaginaries on respectability;
how large vocabulary reductions, that represent the mental and emotional states of speakers rather than their words, influence the ability of models to classify in-domain and out-of-domain data;
how different, seemingly related, tasks can be used to jointly optimise model representations to gain models that more closely come to reflect the contexts a given speaker is operating in when speaking;
and finally, how the subjective embodiments of data subjects and modellers alike are embodied in the machine learning pipeline and how these collectively come to privilege hegemonic discourses.
These four distinct themes are connected through two over-arching questions: How does the human fit into abusive language technologies and how can machine learning models for content moderation come to more closely respect and represent their humanity?

To adequately answer these questions, I address each theme in turn through a multi-disciplinary perspective that affords insights into the technical, social, and political dimensions of content moderation infrastructures for abusive texts.
The contributions in this dissertation are thus in part of theoretical nature and in part of experimental nature.
By examining the questions through theoretical and experimental lenses, I begin to uncover the political and technical complexities of content moderation infrastructures and obtain insights that are opaque when these questions are addressed purely theoretically or purely experimentally.
