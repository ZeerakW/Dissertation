\ifpdf
    \graphicspath{{Chapter8/Figs/Raster/}{Chapter8/Figs/PDF/}{Chapter8/Figs/}}
\else
    \graphicspath{{Chapter8/Figs/Vector/}{Chapter8/Figs/}}
\fi

\chapter{Conclusion}
%- - Chapter arguments:
%- - - Chap 4 (Dirt): Abuse detection is broken because it operates with trying to classify uncertain boundaries
%- - - Chap 5 (LIWC): If the boundary is porous then maybe we're looking at the wrong problem, maybe the right problem is what people mean
%- - - Chap 6 (MTL): If we must look at dirt, then maybe we need to look more broadly to understand how people are trying to communicate
%- - - Chap 7 (Disembodied): But maybe this is all wrong if what we want to do is protect people but we're actually harming them.
%- - - Chap 8 (Conclusion): Well, maybe we need to rethink this all, but some components of rethinking it are in thinking about the things around the abuse, not the abuse itself.

In this dissertation I have sought to explore the content moderation infrastructures that are built for classifying textual abuse in online spaces.
The contributions of the thesis are structured around four central themes:
How the notions of ``healthy'' and ``toxic'' content are operationalised, the implications of such operationalisation and how these come to embody hegemonic imaginaries on respectability;
how large vocabulary reductions, that represent the mental and emotional states of speakers rather than their words, influence the ability of models to classify in-domain and out-of-domain data;
how different, seemingly related, tasks can be used to jointly optimise model representations to gain models that more closely come to reflect the contexts a given speaker is operating in when speaking;
and finally, how the subjective embodiments of data subjects and modellers alike are embodied in the machine learning pipeline and how these collectively come to privilege hegemonic discourses.
These four distinct themes are connected through two over-arching questions: How does the human fit into abusive language technologies and how can machine learning models for content moderation come to more closely respect and represent their humanity?

To adequately answer these questions, I address each theme in turn through a multi-disciplinary perspective that affords insights into the technical, social, and political dimensions of content moderation infrastructures for abusive texts.
The contributions in this dissertation are thus in part of theoretical nature and in part of experimental nature.
By examining the questions through theoretical and experimental lenses, I begin to uncover the political and technical complexities of content moderation infrastructures and obtain insights that are opaque when these questions are addressed purely theoretically or purely experimentally.
The dissertation has been structured such that I start and finish with primarily theoretical contributions while the primarily experimental contributions constitute the middle of the thesis.
I choose this structure to remain faithful to the machine learning pipeline for content moderation, addressing first definitional questions and then questions of modelling.
Finally, I take a step back and reflect on the machine learning pipeline from start to an end.

% TODO You are here, describe what you do and what you find in the chapters!
I begin with \cref{chap:filter}, where I consider how the concept of sanitisation is invoked and operationalised in content moderation settings through terminology such as ``healthy conversations'' \citep{Twitter:Health:2018} and ``toxic'' content \citep{Wulczyn:2017,Perspective:Github}.
Drawing on Mary Douglas' \citeyearpar{Douglas:1966} theories of social pollution and Josh Lepawsky's \citeyearpar{Lepawsky:2019} on dirt an toxicity, I argue that content moderation infrastructures can be understood as practices of meaning making and community building.
Moreover, I reposition the role of content moderation from negative removal to positive re-ordering of environments.
Such positive re-ordering seeks to maintain communities and their cultural values, e.g. the sanitisation of online platforms to constitute a child-friendly environment.
Such sanitisation of online spaces carries with it social and cultural codes of ``dirty'' and ``sanitised'', the ``toxic'' and the ``healthy''.
Precisely as \citet{Douglas:1966} reminds us ``no single item is dirty apart from a particular system of classification in which it does not fitâ€™'' \citep[pp. vii]{Douglas}, which in terms of content moderation systems means that they are situated in specific cultural understandings of ``toxicy'' and ``healthy''.
For the moderation of language, these cultural codes are often applied to speech produced by marginalised communities.
For instance, as \citet{Davidson:2019} show, automated content moderation systems disproportionately identify African American English as being abusive while \citet{Dias:2020} show that commercial systems find white supremacist speech to be more acceptable than the speech of queer drag queens.
Thus content moderation systems engage in toxic slippage, where they on one hand fail to protect marginalised communities and on the other hand disproportionately police those communities.

Through an analysis of two different publicly available content moderation systems, I argue that the marginalising effect of content moderation systems are embodied in the modelling pipeline in three distinct processes each of which are often made opaque.
First, in the operationalisation of ``toxic'' or ``abusive'' as these definitions and understandings are derived from cultural understandings of toxicity.
Second, they are embodied in the choice of annotators and the frequent use of majority voting on each document for annotation, encoding a hegemonic perspective on acceptability.
Third, beyond the annotation procedures, dominant discourse perspectives on respectability are embedded into the modelling e.g. in the use of pre-trained embeddings that are trained on texts available on the web.
Through these three means, content moderation systems are constituted by social hegemonies while simultaneously constituting these hegemonies by the replication of them.
Thus, content moderation systems partake in culture wars on what ``acceptability'' is to constitute.
When content moderation systems engage in such conflicts in power dynamics, they must also address the ongoing reconfigurations of dirt as the cultural contexts surrounding these change.

Using Mary Douglas' framework on social pollution and dirty, I conclude that the notion of a ``sanitised'' space also raises the question of whom it is sanitised for.
Until content moderation infrastructures are centred around the experiences of marginalised communities, they will continue to systematically disenfranchise such communities from the full ability to create boundaries that can constitute their subjective experiences.











In \cref{chap:mtl}, I further investigate how optimising a model for multiple tasks simultaneously can influence model performances on specific forms of abuse.
Using hard parameter sharing Multi-Task Learning \citep{Caruana:1993}, I develop Multi-Layered Perceptron models that are optimised for different forms of abuse and investigated the impact of different auxiliary tasks for each type of abuse.
I narrowed down on the number of abusive tasks from \cref{chap:liwc}, which considered five different abusive language detection tasks to three tasks in \cref{chap:mtl} that I treat as main tasks.
Moreover, in \cref{chap:mtl}, I do not collapse the classes into binary classification, as I do in \cref{chap:liwc}.
The three tasks that I consider are: Disambiguating offensive content from hateful content and non-offensive content, disambiguating between toxic and non-toxic content, and classifying racism, sexism, and the absence of these.
For auxiliary tasks, I explore two different kinds of auxiliary task.
First, I explore four auxiliary tasks for abuse detection: hate speech detection, expert annotated hate speech detection, toxicity detection, and offensive language detection.
Three of these also serve as main tasks and a dataset is not used as an auxiliary task when it is also the main task selected for optimisation.
For the auxiliary tasks that are not abusive in nature, I explore the impact of sarcasm detection, detecting moral sentiment, and detecting whether an argument is based in facts or feelings on the main task.
I compare these MLP models with three different baselines, a linear SVM, a single-task MLP, and an ensemble classifier.
I find that although not all MTL MLP models outperform all baseline models, they consistently outperform the single-task MLP when given a single auxiliary task, regardless of whether the auxiliary task is abusive in nature or not.
Moreover, I find that not all auxiliary tasks are equally suitable, for instance, I find that the \textit{Hate Expert} auxiliary task is only useful when the main task is the \textit{Hate Speech} task. 
These two datasets are annotated from the same data sample and follow the same annotation guidelines.
More generally, I find that there are benefits in main task classification performance when the auxiliary task is abuse detection and at least one of two conditions are met:
1) the main task and auxiliary task datasets are sampled from the same, or 2) the main task and auxiliary tasks share annotation goals.
The first condition arises from observing the patterns in auxiliary task impact across all three main tasks as the \textit{Offence} auxiliary task yields improvements when the main task is either the \textit{Hate Speech} task or the \textit{Toxicity} task, where data source and dataset goals match, respectively.
When the auxiliary is not abusive in nature, \textit{Sarcasm} and \textit{Moral Sentiment} auxiliary tasks stably increases performance across the different main tasks.
The \textit{Argument Basis} auxiliary task on the other hand does not provide improvements across all datasets. 
When considering combinations of auxiliary tasks, I find that using exclusively abusive tasks, or exclusively non-abusive tasks tend to obtain good scores, though these tend to be outperformed when the auxiliary task setting uses a combination of abusive and non-abusive tasks.
In such settings where abusive and non-abusive tasks are combined, the best performing settings tend to be where one abusive auxiliary task is used and two or more non-abusive tasks.
Additionally, \textit{Sarcasm} frequents in all of the best performing configurations.

