% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\chapter{LIWC/text transformation chapter}\label{chap:liwc}

One of the key issues in machine learning for content moderation is that such systems both in deployed settings (see \autoref{chap:filter}) and in research (see \autoref{chap:intro} and \autoref{chap:nlp}) over-fit to individual tokens that see over-representation in the positive and negative classes respectively. While research efforts have been made to address such issues \cite{CITE: cite papers that try to address overfitting}, the problem of over-fitting to words and identity markers remain an open question for the field. While some such approaches have addressed this problem by replacing certain words and phrases with more general tokens \cite{CITE: Replacing token papers} or masking \cite{CITE: Masking token paper} tokens. Other work has attempted to address the problem by treating it as a problem of dataset bias \cite{Dixon:2018}. Here, I propose a different approach which serves to address the issue of models over-fitting to tokens by 1) minimising the vocabulary in order to avoid over-fitting to distributional skews of low-frequency tokens across classes; 2) representing documents in terms of how they represent thoughts, feelings, and personality; and 3) through such vocabulary minimisation highlight the importance of how words are used rather than their surface forms while retaining model performance. An additional benefit of such vocabulary reduction is a proportional reduction in model size and training time for complex models such as neural networks, resulting in models that have a smaller environmental impact \citep{Strubell:2019}.

Through the use of the Linguistic Inquiry and Word Count (LIWC) dictionary \cite{LIWC:2015,Original LIWC Citation}, I pre-process documents from large vocabularies, that are riddled with obfuscations and intentionally and unintentionally misspelled words into a smaller vocabulary set representing instead psycholinguistic properties of words. Through a reduction of thousands, or in some cases hundreds of thousands, of unique tokens to hundreds of LIWC categories, I aim for models to gain deeper insight into language patterns of abuse than simply selecting the most frequently used tokens. Moreover, I show that such a reduction is accompanied by a negligible intra and inter dataset performance in comparison to models using the full surface-token vocabularies.

\zw{Double check the reduction numbers}
Through the use of simple deep neural networks and `shallow' linear models, I show that through reducing the vocabulary sizes by up to $99\%$ and the number of model parameters by up to $99\%$, while increasing the depth of the information in the remaining vocabulary, it is possible to achieve comparable performances within datasets and mild improvements on out-of-domain datasets. This holds two strong implications for future research on computational hate speech detection: first that current approaches through an over-reliance on surface forms are computationally inefficient, and second that the exclusive use of surface forms of tokens can lead models to overly attend to the occurrence of certain tokens and variations (e.g. prominent misspellings) \citep{Rottger:2021}. Finally, as datasets for hate speech detection frequently contain biases along racialised and dialectal lines \citep{Waseem:2018,Davidson:2019}, the use of LIWC can serve as small aid in avoiding such biases as dialectal spellings of words are likely to not appear in the dictionary, thus being relegated to unknown tokens (see \autoref{tab:liwc_tok} for synthetic examples of LIWC representations). Thus, this chapter seeks to provide answers to the following research questions:

\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{Can LIWC provide a meaningful substitute to using words or sub-word tokens as input tokens and how is model performance affected by such a substitution?}}
        \item{\textit{What are the implications of using LIWC as input on model development and model size?}}
        \item{\textit{What are the implications on generalisability of LIWC-based models?}}
    \end{enumerate}
\end{minipage}

\newpage
\section{Previous work}

In the interest of curtailing the spread of online abuse, a large number of technical approaches have been considered in the ever-increasing body of research on the topic (please see \autoref{sec:nlp} for a broad overview on the topic). Here, I focus on three different strands of research. First, I briefly introduce the LIWC dictionary. Second, I consider manual development of features for machine learning models as it is necessary to form hypotheses for what might may serve as indicators of abuse on the basis of the dataset and problem in question. Third, I examine neural network approaches for abusive language detection. Finally, I consider the growing body of research devoted to examining the generalisability of computational models for abusive language detection. I restrict my attention to studies in conducted on abuse in English as it is most pertinent to this work.

\subsection{Linguistic Inquiry and Word Count}

The Linguistic Inquire and Word Count dictionary and software was initially developed by \citet{Pennebaker:2001} in an effort to address the issue of high disagreement and negative effects on well-being of judges, as they reviewed essays written on people's experiences of emotional upheaval. In order to minimise such costs, \citet{Pennebaker:2001} turned to computationally counting words that were in $80$ ``psychology-relevant categories'' in order to gain an understanding of the emotional states and cognition of the authors at the time of writing. By passing over a large body of text within a single document, e.g. personal essays, the \citet{Pennebaker:2001} compute how percentage occurrence of each invoked category. While there are some examples that appear clear cut, e.g. the categorisation of articles such as `a' and `the',
% TODO review footnote
\footnote{Though the categorisation of word classes may seem trivial, however which class a word is categorised into depends on the linguistic theory that a given classification is based on \cite{CITE: Wait for Adina}.}
other word classes, such as ``emotion word categories'' are more clearly subjective and require deeper human consideration \citep{Tauscik:2010}.
Though LIWC was initially developed using long form texts, the version of the dictionary that I use in this dissertation is an expanded version that also used Twitter and ``blogs'' in the development of the dictionary \citep{Pennebaker:2015}. As such, though not originally intended for the use on short-form messages, LIWC has evolved with the rise of new forms of communication in efforts for the dictionary to accurately reflect language use in short-form documents.\vspace{5mm}

In this thesis, I utilise LIWC to provide the word categories that each word invokes, and rather than compute the overall word classes exhibited in a document, I use the LIWC categories of each word in a document as an alternative representation of the document from which percentages of word categories invoked can be recovered. Thus, my approach diverges slightly in the goals of using LIWC, however it does not diverge in the method for obtaining information about the psychological state of the author.

\subsection{Modelling}
\subsubsection{Manually Crafted Features}

A large body of work has sought to use manually developed features for online abuse detection \citep{Davidson:2017,Waseem:2017,Ibrohim:2019,Vega:2019,Wiegand:2018,Tian:2020,Kumar:2019,Fortuna:2018}, showing performance boosts from using manually developed features such as the predicted author gender \citep{Waseem-Hovy:2016} or Part-of-Speech (POS) tags \citep{Davidson:2017}. The primary reasons for using manually crafted features is two-fold: First, by using manually crafted features it is necessary to have some understanding of the data at hand and some intuition about which features may distinguish the classes from the data from one another. Second, as manual features are frequently used with models that don't use neural architecture, they allow for interpretable machine learning models, in the sense that one can often identify how each token contributed towards a final prediction. Moreover, as features are often computationally fast to compute, the use of features along with their expressive interpretability, allow for quickly testing hypothesis surrounding online abuse and its nature.
Considering a handful of systems that use some of the most frequently used features for the development of automated systems for detecting various forms of online abuse, distinct modelling choices, features and rationales for their use become prominent. Here I provide a brief overview of prominent features; how they are used, including which models and feature weighting schemes they are used with; and the explicit and implicit rationales for the use of each feature.

First, the most common feature used, and rarely used on its own, is a Bag-of-Words (BoW) \citep{Fortuna:2018,Davidson:2017}, where each token in a document is treated as independent from the remainder of the document. The use of this feature frequently relies on the use of stop-word lists to remove tokens that are bound to occur frequently across a majority of documents, such as determiners, to avoid models from learning spurious correlations with such words and an individual class due to fluctuations in the data. The understanding of abuse that underlies this feature, is that the occurrence of some tokens are likely to disproportionately occur in abusive contexts, and that those tokens, in isolation, will indicate abuse. Several works have complicated this notion \citep[e.g.]{Waseem:2018,Davidson:2019}, arguing that tokens, in isolation, do not provide the necessary context to determine whether a text is abusive and due to certain perspectives on abuse being overly represented \citep{Waseem:2016} in annotation guidelines and annotations, some words that have been reclaimed, and thus have an innocuous usage potentially in addition to an abusive use, may be disproportionately represented in the positive classes.

To address the issue of token independence, several approaches use n-grams, often bi-grams \citep{Waseem:2016} and tri-grams \citep{Davidson:2017} to aid with identifying abuse. Here, by considering groups of sequential token occurrences independently from one another, a step is taken away from the independence of individual tokens, instead to the independence of short sequences of tokens. Due to this remaining independence assumption, similar issues arise to the limitations of BoW hold for n-grams.

POS tags have also seen frequent use in abusive language detection tasks \citep{Fortuna:2018} and are often used as n-grams. The intuition behind the use of POS tags for abuse detection is that abuse may differ from non-abuse in terms of linguistic structure. While n-grams of POS tags with an independence assumption may not reveal the full depth of the linguistic syntax available through POS tagged data (in contrast to the POS tags of the entire sequence being treated as a single feature), it does relay \textit{some} information on the linguistic structure which has been proven helpful for predicting abuse \citep{Fortuna:2018}.

Another frequently used feature is sentiment analysis \citep{Fortuna:2018} with the underlying assumption that abuse and negative sentiment are correlated, and can thus aid in detecting some forms of abuse. Similarly to BoW and n-grams, this is a feature that is most frequently used in combination with other features as sentiment alone is not presumed to be a good predictor of abuse \citep{Fortuna:2018}. Sentiment as a feature, like the use of LIWC proposed in this dissertation, assumes that some higher level reasoning about the data can be helpful to automatically detecting abuse. Specifically, its use suggests that there the concepts of negativity and hostility towards entities will be relevant to detecting abuse in texts. Notably, some previous work that uses sentiment as a feature for abuse detection \citep{Davidson:2017} relies on previously built systems for detecting sentiment. An implication of using previously trained systems for computing sentiment, rather than assuming that sentiment can be extrapolated only from the dataset, is that sentiment and abuse, while correlated are not equated and thus that the task of detecting sentiment, while related is a distinct task from detecting abuse. As such, a sentiment and abuse are tasks that in some cases co-constitute each other while there may be no correlation in other cases.

Finally, LIWC has previously been proposed as a feature for the classification of abuse in a small number of studies \citep{Nina-Alcocer:2019,Joksimovic:2019}. In these studies, LIWC has been used in conjunction with other features such as lexical features (e.g. word n-grams) and syntactic features (e.g. POS tags) \citep{Joksimovic:2019}. This use of LIWC, similar to the motivations for its use in this chapter, relies on an assumption that the mental states of the speaker and the interpretations of readers will relay information on the intention of the speaker to cause offence. For instance, \citet{Nina-Alcocer:2019} compute the percentages of emotions that are expressed in abusive documents in efforts to identify correlations between impassioned speech and abuse, asserting an intuition that abusive speech is likely to occur in individual moments dominated by emotion rather than rationality. A position that \citep{Waseem:2016} argue is likely as they find that considering the top $100$ most frequently occurring tokens, ranked using Term Frequency - Inverse Document Frequency (TF-IDF), does not aid in the prediction of hate speech, suggesting that in many cases it may be a question of moments of abuse rather than consistently abusive people.

All features must be weighted, either through raw counts or their relative frequency. One such frequently used weighting scheme is TF-IDF which weights features by their relative frequency in the corpus \citep{Fortuna:2018}, assigning higher weight to the features that are rare corpus-wide and lower weights to those that common. As such, TF-IDF can be a useful measure to address the dominance of high-frequency tokens. At the same time, TF-IDF also increases the capacity for models to overfit to the corpus and generalise poorly, as tokens that are unique to a corpus may not exist in other data or even be common to other data. The use of n-grams as features provides a similar double-edged benefit, where models learn sequences of words, in abuse detection the most common n-grams are unigrams, bi-grams, and trigrams. Such word-sequences can be helpful for models in uncovering patterns of language use in the corpus but are also sensitive to the vocabulary changes that occur across datasets. For instance \citet{Waseem-Hovy:2016} train a Logistic Regression classifier and identify that character n-grams of innocuous words such as `Islam' and `Muslim' rank as some of the most predictive features due to the disproportionate occurrences of such terms in the hateful classes.

Many of the previously mentioned works use similar machine learning models, with a particular dominance of Logistic Regression and Support Vector Machines (SVMs) (please see \autoref{chap:nlp} for more detail). One notable exception to this is the work of \citet{Gorrell:2018}. In this work, the authors use a ``set of NLP tools, combining them into a semantic pipeline'' \citep[pp. 601]{Gorrell:2018}. Rather than using supervised classification techniques, a rule-based systems was developed to detect abuse that they argue allows for a interpretable and easy to modify method to address weaknesses of the approach without the need for additional large quantities of data.\footnote{This detail on the rule-based nature of the classification systems was provided by Genevieve Gorrell in personal communications.} However, this approach is a laborious one as it requires the researchers to manually identify patterns of abuse and construct rules that can address such patterns along with any exceptions to the patterns that are not abusive.
\vspace{5mm}

In this chapter, we take inspiration from the use of manually crafted features as a way to provide testable hypothesis while departing from the notion of feature generation. Specifically, we hypothesise that LIWC categories can provide deep information for predictive modelling that can allow for high performance in spite of token sparsity when using neural network methods.

\subsubsection{Neural Networks}\label{sec:liwc_nn}
Though the earliest models for the tasks were predominately linear models that used manually generated features \citep{Waseem-Hovy:2016,Davidson:2017,Warner:2012} more recent work has been dominated by the development of neural network based models for automated abuse detection, posting ever-evolving State-of-the-Art models and classification performances \citep[e.g.]{Park:2017,Badjatiya:2017,Zimmerman:2018,Stoop:2019,Isaksen:2020}. Here I consider a handful of neural network methods for detecting abuse, focusing on the distinct implications following the modelling choices and the logics that underpin them. As all neural network based methods that I examine receive only the text as input, the primary differences between the models is in their use and organisation of different types of layers and the loss function selected for the respective models.

The most commonly used architecture for neural networks that in the surveyed literature is a CNN \citep{Park:2017,Gamback:2017,Wulczyn:2017,Kolhatkar:2021,Zimmerman:2018,Wang:2020}. As CNNs have been the subject of particularly interest, a number of distinct modelling approaches have been proposed. First, relying a simple neural network architecture, \citet{Kolhatkar:2021} use GloVe embeddings as the first layer, followed by three convolutional layers (that have window sizes $3, 4,$ and $5$, respectively) with global maximum pooling layers. Prior to passing to an output layer, dropout is applied to the output of the convolutional layers which is then passed to a dense layer. For all layers prior to the output layer use a ReLU (see \autoref{chap:nlp} for more detail) activation function. The output layer applies the sigmoid function to provide a prediction from the model. This model most closely resembles the CNN architecture used in this chapter. As this model uses a pre-trained word-embedding layer as its input layer, the input the model receives are documents that have been subject to tokenisation processes.

A different architecture is proposed by \citet{Park:2017}. In their work they compare a single classifier, what they dub a `one-step classifier', that predicts the final classes directly with a stacked architecture of two models, or a `two-step classifier' in their vernacular that first predicts whether content is abusive and second predicts which type of abuse the documents predicted as abusive are. There are two governing distinctions between the two-step architecture proposed by \citet{Park:2017} and the architecture proposed by \citet{Kolhatkar:2021}. First, \citet{Kolhatkar:2021} acts as a one-step classifier whereas the architecture proposed by \citet{Park:2017} acts in two steps. Second, \citet{Kolhatkar:2021} only acts on documents tokenised into words and punctuation whereas \citet{Park:2017} propose a CNN that takes documents tokenised into words and punctuation in addition to documents tokenised into characters. \citet{Park:2017} show that through the use of a one-step CNN trained on word and character input, they achieve a performance boost obtaining a F1-score of $0.827$ on the datasets proposed by \citet{Waseem-Hovy:2016} and \citet{Waseem:2016}, though the performance boost is lost once a two-step hybrid CNN is used.

As CNNs build feature mappings by passing over the data using filters, they come with certain assumptions built into them. As researchers define the number of filters and the stride size, they also define the range within which they believe that relevant terms are likely to occur. The implication of this is then that there will likely be some, potentially overlapping, ranges that the models learn patterns from. Depending on how researchers define these, the models will develop feature mappings corresponding to the ranges provided.

Another frequently used architecture is LSTMs \citep{Badjatiya:2017,Kolhatkar:2021,Meyer:2019}. Here, \citet{Kolhatkar:2021} propose using a bi-directional LSTM that, like their CNN, has a pre-trained embedding layer, a recurrent layer, a dropout layer, and a fully connected output layer with a sigmoid activation to predict the output classes. \citet{Meyer:2019} on the other hand take develop on the idea of a hybrid CNN, developing a LSTM architecture that takes documents tokenised into words and characters as input. The word representation is obtained through tokenisation passed through an embedding layer and the character representation is obtained by processing the documents with a CNN.
Using these approaches, \citet{Kolhatkar:2021} show comparable performances between the CNN and bi-directional LSTM on their dataset. \citet{Meyer:2019} on the other hand show that a baseline model only using character level information performs comparably with other more complex approaches, obtaining a macro F1-score of $0.7923$ for the baseline and $0.7924$ for the final system on the dataset proposed by \citet{Waseem-Hovy:2016}, and notably outperforms several other previously proposed methods.

The use of LSTMs, that rely on recurrence, break with the independence assumption of the manual feature-based models. By recurring over a document, each new token is considered in conjunction with the previous tokens that have not been forgotten. In this way, an assumption is built into the models that through processing enough token sequences, it will be possible to identify patterns that connote abuse. Such a reliance on the text alone does not consider the positionality of abuse; \citet{Waseem:2018} argue only through understanding the context within which the speaker and audience exist in, is it possible to deem something as abusive. For instance, it is only through an understanding of the speaker that one can deem whether the \textit{n-word} is weaponised as abuse or is reclaimed to connote complex social identity.\vspace{5mm}

All methods described that rely on documents tokenised into sentences rely on pre-trained embedding layers (most frequently GloVe \citet{Pennington:2014}) that come with their own benefits and costs. For instance, word embeddings that are trained on web-text are likely to harbour social biases \citep{Bolukbasi:2016} that have been proven hard to address \citep{Gonen:2019}. On the other hand, they also allow for better representations of related concepts and will be less susceptible to creating different representations for closely related concepts as a result of dataset biases. For instance, the concepts `Television' and `T.V.' might only be distantly related, if at all, in a small dataset due to few co-occurrences within the dataset. In a larger dataset, spanning millions of documents, these two concepts are likely to appear as closely related as a robust language representation will likely have been achieved for such commonly occurring tokens.
The methods that rely on character embeddings are also subject to similar distributional concerns, however this can be a benefit when used in conjunction with word embeddings. As there are only a much smaller set of possible characters compared to words, less data is needed to train robust embedding layers, though the trained character embeddings will be particularly attuned to the dataset at hand. On the other hand, due to such particularity of the character embeddings, they are less likely to map well onto other domains even if they show good performance on the dataset that they are derived from.\vspace{5mm}

For the work in this chapter, the use of pre-trained embeddings is not appropriate for some models. Specifically the models that use LIWC-represented documents as LIWC embeddings are not publicly available or have been developed, to the best of my knowledge. Moreover, documents represented through LIWC categories are poorly suited for training general embeddings as only a small set of tokens are defined and they are not necessarily distributed in a fashion suitable for developing generalised such embeddings. Second, I don't use pre-trained embeddings in the architectures for all other models to ensure that any comparisons with the LIWC-based models are a direct comparison of the influence of using LIWC as input tokens and avoiding potentially confounding factors.

\subsection{Datasets}
In order to understand and validate my approach, I train a model on multiple datasets. Moreover, I take each model that is learned on a given dataset and apply it to all other datasets. To accommodate prediction on a model trained on one dataset to others, I reduced all classification tasks to a binary task of abusive and not-abusive. This has downstream implications for the construction of the datasets and for the validity of the prediction task on the auxiliary datasets.
First, the dataset distributions are modified as tasks with more than two classes see their data collapsed. For the some datasets, this means that the class imbalances are improved, as the majority class is non-abusive. The exception to this is the dataset proposed by \citet{Davidson:2017} where the largest class is the `offensive' class, which I combine with `hateful', further minimising the negative class.
Second, as each dataset has been collected with different rationales and annotated with distinct purposes (please see \autoref{sub:abuse_data} for more detail), direct comparisons, and subsequently model predictions on each dataset, can be at odds with the goals of the datasets. For this reason, high scores on prediction metrics on external datasets should be viewed as a weak indication of the ability to identify general patterns while low scores can indicate a number of factors including, but not limited to, highly distinct data sources, annotation strategies, and lastly the questions each dataset inherently seeks to ask.

With these concerns in mind, I decide to use datasets with distinct sources and are developed for different purposes. Rather than resist or seek to minimise the modelling concerns, I choose to lean into them to allow space for understanding how LIWC-based modelling may influence the training and model performance on each dataset as well as seek to gain an understanding on which axes model generalisation may be afforded using LIWC-based modelling (see \autoref{tab:vocab_sizes} for the vocabulary sizes for each dataset and input type).

In this chapter I use the \textit{StormFront} dataset~\citep{Garcia:2019}, the \textit{Offence} dataset \citep{Davidson:2017}, the \textit{Hate Speech} dataset~\citep{Waseem-Hovy:2016}, the \textit{Expert Hate} dataset~\citep{Waseem:2016}, and finally the \textit{Toxicity} dataset~\citep{Wulczyn:2017} (please see \autoref{sub:abuse_data} for a detailed overview on each dataset).

\subsubsection{StormFront}
First, I use the \textit{StormFront} dataset which is collected from the white supremacist web forum by \citet{Garcia:2019}. The data consists of $2,392$, split into $1,531$ training documents, $383$ documents for validation, and $478$ test documents. While the full dataset published consists of $10,000$ documents annotated as `hate' and `not-hate', with a large class imbalance towards non-abusive comments, I choose to use a balanced subset of the data provided by the authors to test how LIWC-based models perform when trained on a) small data and b) a balanced data distribution.
The dataset is initially split into a training and a test set, I created a validation set by pulling extracting a stratified sample from the training data, retaining the class balance from the balanced subset.

\subsubsection{Offence}
The second dataset used to train and evaluate my models is the \textit{Offence} dataset collected from Twitter by \citet{Davidson:2017}. This dataset was collected to distinguish offensive tweets from hateful ones. This dataset is distinguished from all other datasets in that the positive classes, i.e. `offensive' and `hateful' accounting for $1,430$ documents and $19,190$ documents, respectively. This leaves only $4,163$ documents in the negative class. Once binarised, the dataset consists of $4,163$ documents in the negative class and $20,620$ documents in the positive class. The dataset is provided by the authors as a single file containing all documents, so I create stratified splits of the data into a training set ($80\%$ or $19,826$ documents), a validation set ($10\%$ or $2,478$ documents), and a test set ($10\%$ or $2,479$ documents), retaining the class distribution in each split. Using this dataset further allows for an investigation into how sensitive LIWC-based modelling is to dataset skews.

\subsubsection{Hate Speech}
I also use the \textit{Hate Speech} dataset, which is collected from Twitter by \citet{Waseem-Hovy:2016}. This dataset contains $16,914$ documents that follow a more traditional class distribution for abusive language data. In this dataset the positive classes of `racism' and `sexism' are collapsed into a single positive class, `abuse', consisting of $5,355$ documents and the negative class occupying the remaining $11,559$ documents. The primary function this dataset serves in this chapter is to allow some insight into whether the LIWC-based models would function under a distinct annotation criteria that is motivated by academic studies in Gender Studies and Critical Race Theory on marginalisation, rather than social media guidelines for acceptable behaviour.

\subsubsection{Expert Hate}
The \textit{Expert Hate} dataset proposed by \citet{Waseem:2016} contains $6,909$ documents and is also collected from Twitter and is also designed as a multi-class classification task. In this dataset the positive classes consist of `sexism' ($13\%$ or $898$ documents), `racism' ($1.41\%$ or $97$ documents) and `both' ($0.70\%$ or $48$ documents) while the negative class consists of $84.19\%$ of the dataset. I reduce this down to a binary classification task and split the dataset into a training set ($80\%$ or $5,527$ documents), a validation set ($10\%$ or $690$ documents), and a test set ($10\%$ or $692$ documents) ensuring that binary the class distribution is retained. This dataset is annotated following the annotation guidelines proposed by \citet{Waseem-Hovy:2016}, however it is annotated using intersectional feminist activists as crowd-workers. This dataset then allows for testing the influence of LIWC-based models on data annotated by experts.

\subsubsection{Toxicity}
Finally, I use the \textit{Toxicity} dataset published by \citet{Wulczyn:2017}. This dataset was collected from Wikipedia editor discussion pages and annotated as `toxic' and `not-toxic' and it is the largest dataset with $159,686$ documents. These documents are provided split into a training set consisting of $95,692$ documents, a validation set with $32,128$ documents and a test set containing $31,866$ documents. Similarly to the \textit{Hate Speech} and \textit{Expert Hate} datasets, this dataset is highly imbalanced with the positive class accounting for $~16\%$ of the entire dataset. I use this dataset to gain an understanding of how large scale datasets can influences the performance, size, and training time of LIWC-based models.

\section{Modelling}

In efforts to understand the impact of LIWC-based modelling, I design feature-based and neural network models. I develop a Logistic Regression and a SVM model with a linear kernel for each type of input data (Word unigrams, BPE unigrams, and LIWC unigrams) to allow for feature-based analysis of what patterns are identified. To investigate how neural network models operate on the input data, I develop three types of neural networks for each input type: First, I train a MLP to provide an initial insight into whether neural network approaches might be appropriate, second I develop a LSTM model to investigate whether there are any benefits from its recurrent nature and finally, I develop a CNN model due to their dominance in the literature.\vspace{5mm}

I specify two different training procedures, one for the linear baseline models and one for the neural networks.
For the linear baseline models, I tokenise and pre-process the data and perform a grid-search over the parameter space.
For neural network models, I similarly tokenise and pre-process the data and perform a Bayesian hyper-parameter search to identify the best performing parameter setting given by macro F1-score. I then reuse this best performing parameter settings and re-run the model with 5 different random seeds to ensure that the behaviour of the model on the dataset is not the result of the consequences of the random seed propagated into a model's initialisation of its tensors.

\subsection{Pre-processing}

Prior to providing any model data, it is necessary to pre-process the data to make it suitable for the experiment conducted. In my experiments I examine how modifying the vocabulary that a model relies on might influence model construction. To this effect, it is necessary to have some distinct pre-processing steps for the datasets depending on the experiment while others are shared. For the shared pre-processing steps, I lower-case all documents, replace all usernames, that follow the Twitter standard of an `@' followed by a string, with a generic \textit{<USER>} token, replace all website URLs with a generic \textit{<URL>} token, and finally, replace all hashtags with a generic \textit{<HASHTAG>} token. The resulting vocabulary sizes for each dataset and data type can be seen in \autoref{tab:vocab_sizes}.

For the LIWC-based models and the word-based models, I pre-process documents using the python library Ekphrasis \citep{baziotis:2017} which was developed specifically to handle the particularities of social media texts. One such particularity is the use of elongation of words which the library addresses by mapping to the unelongated form, e.g. `heyyyy' is mapped to `hey' (see \autoref{bpe_tok} and \autoref{tab:liwc_tok} for examples of tokenisation). No further processing is done for experiments using word tokens as input.

\zw{Once models are re-run update \autoref{tab:vocab_sizes}}
\begin{table}[]
\centering
\begin{tabular}{llll}
Dataset     & Word Vocabulary & BPE Vocabulary & LIWC Vocabulary \\\hline
Offence     & $16,768$        & $16,663$       & $851$           \\
Toxicity    & $95,710$        & $95,712$       & $1,022$         \\
Hate Expert & $9,110$         & $9,181$        & $739$           \\
Hate Speech & $14,730$        & $14,834$       & $837$           \\
StormFront  & $5,566$         & $5,510$        & $622$
\end{tabular}
\caption{Vocabulary sizes for each input type and the training set for each dataset.}
\label{tab:vocab_sizes}
\end{table}

For the LIWC experiments on the other hand, I take another step after the initial tokenisation to compute the LIWC categories invoked by each word. Each token obtained is passed through a function which identifies all LIWC categories that the token invokes and combines them into a single token, where each LIWC category is separated by an underscore. All tokens that are not recognised by LIWC are replaced with a general token for \textit{<UNK>} token (see \autoref{tab:liwc_tok} for examples on the result of the pre-processing of documents).

For the BPE-based models on the other hand, I pre-process documents by computing using the BPE python library \citep{Heinzerling:2018}. Byte-Pair Encodings are well suited to handle the particularities of social media text, as it breaks unrecognised words into subwords, thus minimising unknown tokens in the validation and test sets. Through this minimisation, the hope is that even if part of a of a word is out-of-vocabulary for the model some of its subwords will be within a model's vocabulary, allowing the remaining subwords to be used for inference.

\begin{table}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|l|l}
    Document                    & Word Token Representation    & Byte-Pair Representation\\\hline
    Man I fucking hate animals! & Man I fucking hate animals ! & \_man \_i \_fucking \_hate \_animals !\\
    Man I fking h8 animals!     & Man I fking h8 animals !     & \_man \_i \_f king \_h 0 \_animals   !\\
    Bruv I fking hate animals!  & Bruv I fking hate animals !  & \_br uv \_i \_f king \_hate \_animals !
  \end{tabular}%
  }
  \caption{Word token and BPE representation.}
  \label{tab:bpe_tok}
\end{table}

In reviewing \autoref{tab:vocab_sizes}, it is clear that computing LIWC representations result in smaller vocabularies while BPE representations of the documents results in similar sized vocabularies for all datasets with the exception of the BPE representation of the \textit{StormFront} dataset and the \texitit{Offence} dataset where the vocabulary sizes decrease slightly. It is unsurprising that the vocabulary size would grow using BPE as subwords for all unrecognised tokens are computed using the Byte-Pair Encoding. More surprising is the small drop in BPE vocabulary sizes. These drops suggest that a set of the words that are unrecognised by the pre-trained Byte-Pair Embeddings as complete words share a relatively small set of subwords.

\begin{table}[]
\centering
\footnotesize
\begin{tabular}{l|p{10.5cm}}
Document                   & LIWC Representation \\ \hline
Man I fucking hate animals & MALE\_SOCIAL PPRON\_FUNCTION\_I\_PRONOUN AFFECT\_SEXUAL\_BIO\_INFORMAL\_NEGEMO\_ANGER\_ADJ\_SWEAR AFFECT\_NEGEMO\_ANGER\_VERB\_FOCUSPRESENT UNK UNK \\\hline
Man I fking h8 animals     & MALE\_SOCIAL PPRON\_FUNCTION\_I\_PRONOUN UNK NUM UNK UNK \\\hline
Bruv I fking hate animals  & UNK PPRON\_FUNCTION\_I\_PRONOUN UNK AFFECT\_NEGEMO\_ANGER\_VERB\_FOCUSPRESENT UNK UNK
\end{tabular}
\caption{Examples of LIWC representations.}
\label{tab:liwc_tok}
\end{table}

\zw{Double check these numbers after retraining}
For the documents represented through the LIWC categories that they invoke, I observe a sharp decline in the sizes of the vocabularies, with the smallest decrease in vocabulary being an $88.6\%$ decrease while the largest decrease is $99.7\%$. This is expected as the LIWC dictionary only encompasses a small number of words as many words used in informal conversations on online platforms are likely to fall outside of those considered when developing the dictionary. Moreover, it is also not surprising that a drop would occur as many of the datasets are created and published after the creation of the LIWC dictionary and examine domains that are unlikely to be well represented within the LIWC dictionary. Consequently will be subject to some language drift in addition to domain shifts.

\begin{table}[h]
\centering
\resizebox{0.8\textwidth}{!}{%

\begin{tabular}{lllll}
                    & Not Abuse        & Abuse            & Intersection     & Vocab size\\\hline
  Offence           & $24\; (2.8\%)$   & $150\; (17.6\%)$ & $677\; (79.6\%)$ & $851$\\
  Toxicity          & $131\; (12.8\%)$ & $5\; (0.5\%)$    & $886\; (86.9\%)$ & $1,022$\\
  Hate Expert       & $241\; (32.6\%)$ & $25\; (3.4\%)$   & $473\; (64\%)$   & $739$  \\
  Hate Speech       & $116\; (13.9\%)$ & $47\; (5.62\%)$  & $674\; (80.5\%)$ & $837$\\
  StormFront        & $74\; (11.9\%)$  & $117\ (18.8\%)$  & $431\; (69.3\$)$ & $622$
\end{tabular}%
}
\caption{Number of unique LIWC tokens in each class for each dataset and the size of their intersection.}
\label{tab:liwc_vocab_overlaps}
\end{table}

\begin{table}[h]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lllll}
                   & Not Abuse           & Abuse              & Intersection        & Vocab size\\\hline
  Offence          & $3,303\; (19.7\%)$  & $8,656\; (51.6\%)$ & $4,809\; (28.7\%)$  & $16,768$\\
  Toxicity         & $71,491\; (74.7\%)$ & $1,560\; (1.6\%)$  & $22,659\; (23.7\%)$ & $95,710$\\
  Hate Expert      & $6,155\; (67.6\%)$  & $953\; (10.5\%)$   & $2,002\; (22.98\%)$ & $9,110$\\
  Hate Speech      & $7,042\; (47.8\%)$  & $2,599\; (17.6\%)$ & $5,089\; (34.6\%)$  & $14,730$\\
  StormFront       & $1,834\; (32.9\%)$  & $2,273\; (40.8\%)$ & $1,459\; (26.2\%)$  & $5,566$
\end{tabular}%
}
\centering
\caption{Number of unique word tokens in each class for each dataset and the size of their intersection.}
\label{tab:word_vocab_overlaps}
\end{table}

\begin{table}[h]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lllll}
                  & Not Abuse           & Abuse              & Intersection        & Vocab size\\\hline
  Offence         & $3,199\; (19.2\%)$  & $7,978\; (47.9\%)$ & $5,486\; (32.9\%)$  & $16,663$\\
  Toxicity        & $71,493\; (74.7\%)$ & $1,560\; (1.6\%)$  & $22,659\; (23.4\%)$ & $95,712$\\
  Hate Expert     & $6,231\; (67.9\%)$  & $971\; (10.6\%)$   & $1,979\; (21.6\%)$  & $9,181$\\
  Hate Speech     & $7,074\; (47.7\%)$  & $2,653\; (17.9\%)$ & $5,107\; (34.4\%)$  & $14,834$\\
  StormFront      & $1,804\; (32.7\%)$  & $2,240\; (40.7\%)$ & $1,466\; (26.6\%)$  & $5,510$
\end{tabular}%
}
\caption{Number of unique BPE tokens in each class for each dataset and the size of their intersection.}
\label{tab:bpe_vocab_overlaps}
\end{table}

Considering Tables \ref{tab:liwc_vocab_overlaps}, \ref{tab:word_vocab_overlaps}, and \ref{tab:bpe_vocab_overlaps} that display the token distribution for each type of input on each dataset, there are some clear implications for my research questions.
First, only minor distributional shifts between word-based vocabularies (see \autoref{tab:word_vocab_overlaps}) and BPE-based vocabularies (see \autoref{tab:bpe_vocab_overlaps}). As processing and representing documents as their byte-pair represented counter-parts results in the computation of subwords, such small distributional discrepancies are to be expected.
Second, observing the differences between LIWC vocabulary distributions (see \autoref{tab:liwc_vocab_overlaps}) and the word vocabulary distributions, it is clear that the distributional changes are large and that the LIWC-based representation has large ramifications on the datasets and subsequently models trained for the task.
For instance, as the vast majority of tokens are shared between both classes, there are fewer potential signals for models to overfit to, e.g. where a word-based model trained on the \textit{Toxicity} dataset may have at least $73,051$ unique tokens, that is $76.3\%$ of all unique tokens in the dataset, that it can potentially learn spurious correlations on, a LIWC-based model is only provided with $136$ unique tokens, or $13.3\%$ of all unique tokens, that it is likely to overfit to.\footnote{In both instances disregarding the possibility of over-fitting to the patterns of occurrences between different tokens.}
Similarly to n-gram character-based modelling, a smaller set of unique tokens is likely to result in a matrix that, in places, more dense, allowing for a model to identify patterns based on the interaction of tokens rather than individual tokens. This particular case is likely for LIWC-based models as the vast majority (between $64\%$ and $86\%$) of tokens are shared between both classes.

\subsection{Linear Baseline Models}\label{sec:baseline_models}

For the linear baseline models, I perform a cross-validated grid-search over all possible setting of the model parameters, which vary depending on the model. For both SVM and Logistic Regression models, I explore values of $C\in [0.1, 0.2, 0.3, \ldots, 0.9]$ to examine the strength of regularisation, and I explore with $L1$ and $L2$ regularisers. For Logistic Regression, I also set the parameter search to consider $Elasticnet$ as a third regulariser option.

\subsection{Neural Models}\ref{sec:redux_neural}

I implement three different neural network model types using PyTorch~\citep{Paszke:2019} and perform a hyper-parameter search on each model type for every dataset. Specifically, I implement a Multi-Layered Perceptron model, a Long-Short Term Memory network, and a Convolutional Neural Network.
I choose to implement an MLP as it is the simplest form of neural networks and it can provide early insights into the applicability of neural network based architectures.
As the LIWC tokens are distributed such that the vast majority of tokens are shared by both classes, I also train a LSTM network to take long-range dependencies into account. I choose a LSTM over a basic RNN model as unknown tokens are likely to occur frequently in the LIWC-based data due to the small size of the dictionary and resulting vocabularies, and it may be desirable for any recurrent model trained on the data to be afforded the ability to forget sequences of unknown tokens.
Finally, I train a CNN model as this model type has been dominant in the literature.

In order to focus on the utility of the different document representations, I train models with simple architectures. To this end, I also don't use pre-trained embeddings as embedding layers within the model as I am not aware of any general purpose pre-trained LIWC-embeddings and, as is apparent from \autoref{tab:liwc_tok}, the LIWC tokens generated for each token would most likely be out-of-vocabulary for most pre-trained word embeddings. Instead I opt to train the embedding layer along with all other layers.
To address the issue of the model over-fitting the data, either by identifying spurious correlations in the data or by over-training the model, I subject each model to dropout and early stopping (see \autoref{sec:dropoutearly} for more detail on dropout and early stopping). To address the issues of exploding and vanishing gradients, I employ gradient clipping \citep{Bengio:1994}, to normalise the value of the gradients in the training procedures.\vspace{5mm}

In training the neural network based models, I first perform a hyper-parameter tuning operation for each dataset and each model type. Following the hyper-parameter tuning, I identify the best-performing hyper-parameter settings for each model type on each dataset, as given by macro F1-score. I then re-run the model with the best performing hyper-parameter settings with five different random seeds, to ensure that the model performances are not the result of a happen stance due to how the layers of the models are initialised with non-zero values. It is at this last step of retraining the models with different random seeds that I also obtain evaluation for each model on its performance on the external datasets.

\subsection{Hyper-Parameter Search}

In efforts to identify the best hyper-parameters without performing a grid-search of all possible combinations, I turn towards Bayesian Hyper-Parameter Tuning \citep{Neal:1996}. Briefly, Bayesian Optimisation allows for estimating the best hyper-parameters for a model through a series of trials with different hyper-parameter settings. I use the implementation of Bayesian Hyper-Parameter Optimisation offered through \citet{Wandb} and set the objective of the hyper-parameter optimisation to maximise the macro F1-score on the development data (please refer to \autoref{sub:bho} for more detail).

The parameters that I perform the optimisation for varies across the different model types as they require different hyper-parameters to be defined. A set of hyper-parameters are constant across models: the size of mini-batches provided to the model for training, the learning rate, the number of epochs, and the embedding size. For each dataset and model type, I perform at last $200$ trials with different parameter settings, leading to choosing a final set of hyper-parameters that I run with five different random seeds.

As a result of the $200$ trials, several parameter settings compete to be the best performing model, with little difference in their scores on the validation set. Although only the model with the highest macro F1-score performance on the validation set is chosen, I make note of the top $5$ models, should one of the best performing models result in inconsistent results across different values of the random seed. The hyper-parameters for the best and most stably performing model are presented in \autoref{tab:redux_embedding_offence,tab:redux_embedding_toxicity,tab:redux_embedding_hate_expert,tab:redux_embedding_hatespeech,tab:redux_embedding_stormfront}.


% TODO Minipage these so they stack?
\begin{landscape}
\begin{table}[]
\centering
\begin{tabular}{l|llll|llll}
                      & \multicolumn{4}{c|}{BPE}                 & \multicolumn{4}{c}{LIWC} \\
                       & MLP     & CNN      & RNN     & LSTM    & MLP     & CNN     & RNN     & LSTM     \\ \hline
Embedding Dimension    &         &          &         &         &         &         &         &          \\
Hidden Dimension       &         &          &         &         &         &         &         &          \\
Window Size            &         &          &         &         &         &         &         &          \\
Batch Size             &         &          &         &         &         &         &         &          \\
Learning Rate          &         &          &         &         &         &         &         &          \\
Dropout                &         &          &         &         &         &         &         &          \\
Activation Function    &         &          &         &         &         &         &         &          \\
\# Epochs              &         &          &         &         &         &         &         &          \\
Validation F1-score    &         &          &         &         &         &         &         &
\end{tabular}
\caption{Best hyper-parameter setting for neural models with embedding input layer trained on \citet{Davidson:2017}.}
\label{tab:redux_hyperparam_search_davidson}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[]
\centering
\begin{tabular}{l|llll|llll}
                      & \multicolumn{4}{c|}{BPE}                 & \multicolumn{4}{c}{LIWC} \\
                      & MLP     & CNN     & RNN     & LSTM    & MLP     & CNN     & RNN     & LSTM    \\ \hline
Embedding Dimension   &         &         &         &         &         &         &         &         \\
Hidden Dimension      &         &         &         &         &         &         &         &         \\
Window Size           &         &         &         &         &         &         &         &         \\
Batch Size            &         &         &         &         &         &         &         &         \\
Learning Rate         &         &         &         &         &         &         &         &         \\
Dropout               &         &         &         &         &         &         &         &         \\
Activation Function   &         &         &         &         &         &         &         &         \\
\# Epochs             &         &         &         &         &         &         &         &         \\
Validation F1-score   &         &         &         &         &         &         &         &
\end{tabular}
\caption{Best hyper-parameter setting for neural models with embedding input layer trained on \citet{Wulczyn:2017}.}
\label{tab:redux_hyperparam_search_wulczyn}
\end{table}
\end{landscape}

For all models trained, we perform hyper-parameter tuning over the same batch size, learning rate, and the maximum number of training epochs. The values for the learning rate are sampled from a uniform distribution while the batch size and epoch count are sampled from a categorical distribution. More generally, the values for all hyper-parameters, asides from dropout and the learning rate, are sampled from a categorical distribution. The trial values for Dropout and the learning rate are both sampled from a uniform distribution. We select a Rectified Linear Unit \cite{CITE: RELU paper} activation function for all models except for the Recurrent Neural Network and the Long-Short Term Memory model, as the latter is only implemented with a $Tanh$ activation function in PyTorch and we use the former to ensure comparison between the Long-Short Term Memory model and all other models.

\begin{itemize}
  \item Maximum epoch count: $\{50, 100, 200\}$,
  \item Batch size: $\{16, 32, 64\}$,
  \item learning rate: $[0.00001, 1.0]$
\end{itemize}

\subsubsection{Multi-Layered Perceptron}

The first neural model that we use, is a Multi-Layered Perceptron. We choose the model as it is a simple neural network, that can act as a minimal setting of the usefulness of neural networks. Our Multi-Layered Perceptron consists of either a linear input layer or an embedding input layer. The obtained representation of a batch of documents are then passed on to a linear hidden layer and passed on to a linear output layer, which is subject to a softmax layer computing probability estimates for each class. Following the first and second layer of the model architecture, we subject the output of the layer to a non-linear activation function and a dropout layer. The model architecture is depicted in \autoref{fig:liwc_mlp}. For the Multi-Layered Perceptron models, we search over the following values:

\begin{itemize}
  \item dropout probability: $[0.0, 0.5]$,
  \item hidden layer dimension: $\{64, 100, 200, 300\}$, and
  \item the activation function: $\{ReLU\}$
\end{itemize}

For the models that use an embedding layer as their input layers, we additionally search for the dimension of the embedding layer, allowing the model to search between $[100, 300]$. For the linear input layer model, the hidden dimension search functionally replaces the search over the embedding size.

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{mlp.jpg}
  \caption{Multi-Layered Perceptron model architecture.}
  \label{fig:liwc_mlp}
\end{figure}


\subsubsection{Recurrent Neural Network}

The second neural model we implement a Recurrent Neural Network; we choose this model as it offers improvements over Multi-Layered Perceptron due to the introduction of the recurrence over the tokens in the documents (see \autoref{chap:nlp} for more detail). Our Recurrent Neural Network consists of an input layer, which can be a linear layer or an embedding layer, a recurrent neural network layer, a linear output layer, a dropout layer, and a softmax layer to compute the probabilities of each class. The recurrent neural network layer is provided an activation function, which is applied within the layer.

The model is trained by first passing batches of index or onehot encoded documents through the input layer, and are passed on to the recurrent neural network layer.\footnote{We use the PyTorch implementation of the Recurrent Neural Network layer.} The resulting representation is then subject to a dropout layer before it subject to a linear layer that maps to the number of output classes. Finally, the softmax layer computes the probability estimates for each class. See \autoref{fig:liwc_rnn} for a depiction of the models. We set the activation function for the recurrent neural network to $\tanh$.

For the Recurrent Neural Networks, we perform a hyper-parameter tuning over the following parameters and values:

\begin{itemize}
  \item dropout probability: $[0.0, 0.5]$,
  \item embedding layer dimension: $\{64, 100, 200, 300\}$,
  \item hidden layer dimension: $\{64, 100, 200, 300\}$, and
   \item the activation function: $\{Tanh, ReLU\}$
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{rnn.jpg}
  \caption{Recurrent Neural Network model architecture.}
  \label{fig:liwc_rnn}
\end{figure}

\subsubsection{Long-Short Term Memory}

The Long-Short Term Memory network that we implement, consists of an input layer, that similarly to the RNN and MLP can be either a linear layer or an embedding layer; a one-directional Long-Short Term Memory network layer;\footnote{We use the PyTorch implementation of the Long-Short Term Memory Network layer.} an output layer; a dropout layer; and a softmax layer to compute the probabilities. The implementation of the Long-Short Term Memory layer is such that it always uses \textit{Tanh} as its non-linear activation function. We use Long-Short Term Memory networks due to their prior successes in other works \cite{CITE: LSTM papers} and because they present a development over RNNs, in that they identify information to ``forget'' in to address the issue of long-range dependencies that occur (please see \autoref{chap:nlp} for more detail).

The model is trained by passing batches of documents through the input layer prior to feeding them into the Long-Short Term Memory network layer. The output of the Long-Short Term Memory network layer is then subject to the dropout layer, before the output layer maps down to the number of label classes. Finally, the softmax layer is used to obtain an estimation of the probability distributions for each class (please see \autoref{fig:liwc_lstm} for depiction of model architecture.).

For these models, our hyper-parameter tuning considers the following parameters and values:

\begin{itemize}
  \item dropout probability: $[0.0, 0.5]$,
  \item embedding layer dimension: $\{64, 100, 200, 300\}$, and
  \item hidden layer dimension: $\{64, 100, 200, 300\}$
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{lstm.jpg}
  \caption{Long-Short Term Memory Network model architecture.}
  \label{fig:liwc_lstm}
\end{figure}

\subsubsection{Convolutional Neural Network}

For our final neural model type, we use a Convolutional Neural Network. We select this model as it has been applied previously in academic research \cite{CITE: CNN papers} and in industry (e.g. the Perspective API\footnote{https://github.com/conversationai/perspectiveapi}). Similarly to the previous model types, the input layer of the Convolutional Neural Network models can either be an embedding layer or a linear layer. The second layer of the model is a two-dimensional convolutional layer. Finally, there is an output layer and a softmax layer (See \autoref{fig:liwc_cnn} for depiction of model architecture).

For these models, we only consider the activation function, embedding, and hidden dimension in our hyper-parameter tuning, in addition to batch size and learning rate.

\begin{itemize}
  \item window size: $\{(1, 2, 3), (2, 3, 4), (3, 4, 5)\}$,
  \item Number of filters: $\{64, 128, 256\}$,
  \item hidden layer dimension: $\{64, 100, 200, 300\}$, and
  \item the activation function: $\{ReLU\}$
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{cnn.jpg}
  \caption{Convolutional Neural Network model architecture.}
  \label{fig:liwc_cnn}
\end{figure}

\subsection{Baseline Models}

We develop several different baseline methods to compare our method with. For each shallow baseline model (i.e. Logistic Regression and Support Vector Machines), we train two different types: a surface-token based model that uses the surface forms of the documents (e.g. words), and a LIWC based model. For each of these models, we represent each document for training and classification as a bag-of-words after removing stop words. In addition to the aforementioned models, we also train deep neural networks that similarly rely on surface forms. Specifically, we use the models described in \autoref{sec:redux_neural} providing surface level tokens as the input to the models.

\subsubsection{Baseline hyper-parameters}

Similarly to our neural models, we perform a parameter search to identify the optimal parameters for training our linear baseline models. For the Support Vector Machine models, we explore a regularisation strength of $C \in \{0.1, 0.02 \ldots 1.0\}$ and $penalty \in \{L1, L2\}$. For the Logistic Regression models, we explore the same values of $C$ and append \texttt{elasticnet} to the possible space of penalties, yielding $penalty \in \{L1, L2, elasticnet\}$. We report the optimal settings in \autoref{tab:liwc_baseline_linear_params}.

\zw{EDIT: Update \autoref{tab:liwc_baseline_linear_params} based on binary results}

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccc|cccc}
                       & \multicolumn{4}{c|}{BPE}                         & \multicolumn{4}{c}{LIWC}\\ \hline
                       & \multicolumn{3}{c}{LR} & \multicolumn{2}{c}{SVM} & \multicolumn{2}{c}{LR} & \multicolumn{2}{c}{SVM}\\\hline

                       & C     & Penalty & C     & Penalty                & C     & Penalty & C     & Penalty \\ \hline
\textit{Offence}       & $1.0$ & L2      & $0.1$ & L2                     & $0.4$ & L2      & $0.1$ & L2      \\
\textit{Toxicity}      & $1.0$ & L2      & $0.2$ & L2                     & $1.0$ & L2      & $1.0$ & L2
\end{tabular}%
}
\caption{Optimal parameters for linear Support Vector Machine baselines.}
\label{tab:liwc_baseline_linear_params}
\end{table}

Considering the performances on the in-domain during training, we see in \autoref{tab:redux_linear_baselines_dev} reasonable baseline performances on the validation set. The validation scores described in \autoref{tab:redux_linear_baselines_dev} are not as strong as the state-of-the-art in-domain models \cite{Salminen:2020}, in fact they are comparable to the scores reported on the test set of the in the original paper \cite{Davidson:2017} which provided initial baseline scores.\footnote{We do not report these baseline scores as their work does not identify which weighting of their F1-score was used.}. While several previous work augment the textual data with syntactic knowledge \cite{Davidson:2017} or advanced token representations \cite{Salminen:2020} to boost classification performance, we only use the byte-pair encoded documents and the LIWC encoded documents, to ensure comparability with our experimental models. Moreover, as our primary concern is learning classifiers whose performance generalise to other datasets, unlike much prior work which concerns itself with learning classifiers that perform well within the dataset, we do not take further steps towards boosting our baseline classifiers in-domain performances.

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|clllll}
Training data             & Document Representation   & Model                   & F1-macro & Accuracy & Precision & Recall \\\hline
\multirow{4}{*}{Davidson} & \multirow{2}{*}{BPE}      & Logistic Regression     & $92.02$  & $95.52$  & $91.82$   & $92.22$  \\
                          &                           & Support Vector Machine  & $92.13$  & $95.56$  & $91.73$   & $92.53$  \\
                          & \multirow{2}{*}{LIWC}     & Logistic Regression     & $87.75$  & $93.09$  & $87.43$   & $88.08$  \\
                          &                           & Support Vector Machine  & $89.02$  & $93.62$  & $87.64$   & $90.60$  \\
\multirow{4}{*}{Wulczyn}  & \multirow{2}{*}{BPE}      & Logistic Regression     & $86.35$  & $95.67$  & $90.04$   & $83.42$  \\
                          &                           & Support Vector Machine  & $86.47$  & $95.50$  & $89.02$   & $84.31$  \\
                          & \multirow{2}{*}{LIWC}     & Logistic Regression     & $82.32$  & $94.86$  & $90.48$   & $77.34$  \\
                          &                           & Support Vector Machine  & $82.98$  & $94.96$  & $90.14$   & $78.36$
\end{tabular}%
}
\caption{In-domain scores on validation set by linear baselines.}
\label{tab:redux_linear_baselines_dev}
\end{table}

\section{Experimental Models}

To evaluate which training dataset allows for better generalisation, we train our four models described in \autoref{sec:redux_neural} and their variations on each of our training dataset, resulting in $16$ different trained models. We show the best performing model parameters on the respective validation sets in \autoref{tab:redux_hyperparam_search}. In order to gain confidence intervals, we select a subset of these models and train them with $5$ different initial random seeds, to allow us to make claims of statistical significance of our models.

We train all of our neural network models following the same training procedure. We iterate over the training dataset in multiple epochs, shuffling the order of the data at the beginning of each epoch. As we train on a single task, the loss that is propagated through the network using backpropagation is computed on the validation set for the given task. To avoid over-training our model, we implement set our models to stop training after $15$ epochs of worse, that is strictly higher, loss values. As our training procedure closes, we apply the model on each test set, allowing us to evaluate its in-domain performance as well as its out-of-domain performance.

Using this training scheme, we define and search a hyper-parameter space for each model (see \autoref{sec:redux_neural} for the search space for each model and \autoref{tab:exp_model_parameters_davidson} and \autoref{tab:exp_model_parameters_wulczyn} for the best hyper-parameters for each model). Though some previous work \cite{Waseem:2018, CITE: Other papers that restrict vocabulary sizes} limit the vocabulary that is used to train models, we make no such limitations on the surface level tokens. Instead, for all models that use surface level representations, we pre-process the documents using the 200 dimensional Byte-Pair Encoding \cite{Heinzerling:2018} for two reasons: 1) computing the sub-words allows for a minimisation of the number of out-of-vocabulary tokens and 2) computing the sub-words also minimises the sizes of the vocabularies for each dataset. For all models that take documents represented through LIWC, we dramatically reduce the vocabulary to only the tokens that exist within LIWC, setting all other tokens to a token representing that it is out-of-vocabulary.

\zw{CODING: retrain the best parameters for the models with 4 new random seeds}


\zw{Add results and start by rough analysis of just numbers}
\zw{Look at predictions in detail, try to identify where they still fail}

\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{l|clllll}
  Training data             & Document Representation & Model                          & F1-macro & Accuracy & Precision & Recall \\ \hline
\mrow{8}{*}{\rot{Davidson}} & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           &  \\
                            &                         & Convolutional Neural Network   &          &          &           &  \\
                            &                         & Long-Short Term Memory Network &          &          &           &  \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           &  \\
                            &                         & Convolutional Neural Network   &          &          &           &  \\        
                            &                         & Long-Short Term Memory Network &          &          &           &  \\ \midrule       
\mrow{8}{*}{\rot{Wulczyn}}  & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           &  \\        
                            &                         & Convolutional Neural Network   &          &          &           &  \\        
                            &                         & Long-Short Term Memory Network &          &          &           &  \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           &  \\
                            &                         & Convolutional Neural Network   &          &          &           &  \\        
                            &                         & Long-Short Term Memory Network &          &          &           &  
\end{tabular}%
}
\caption{In-domain scores on validation set by onehot encoded neural models.}
\label{tab:redux_onehot_neural_dev}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{l|clllll}
  Training data             & Document Representation & Model                          & F1-macro & Accuracy & Precision & Recall  \\ \hline
\mrow{8}{*}{\rot{Davidson}} & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           & \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           & \\ \midrule
\mrow{8}{*}{\rot{Wulczyn}}  & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           & \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           &
\end{tabular}%
}
\caption{In-domain scores on validation set by index encoded neural models.}
\label{tab:redux_index_neural_dev}
\end{table}
\end{landscape}

\section{Results}

\zw{UPDATE: Change F1, Accuracy, precision, recall names to full names from acc, prec, rec}
\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{ccl|llll|llll|llll|llll|llll}
                                     &                         &                                & \multicolumn{4}{c|}{Davidson} & \multicolumn{4}{c}{Wulczyn}   & \multicolumn{4}{c}{Waseem}    & \multicolumn{4}{c}{Waseem-Hovy} & \multicolumn{4}{c}{Garcia} \\
 Training data                       & Representation          & Model                          & F1    & acc    & prec & rec   & F1    & acc   & prec  & rec   & F1    & acc   & prec  & rec   & F1    & acc   & prec  & rec     & F1    & acc    & prec & rec   \\ \hline
\multirow{12}{*}{\rot{Davidson}}         & \mrow{6}{*}{\rot{BPE}}  & Support Vector Machine         &$92.19$&$95.60$&$91.91$&$92.47$&$71.26$&$86.83$&$67.87$&$92.47$&$49.06$&$68.64$&$49.56$&$49.41$&$58.49$&$65.66$&$49.41$&$58.25$  &$60.62$&$60.66$&$60.71$&$60.66$\\
                                     &                         & Logistic Regression            &$91.39$&$95.19$&$91.51$&$91.27$&$71.18$&$86.40$&$67.68$&$79.85$&$48.34$&$66.04$&$49.35$&$49.03$&$58.12$&$64.36$&$58.37$&$57.98$  &$58.15$&$58.15$&$58.16$&$58.15$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     & \mrow{6}{*}{\rot{LIWC}} & Support Vector Machine         &$86.85$&$92.01$&$84.41$&$90.12$&$75.32$&$90.02$&$72.41$&$79.78$&$53.29$&$71.24$&$53.27$&$54.42$&$55.89$&$65.36$&$57.59$&$55.93$  &$44.92$&$49.58$&$49.36$&$49.58$\\
                                     &                         & Logistic Regression            &$87.19$&$92.41$&$85.37$&$89.41$&$74.91$&$89.69$&$71.85$&$79.81$&$51.90$&$68.78$&$52.30$&$53.36$&$56.43$&$64.83$&$57.51$&$56.32$  &$45.23$&$48.74$&$48.31$&$48.74$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\\hline

\mrow{12}{*}{\rot{Wulczyn}}          & \mrow{6}{*}{\rot{BPE}}  & Support Vector Machine         &$63.89$&$70.55$&$66.24$&$78.47$&$86.42$&$95.59$&$89.11$&$84.15$&$53.45$&$80.34$&$55.30$&$53.23$&$51.95$&$67.90$&$59.86$&$54.19$  &$52.89$&$58.99$&$68.66$&$58.99$\\
                                     &                         & Logistic Regression            &$59.86$&$64.30$&$64.25$&$75.47$&$86.24$&$95.63$&$89.94$&$83.31$&$51.22$&$80.92$&$53.43$&$51.64$&$50.56$&$68.14$&$60.38$&$53.64$  &$50.04$&$57.53$&$68.78$&$57.53$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     & \mrow{6}{*}{\rot{LIWC}} & Support Vector Machine         &$76.06$&$81.72$&$73.73$&$88.34$&$83.20$&$95.05$&$90.72$&$78.42$&$54.49$&$82.51$&$59.19$&$54.12$&$49.82$&$68.02$&$59.77$&$53.23$  &$39.70$&$51.67$&$58.13$&$51.67$\\
                                     &                         & Logistic Regression            &$72.62$&$78.17$&$71.52$&$86.30$&$82.67$&$94.95$&$90.88$&$77.65$&$54.26$&$82.94$&$59.99$&$53.99$&$49.11$&$67.78$&$58.88$&$52.81$  &$39.41$&$51.67$&$58.77$&$51.67$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $
\end{tabular}%
}
\caption{Performance of linear baseline models and index-encoded models across in-domain and out-of-domain evaluation sets.}
\label{tab:redux_embedding_davidson}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{ccl|llll|llll|llll|llll|llll}
                                     &                         &                                 & \multicolumn{4}{c|}{Davidson} & \multicolumn{4}{c}{Wulczyn} & \multicolumn{4}{c}{Waseem} & \multicolumn{4}{c}{Waseem-Hovy} & \multicolumn{4}{c}{Garcia} \\
 Training data                       & Representation          & Model                           & F1 & acc & prec & rec         & F1 & acc & prec & rec       & F1 & acc & prec & rec      & F1 & acc & prec & rec           & F1 & acc & prec & rec      \\ \hline
\mrow{12}{*}{\rot{Davidson}}         & \mrow{6}{*}{\rot{BPE}}  & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     & \mrow{6}{*}{\rot{LIWC}} & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\\hline

\mrow{12}{*}{\rot{Wulczyn}}          & \mrow{6}{*}{\rot{BPE}}  & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     & \mrow{6}{*}{\rot{LIWC}} & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
\end{tabular}%
}
\caption{Performance of onehot-encoded models across in-domain and out-of-domain datasets (\textit{italic} denotes baseline models).}
\label{tab:redux_embedding_davidson}
\end{table}
\end{landscape}
\zw{Add results and start by rough analysis of just numbers}
\zw{Add plots for development of loss over each epoch}
\zw{Add plots for F1 score during the evaluation set}
\zw{Look at predictions in detail, try to identify where they still fail}
\zw{Do logistic regression and to find out what clear patterns there are}

While the onehot and index encoded tensors should functionally be equal to one another, we see a direct influence of the input layers on the classification scores; with all models showing stronger performance using linear input layers. We propose that the reason for such discrepancies lie in the simpler training procedure of linear layers which don't seek to find relationships between different tokens but instead simply provide a linear function, and embedding layers that seek to identify the relationships between each all tokens in the dataset.

\section{Conclusions and future work}

\zw{Something about BERT based models}
BERT \cite{Koufakou,Vidgen,Tran:2020,Isaksen:2020}
As our aim is to consider the influence of LIWC-represented documents, we do not consider the more recent pre-trained Transformer-based language models \citep[e.g.]{Devlin:2019,Liu:2019} as the amounts of data necessary to train such a masked language model with LIWC representations are unavailable. Moreover, as the LIWC dictionary only occupies a small fraction of the entire English lexicon, and its tokens are abstractions on use of the language, training a language model is a fruitless endeavour.


\zw{Some concluding remarks}
While functionally this limits the vocabulary, there is also loss of information. Future work, could then employ both simple and complex mappings of different forms of words to single tokens that cohere with the LIWC dictionary, thus limiting information loss while retaining the predictive power.

\subsection{Limitations}
Although such lack of recognition can have positive effects, such as lower false positive rate, the politics of not being recognised, as argued by \citet{Benjamin:2019} are not straightforward and the lack of recognition does not provide a guarantee that systemic harm will not occur. For instance, if systems developed to detect abuse did not recognise Multi-cultural London English due to vocabulary reductions, any abuse that was written in that dialect would not be recognised, leaving those users in harms way. Given that LIWC was developed using ``dictionaries, thesauruses, questionnaires, and lists made by research assistants'' \citep{Tauscik:2010} in a North American context, it is highly unlikely that word forms that differ from mainstream usage were included. For instance, the commonly used `brotha' and `bruva' in North American and British contexts, respectively, are absent from the dictionary.
