% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

\chapter{LIWC/text transformation chapter}\label{chap:liwc}

\zw{Describe problem field}
One of the key issues in machine learning for content moderation is that such systems both in deployed settings (see \autoref{chap:filter}) and in research (see \autoref{chap:intro} and \autoref{chap:nlp}) over-fit to individual tokens that see over-representation in the positive and negative classes respectively. While research efforts have been made to address such issues \cite{CITE: cite papers that try to address overfitting}, the problem of over-fitting to words and identity markers remain an open question for the field. While some such approaches have addressed this problem by replacing certain words and phrases with more general tokens \cite{CITE: Replacing token papers} or masking \cite{CITE: Masking token paper} tokens. Other work has attempted to address the problem by treating it as a problem of dataset bias \cite{Dixon:2018}. Here, I propose a different approach which serves to address the issue of generalisable models by 1) minimising the vocabulary in order to avoid over-fitting to distributional skews of low-frequency tokens across classes; 2) representing documents in terms of how they represent thoughts, feelings, and personality; and 3) through such vocabulary minimisation highlight the importance of how words are used rather than their surface forms while retaining model performance. An additional benefit of such vocabulary reduction is a proportional reduction in model size and training time for complex models such as neural networks, resulting in models that have a smaller environmental impact \citep{Strubell:2019}.

Through the use of the Linguistic Inquiry and Word Count (LIWC) dictionary \cite{LIWC:2015,Original LIWC Citation}, I pre-process documents from large vocabularies, that are riddled with obfuscations and intentionally and unintentionally misspelled words into a smaller vocabulary set representing instead psycholinguistic properties of words. Through a reduction of thousands, or in some cases hundreds of thousands, of unique tokens to hundreds of LIWC categories, I aim for models to gain deeper insight into language patterns of abuse than simply selecting the most frequently used tokens. Moreover, I show that such a reduction is accompanied by a negligible intra and inter dataset performance in comparison to models using the full surface-token vocabularies.

\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{l|l|p{5cm}|l}
    Document                   & Tokenised Document         & LIWC Encoding                                                                                                                     & Byte-Pair Encoding\\\hline
    Man I fucking hate animals & Man I fucking hate animals & 'MALE\_SOCIAL PPRON\_FUNCTION\_I\_PRONOUN AFFECT\_SEXUAL\_BIO\_INFORMAL\_NEGEMO\_ANGER\_ADJ\_SWEAR AFFECT\_NEGEMO\_ANGER\_VERB\_FOCUSPRESENT UNK & \▁man \▁i \▁fucking \▁hate \▁animals \\
    Man I fking h8 animals     & Man I fking h8 animals     & MALE\_SOCIAL PPRON\_FUNCTION\_I\_PRONOUN UNK NUM UNK                                                                                  & \▁man \▁i \▁f king \▁h 0 \▁animals \\
    Bruv I fking hate animals  & Bruv I fking hate animals  & UNK PPRON\_FUNCTION\_I\_PRONOUN UNK AFFECT\_NEGEMO\_ANGER\_VERB\_FOCUSPRESENT UNK                                                        & \▁br uv \▁i \▁f king \▁hate \▁animals
  \end{tabular}
  \caption{Tokenisation and LIWC transformation.}
  \label{tab:liwc_transform}
\end{table}
\end{landscape}

Through the use of simple deep neural networks and `shallow' linear models, I show that through reducing the vocabulary sizes by up to $99\%$ and the number of model parameters by up to $99\%$, while increasing the depth of the information in the remaining vocabulary, it is possible to achieve comparable performances within datasets and mild improvements on out-of-domain datasets. This holds two strong implications for future research on computational hate speech detection: first that current approaches through an over-reliance on surface forms are computationally inefficient, and second that the exclusive use of surface forms of tokens can lead models to overly attend to the occurrence of certain tokens and variations (e.g. prominent misspellings) \citep{Rottger:2021}. Finally, as datasets for hate speech detection frequently contain biases along racialised and dialectal lines \citep{Waseem:2018,Davidson:2019}, the use of LIWC can serve as small aid in avoiding such biases as dialectal spellings of words are likely to not appear in the dictionary, thus being relegated to unknown tokens.

\section{Previous work}

In the interest of curtailing the spread of online abuse, a large number of technical approaches have been considered in the ever-increasing body of research on the topic (please see \autoref{sec:nlp} for a broad overview on the topic). Here, I focus on three different strands of research
Here, we focus on three different strands of research. First, methods that rely on manually crafted feature. Such methods, potentially the most rigerous from a scientific point of view, require an understanding of the problem and dataset as well as intuition and knowledge on how it may be addressed; thus here one might find suggestions for features that have the potential to travel across datasets. Second, we examine methods that rely on automated selection and weighting of features, that is neural network based approaches. Finally, we consider the comparatively small body of research devoted to the development of computational models that explicitly seek to generalise across datasets and domains. As we apply our models to datasets in English, we limit our consideration of methods to those that have been applied to datasets on English.

\subsection{Modelling}
\subsubsection{Manually Crafted Features}

A large body of work has sought to use manually developed features for online abuse detection \cite{Davidson:2017,Waseem:2017,Ibrohim:2019,Vega:2019,Wiegand:2018,Tian:2020,Kumar:2019}. While we do not manually craft the features for our experiments, we draw heavily on the rationale for manually developing features. The governing reasoning for using manually crafted features, such as Part-of-Speech tags \cite{Davidson:2017} is that they reveal intuitions of what the researcher believes may surface salient correlations that aid in high classification performance. Frequently used features are Bag-of-Words, that is the un-ordered occurrence of each word in a document; Term Frequency-Inverse Document Frequency which takes the fraction of the frequency of each unique token and the number of tokens in a document and multiplies it with the natural logarithm of the fraction of the number of documents and the number of documents that contain the given token under examination (see \autoref{eq:tfidf} detailing the implementation of \cite{Pedregosa:2011}); n-grams, e.g. bi-grams and tri-grams that consider tuples  and triplets of tokens, respectively; and Part-of-Speech tags.

\begin{equation}\label{eq:tfidf}
  \text{Term Frequency:}tf(t, d) = f_{t, d}\\
  \text{Inverse Document Frequency:}idf(t) = \log \dfrac{1 + n}{1 + df(t)} + 1\\
  TF-IDF(t, d) = tf(t, d) \cdot idf(t)
\end{equation}

LIWC has previously been examined in the context of abusive detection, \citet{Nina-Alcocer:2019} use LIWC to compute percentages of emotions that are present in abusive docuemnts. They also incorporate ``sexual, anxiety feeling, anger, and so on'' into their feature representation, however withouth clarifying what such features may be or how they are incorporated.

\subsubsection{Neural Networks}

\cite{Zimmerman:2018,Park:2017,Gamback:2017}

As our aim is to consider the influence of LIWC-represented documents, we do not consider the more recent pre-trained Transformer-based langauge models (e.g. \cite{Devlin:2019, CITE: Roberta}) as the amounts of data necessary to train such a masked language model with LIWC representations are unavailable. Moreover, as the LIWC dictionary only occupies a small fraction of the entire English lexicon, and its tokens are abstractions on use of the language, training a language model is a fruitless endeavor.

\subsubsection{Generalisable Models}
\zw{Talk about NLP models that try to generalise to other datasets.}

\cite{Waseem:2016,Waseem:2018,Karan:2018,Wiegand:2019}

\subsection{Linguistic Inquiry and Word Count}

\zw{Talk about LIWC: Designed for long form documents}






\subsection{Datasets}
Beyond the different specific models and types of input layer, the datasets themselves also differ strongly from one another across a number of attributes: the various sizes of the datasets, the platforms the datasets have been sampled from, the annotator selection, the annotation guidelines, and the aims of the datasets. As we seek to build generalisable models and compare the performance across datasets, we reduce the classification task in each dataset to a binary `abuse/not-abuse' class, as each dataset addresses different aspects and notions of abuse.
We train our models on either the Twitter dataset collected by \citet{Davidson:2017} and the Wikipedia Talk pages dataset sampled by \citet{Wulczyn:2016}, respectively, as our training datasets. We choose these datasets in part due to their sizes and in part due to the different breadth of communicative styles. To estimate cross-dataset performance, we evaluate our models on \citet{Waseem-Hovy:2016}, \citet{Waseem:2016}, and \citet{Garcia:2019}.

\zw{INSERT: Table of BPE and LIWC vocabularies}
\zw{INSERT: Short paragraph on vocabularies.}

\subsubsection{Training Datasets}
Our first dataset for training is the dataset developed by \citet{Davidson:2017} consists of $24,784$ tweets that are sampled from Twitter using keywords obtained from \citet{Hatebase}. The dataset is annotated for ``hate speech'', ``offensive language'' and ``neither''. The collection rationale was that not all content that immediately appears to be abusive is necessarily that, and that hate speech models must be able to distinguish between what is offensive and what is hateful \cite{Davidson:2017} (please see \autoref{chap:nlp} and \autoref{chap:filter} for more in-depth discussions on the implications of label categories, their overlap and differences). The dataset was annotated by crowd-workers on FigureEight\footnote{Previously known as CrowdFlower}. Unlike most datasets for abuse, this dataset consists primarily of positive instances, with $77$\% of the (binarised) dataset belonging to the positive class.

Our second dataset used for training is the dataset presented by \citet{Wulczyn:2016}. This dataset consists of more then $100,000$ comments from Wikipedia talk pages that have been annotated for personal attacks and toxicity \cite{Wulczyn:2016}. The rationale of this dataset is that personal attacks are harmful to ongoing conversations, and that through the identification and removals of comments that poison, or toxify online conversations, more space will be left for healthy and constructive discussions (please see \autoref{chap:filter} for an in-depth consideration of the politics of what constitutes ``toxic'' and ``healthy''). The binarised distribution of documents tagged for toxicity aligns better with prior research, with the positive class consuming $\approx 9$\% of the dataset.

\subsubsection{Evaluation Datasets}
For our evaluation datasets, we use \citet{Waseem-Hovy:2016}, a dataset of $16,000$ documents that are sampled from Twitter and annotated for ``racism'', ``sexism'', and ``neither''. The dataset was annotated by two coders, who labelled $31$\% of the dataset containing as either ``sexist'' or ``racist'' content. This dataset was developed for an early exploration into automated content moderation of online hate speech. We also use the dataset by \citet{Waseem:2016} that followed this first exploration. Here the annotation guidelines remain the same while the label-set is expanded to include the intersection of racism and sexism, the ``both'' category. This dataset contains $6000$ documents, labelled by intersectional feminist activists, and another label-set annotated by crowd-workers from FigureEight. We choose the intersectional feminist tagged annotations, as \citet{Waseem:2016} show that simple computational models perform better using this tagset. The binarised positive labels occupy $15.19$\% of the dataset. Finally, we use the dataset on white-supremacist speech developed by \citet{Garcia:2019}. This dataset, unlike the previous two evaluation datasets does not stem from Twitter, but instead the data is collected from StormFront\footnote{www.stormfront.net}, a web forum dedicated to the preservation and dissemination of white supremacist ideology. This dataset contains \zw{INSERT: Number of total document counts when using the entire dataset instead of the balanced one.} documents labelled for being hateful or not hateful, with \zw{INSERT: Percentage of positive class docs} in the positive class.

\subsubsection{Dataset and Platform Affordances}

As the datasets differ quite significantly in the sizes of the raw number of documents as well as the vocabulary sizes. Moreover, as the datasets are selected from different websites with different communities, purposes, and means of interaction; the data sampled from each platform may differ in content as well as style. Considering for instance \citet{Waseem:2016}, this dataset was collected on Twitter while the maximum length of a tweet was $140$ characters. Documents are thus short as they are given an upper limit on the number of characters. On the other hand, \citet{Wulczyn:2017} collected their data from the Wikipedia Editor Talk pages, where comments are not limited by length. Additionally, these two domains differ in that conversations on Twitter may have no particular topic, conversations on Wikipedia Talk pages always refer back to a specific topic and the conversation of how to address a particular edit to a page. Finally, given Wikipedia's ongoing issues with recruiting editors from a diverse set of backgrounds \cite{CITE: Wikipedia editors issue} and Twitter's comparatively broad user base \cite{CITE: Twitter userbase by demographic ref} may influence which dialects are represented on the platforms, which patterns of speech (e.g. sociolects, slang, and shorthand) occur, and the style of the discussions and conversations.

\subsubsection{Annotator Selection}

There are some interesting discrepancies in the selection of annotators for the datasets that we apply our models to. \citet{Waseem:2016} select their annotators based on socio-political positions, controlling for a specific interpretation of abuse. On the other hand \citet{Wulczyn:2017} select their annotators from the users of the Wikipedia Talk pages. However, the Wikipedia editor community has been accused of being a highly male space that is unwelcoming to women \cite{CITE: Cite article talking about anti-women culture on wikipedia}. This suggests that the influence of their selection of annotators, who are culturally situated in the norms and culture of the Wikipedia editor community, are also likely to be less attuned to content that may be offensive to women, but is accepted communicative practices within the Wikipedia editor community.

On the other hand \citet{Garcia:2019} and \citet{Davidson:2017} select annotators that are removed from the context of the documents they are annotating. This suggests that global understandings of what constitutes abuse are possible, and that it is possible to annotate without a deep understanding of the issues and communicative practices of the specific communities that are being investigated.

While we accept that the influence of annotation guidelines have strong influences on the subject that is being examined (e.g. \citet{Davidson:2017} examine the differences in what is merely offensive and what is hateful, \citet{Garcia:2019} examine what is hateful from a white supremacist community and what is not, and \citet{Wulczyn:2017} examine things that make conversations toxic and hostile), we assume that these different guidelines and questions highlight different aspects of abuse. Through our efforts to develop methods that can identify different forms of abuse across different datasets, we accept the assumption that there are some global understandings of abuse that can be learned by machine learning models. We revisit this assumption in \autoref{chap:disembodied}.

\section{Modelling}

As our aim is to understand how representing documents using only the tokens derived using the Linguistic Inquiry and Word Count dictionary may influence the generalisability of models for hate speech detection across datasets and domains, we use simple modelling architectures and feature representations.

\subsection{Neural Models}\ref{sec:redux_neural}

\zw{Add this where it fits}
Given that onehot encoded models are much larger in size, I limit the exploration of hyper parameter values to embedding dimension, hidden dimension, dropout and learning rate. Particularly dropout and learning rate are of interest as onehot encoded models are larger than index encoded models. For this reason, I explore deviate from the best parameters identified by the index encoded models. For embedding and hidden dimensions on the other hand, I follow the best parameters found by the index encoded models.

We implement and train four different model architectures with two variations each, resulting in a total of 8 different neural models for each input representation. We choose to train a Multi-Layered Perceptron, a Recurrent Neural Network, a Long-Short Term Memory network, and a Convolutional Neural Network. We select these four models as they have each been used in previous work \cite{CITE: Find papers with Neural approaches for each of the models}. Each of the four models are trained with either a linear input layer and an embedding input layer. We choose to make this distinction as our datasets are too small to meaningfully learn embedding layers, yet for the LIWC transformed documents, pre-trained embeddings have little value, as all tokens would be out-of-vocabulary. Thus to compare comparable entities, we train a model with each type of input layer. This difference in the input layers however also implies a difference in how each document is represented: for the models with linear input layers, each document is encoded as a onehot tensor whereas the models that use an embedding layer as it's input layer the documents are represented as index-encoded tensors (please see \autoref{fig:onehot_embedding} for an illustration). We implement all neural network models using PyTorch \cite{CITE: Pytorch paper} and use gradient clipping to prevent the issue of exploding gradients \cite{Bengio:1994}.
In order to focus on the utility of the transformed document representations, we use bare bones models with simple architectures. To address the issue of the models over-fitting to the data either by identifying spurious correlations or by over-training the model, we subject each model to dropout and early stopping criteria (please see \autoref{sec:dropoutearly} for more details on the functionality of early stopping and dropout).

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{onehot_embedding.jpg}
  \caption{Onehot and Index encoded tensors.}
  \label{fig:onehot_embedding}
\end{figure}

\subsection{Hyper-Parameter Search}

We perform Bayesian Hyper Parameter Tuning using Weights and Biases \cite{Wandb} to identify the optimal parameters for each model and the variants of each model, evaluated using macro-F1 score. We also investigate the influence of batch sizes and learning rate. For more in depth explanation of how each model works, please refer to \autoref{sec:model_background}.\vspace{5mm}
For each embedding-based model, we sample $200$ different hyper-parameter settings for each of our models. Due to the number of hyper-parameter combinations that we sample, we find multiple competing hyper-parameter settings that achieve comparative results. Here we only consider the top TODO: INSERT NUMBER OF PARAMETER SETTINGS TO SEARCH the hyper-parameter hyper-parameter settings and report the setting that performs best on the training data and the test data. In an initial, aborted, hyper-parameter search of the onehot encoded models, we identified that the hyper-parameter search with $200$ different parameter settings would take more than 200 days to complete.
To limit the search space, we search only the top TODO: INSERT NUMBER OF PAREMETERS TO SEARCH hyper-parameter settings from the hyper-parameter search of the embedding-layer models to search. See \autoref{tab:appendix-embedding-hyperparam} and \autoref{tab:appendix-onehot-hyperparam} in \autoref{Appendix:hyperparams} for full listing of hyper-parameter values.

\begin{landscape}
\begin{table}[]
\centering
\begin{tabular}{l|llll|llll}
                      & \multicolumn{4}{c|}{BPE}                 & \multicolumn{4}{c}{LIWC} \\
                       & MLP     & CNN      & RNN     & LSTM    & MLP     & CNN     & RNN     & LSTM     \\ \hline
Embedding Dimension    &         &          &         &         &         &         &         &          \\
Hidden Dimension       &         &          &         &         &         &         &         &          \\
Window Size            &         &          &         &         &         &         &         &          \\
Batch Size             &         &          &         &         &         &         &         &          \\
Learning Rate          &         &          &         &         &         &         &         &          \\
Dropout                &         &          &         &         &         &         &         &          \\
Activation Function    &         &          &         &         &         &         &         &          \\
\# Epochs               &         &          &         &         &         &         &         &          \\
Validation F1-score    &         &          &         &         &         &         &         &
\end{tabular}
\caption{Best hyper-parameter setting for neural models with embedding input layer trained on \citet{Davidson:2017}.}
\label{tab:redux_hyperparam_search_davidson}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[]
\centering
\begin{tabular}{l|llll|llll}
                      & \multicolumn{4}{c|}{BPE}                 & \multicolumn{4}{c}{LIWC} \\
                      & MLP     & CNN     & RNN     & LSTM    & MLP     & CNN     & RNN     & LSTM    \\ \hline
Embedding Dimension   &         &         &         &         &         &         &         &         \\
Hidden Dimension      &         &         &         &         &         &         &         &         \\
Window Size           &         &         &         &         &         &         &         &         \\
Batch Size            &         &         &         &         &         &         &         &         \\
Learning Rate         &         &         &         &         &         &         &         &         \\
Dropout               &         &         &         &         &         &         &         &         \\
Activation Function   &         &         &         &         &         &         &         &         \\
\# Epochs              &         &         &         &         &         &         &         &         \\
Validation F1-score   &         &         &         &         &         &         &         &
\end{tabular}
\caption{Best hyper-parameter setting for neural models with embedding input layer trained on \citet{Wulczyn:2017}.}
\label{tab:redux_hyperparam_search_wulczyn}
\end{table}
\end{landscape}

For all models trained, we perform hyper-parameter tuning over the same batch size, learning rate, and the maximum number of training epochs. The values for the learning rate are sampled from a uniform distribution while the batch size and epoch count are sampled from a categorical distribution. More generally, the values for all hyper-parameters, asides from dropout and the learning rate, are sampled from a categorical distribution. The trial values for Dropout and the learning rate are both sampled from a uniform distribution. We select a Rectified Linear Unit \cite{CITE: RELU paper} activation function for all models except for the Recurrent Neural Network and the Long-Short Term Memory model, as the latter is only implemented with a $Tanh$ activation function in PyTorch and we use the former to ensure comparison between the Long-Short Term Memory model and all other models.

\begin{itemize}
  \item Maximum epoch count: $\{50, 100, 200\}$,
  \item Batch size: $\{16, 32, 64\}$,
  \item learning rate: $[0.00001, 1.0]$
\end{itemize}

\subsubsection{Multi-Layered Perceptron}

The first neural model that we use, is a Multi-Layered Perceptron. We choose the model as it is a simple neural network, that can act as a minimal setting of the usefulness of neural networks. Our Multi-Layered Perceptron consists of either a linear input layer or an embedding input layer. The obtained representation of a batch of documents are then passed on to a linear hidden layer and passed on to a linear output layer, which is subject to a softmax layer computing probability estimates for each class. Following the first and second layer of the model architecture, we subject the output of the layer to a non-linear activation function and a dropout layer. The model architecture is depicted in \autoref{fig:liwc_mlp}. For the Multi-Layered Perceptron models, we search over the following values:

\begin{itemize}
  \item dropout probability: $[0.0, 0.5]$,
  \item hidden layer dimension: $\{64, 100, 200, 300\}$, and
  \item the activation function: $\{ReLU\}$
\end{itemize}

For the models that use an embedding layer as their input layers, we additionally search for the dimension of the embedding layer, allowing the model to search between $[100, 300]$. For the linear input layer model, the hidden dimension search functionally replaces the search over the embedding size.

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{mlp.jpg}
  \caption{Multi-Layered Perceptron model architecture.}
  \label{fig:liwc_mlp}
\end{figure}


\subsubsection{Recurrent Neural Network}

The second neural model we implement a Recurrent Neural Network; we choose this model as it offers improvements over Multi-Layered Perceptron due to the introduction of the recurrence over the tokens in the documents (see \autoref{chap:nlp} for more detail). Our Recurrent Neural Network consists of an input layer, which can be a linear layer or an embedding layer, a recurrent neural network layer, a linear output layer, a dropout layer, and a softmax layer to compute the probabilities of each class. The recurrent neural network layer is provided an activation function, which is applied within the layer. 

The model is trained by first passing batches of index or onehot encoded documents through the input layer, and are passed on to the recurrent neural network layer.\footnote{We use the PyTorch implementation of the Recurrent Neural Network layer.} The resulting representation is then subject to a dropout layer before it subject to a linear layer that maps to the number of output classes. Finally, the softmax layer computes the probability estimates for each class. See \autoref{fig:liwc_rnn} for a depiction of the models. We set the activation function for the recurrent neural network to $\tanh$.

For the Recurrent Neural Networks, we perform a hyper-parameter tuning over the following parameters and values:

\begin{itemize}
  \item dropout probability: $[0.0, 0.5]$,
  \item embedding layer dimension: $\{64, 100, 200, 300\}$,
  \item hidden layer dimension: $\{64, 100, 200, 300\}$, and
   \item the activation function: $\{Tanh, ReLU\}$
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{rnn.jpg}
  \caption{Recurrent Neural Network model architecture.}
  \label{fig:liwc_rnn}
\end{figure}

\subsubsection{Long-Short Term Memory}

The Long-Short Term Memory network that we implement, consists of an input layer, that similarly to the RNN and MLP can be either a linear layer or an embedding layer; a one-directional Long-Short Term Memory network layer;\footnote{We use the PyTorch implementation of the Long-Short Term Memory Network layer.} an output layer; a dropout layer; and a softmax layer to compute the probabilities. The implementation of the Long-Short Term Memory layer is such that it always uses \textit{Tanh} as its non-linear activation function. We use Long-Short Term Memory networks due to their prior successes in other works \cite{CITE: LSTM papers} and because they present a development over RNNs, in that they identify information to ``forget'' in to address the issue of long-range dependencies that occur (please see \autoref{chap:nlp} for more detail).

The model is trained by passing batches of documents through the input layer prior to feeding them into the Long-Short Term Memory network layer. The output of the Long-Short Term Memory network layer is then subject to the dropout layer, before the output layer maps down to the number of label classes. Finally, the softmax layer is used to obtain an estimation of the probability distributions for each class (please see \autoref{fig:liwc_lstm} for depiction of model architecture.).

For these models, our hyper-parameter tuning considers the following parameters and values:

\begin{itemize}
  \item dropout probability: $[0.0, 0.5]$,
  \item embedding layer dimension: $\{64, 100, 200, 300\}$, and
  \item hidden layer dimension: $\{64, 100, 200, 300\}$
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{lstm.jpg}
  \caption{Long-Short Term Memory Network model architecture.}
  \label{fig:liwc_lstm}
\end{figure}

\subsubsection{Convolutional Neural Network}

For our final neural model type, we use a Convolutional Neural Network. We select this model as it has been applied previously in academic research \cite{CITE: CNN papers} and in industry (e.g. the Perspective API\footnote{https://github.com/conversationai/perspectiveapi}). Similarly to the previous model types, the input layer of the Convolutional Neural Network models can either be an embedding layer or a linear layer. The second layer of the model is a two-dimensional convolutional layer. Finally, there is an output layer and a softmax layer (See \autoref{fig:liwc_cnn} for depiction of model architecture).

For these models, we only consider the activation function, embedding, and hidden dimension in our hyper-parameter tuning, in addition to batch size and learning rate.

\begin{itemize}
  \item window size: $\{(1, 2, 3), (2, 3, 4), (3, 4, 5)\}$,
  \item Number of filters: $\{64, 128, 256\}$,
  \item hidden layer dimension: $\{64, 100, 200, 300\}$, and
  \item the activation function: $\{ReLU\}$
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{cnn.jpg}
  \caption{Convolutional Neural Network model architecture.}
  \label{fig:liwc_cnn}
\end{figure}

\subsection{Baseline Models}

We develop several different baseline methods to compare our method with. For each shallow baseline model (i.e. Logistic Regression and Support Vector Machines), we train two different types: a surface-token based model that uses the surface forms of the documents (e.g. words), and a LIWC based model. For each of these models, we represent each document for training and classification as a bag-of-words after removing stop words. In addition to the aforementioned models, we also train deep neural networks that similarly rely on surface forms. Specifically, we use the models described in \autoref{sec:redux_neural} providing surface level tokens as the input to the models.

\subsubsection{Baseline hyper-parameters}

Similarly to our neural models, we perform a parameter search to identify the optimal parameters for training our linear baseline models. For the Support Vector Machine models, we explore a regularisation strength of $C \in \{0.1, 0.02 \ldots 1.0\}$ and $penalty \in \{L1, L2\}$. For the Logistic Regression models, we explore the same values of $C$ and append \texttt{elasticnet} to the possible space of penalties, yielding $penalty \in \{L1, L2, elasticnet\}$. We report the optimal settings in \autoref{tab:liwc_baseline_linear_params}.

\zw{EDIT: Update this table based on binary results}

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccc|cccc}
                      & \multicolumn{4}{c|}{BPE}                                                             & \multicolumn{4}{c}{LIWC}                                                             \\ \hline
                      & \multicolumn{2}{c}{Logistic Regression} & \multicolumn{2}{c}{Support Vector Machine} & \multicolumn{2}{c}{Logistic Regression} & \multicolumn{2}{c}{Support Vector Machine} \\ \hline
                      & C               & Penalty               & C                 & Penalty                & C               & Penalty               & C                 & Penalty                \\ \hline
\cite{Davidson:2017}  & $1.0$           & L2                    & $0.1$             & L2                     & $0.4$           & L2                    & $0.1$             & L2                     \\
\cite{Wulczyn:2016}   & $1.0$           & L2                    & $0.2$             & L2                     & $1.0$           & L2                    & $1.0$             & L2
\end{tabular}%
}
\caption{Optimal parameters for linear Support Vector Machine baselines.}
\label{tab:liwc_baseline_linear_params}
\end{table}

Considering the performances on the in-domain during training, we see in \autoref{tab:redux_linear_baselines_dev} reasonable baseline performances on the validation set. The validation scores described in \autoref{tab:redux_linear_baselines_dev} are not as strong as the state-of-the-art in-domain models \cite{Salminen:2020}, in fact they are comparable to the scores reported on the test set of the in the original paper \cite{Davidson:2017} which provided initial baseline scores.\footnote{We do not report these baseline scores as their work does not identify which weighting of their F1-score was used.}. While several previous work augment the textual data with syntactic knowledge \cite{Davidson:2017} or advanced token representations \cite{Salminen:2020} to boost classification performance, we only use the byte-pair encoded documents and the LIWC encoded documents, to ensure comparability with our experimental models. Moreover, as our primary concern is learning classifiers whose performance generalise to other datasets, unlike much prior work which concerns itself with learning classifiers that perform well within the dataset, we do not take further steps towards boosting our baseline classifiers in-domain performances.

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|clllll}
Training data             & Document Representation                     & Model                   & F1-macro & Accuracy & Precision & Recall   \\ \hline
\multirow{4}{*}{Davidson} & \multirow{2}{*}{BPE}                        & Logistic Regression     & $92.02$  & $95.52$  & $91.82$   & $92.22$  \\
                          &                                             & Support Vector Machine  & $92.13$  & $95.56$  & $91.73$   & $92.53$  \\
                          & \multirow{2}{*}{LIWC}                       & Logistic Regression     & $87.75$  & $93.09$  & $87.43$   & $88.08$  \\
                          &                                             & Support Vector Machine  & $89.02$  & $93.62$  & $87.64$   & $90.60$  \\
\multirow{4}{*}{Wulczyn}  & \multirow{2}{*}{BPE}                        & Logistic Regression     & $86.35$  & $95.67$  & $90.04$   & $83.42$  \\
                          &                                             & Support Vector Machine  & $86.47$  & $95.50$  & $89.02$   & $84.31$  \\
                          & \multirow{2}{*}{LIWC}                       & Logistic Regression     & $82.32$  & $94.86$  & $90.48$   & $77.34$  \\
                          &                                             & Support Vector Machine  & $82.98$  & $94.96$  & $90.14$   & $78.36$
\end{tabular}%
}
\caption{In-domain scores on validation set by linear baselines.}
\label{tab:redux_linear_baselines_dev}
\end{table}

\section{Experimental Models}

To evaluate which training dataset allows for better generalisation, we train our four models described in \autoref{sec:redux_neural} and their variations on each of our training dataset, resulting in $16$ different trained models. We show the best performing model parameters on the respective validation sets in \autoref{tab:redux_hyperparam_search}. In order to gain confidence intervals, we select a subset of these models and train them with $5$ different initial random seeds, to allow us to make claims of statistical significance of our models.

We train all of our neural network models following the same training procedure. We iterate over the training dataset in multiple epochs, shuffling the order of the data at the beginning of each epoch. As we train on a single task, the loss that is propagated through the network using backpropagation is computed on the validation set for the given task. To avoid over-training our model, we implement set our models to stop training after $15$ epochs of worse, that is strictly higher, loss values. As our training procedure closes, we apply the model on each test set, allowing us to evaluate its in-domain performance as well as its out-of-domain performance.

Using this training scheme, we define and search a hyper-parameter space for each model (see \autoref{sec:redux_neural} for the search space for each model and \autoref{tab:exp_model_parameters_davidson} and \autoref{tab:exp_model_parameters_wulczyn} for the best hyper-parameters for each model). Though some previous work \cite{Waseem:2018, CITE: Other papers that restrict vocabulary sizes} limit the vocabulary that is used to train models, we make no such limitations on the surface level tokens. Instead, for all models that use surface level representations, we pre-process the documents using the 200 dimensional Byte-Pair Encoding \cite{Heinzerling:2018} for two reasons: 1) computing the sub-words allows for a minimisation of the number of out-of-vocabulary tokens and 2) computing the sub-words also minimises the sizes of the vocabularies for each dataset. For all models that take documents represented through LIWC, we dramatically reduce the vocabulary to only the tokens that exist within LIWC, setting all other tokens to a token representing that it is out-of-vocabulary.

\zw{CODING: retrain the best parameters for the models with 4 new random seeds}


\zw{Add results and start by rough analysis of just numbers}
\zw{Look at predictions in detail, try to identify where they still fail}

\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{l|clllll}
  Training data             & Document Representation & Model                          & F1-macro & Accuracy & Precision & Recall \\ \hline
\mrow{8}{*}{\rot{Davidson}} & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           &  \\
                            &                         & Convolutional Neural Network   &          &          &           &  \\
                            &                         & Recurrent Neural Network       &          &          &           &  \\        
                            &                         & Long-Short Term Memory Network &          &          &           &  \\     
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           &  \\
                            &                         & Convolutional Neural Network   &          &          &           &  \\        
                            &                         & Recurrent Neural Network       &          &          &           &  \\        
                            &                         & Long-Short Term Memory Network &          &          &           &  \\ \midrule       
\mrow{8}{*}{\rot{Wulczyn}}  & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           &  \\        
                            &                         & Convolutional Neural Network   &          &          &           &  \\        
                            &                         & Recurrent Neural Network       &          &          &           &  \\
                            &                         & Long-Short Term Memory Network &          &          &           &  \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           &  \\        
                            &                         & Convolutional Neural Network   &          &          &           &  \\        
                            &                         & Recurrent Neural Network       &          &          &           &  \\        
                            &                         & Long-Short Term Memory Network &          &          &           &  
\end{tabular}%
}
\caption{In-domain scores on validation set by onehot encoded neural models.}
\label{tab:redux_onehot_neural_dev}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{l|clllll}
  Training data             & Document Representation & Model                          & F1-macro & Accuracy & Precision & Recall  \\ \hline
\mrow{8}{*}{\rot{Davidson}} & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Recurrent Neural Network       &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           & \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Recurrent Neural Network       &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           & \\ \midrule
\mrow{8}{*}{\rot{Wulczyn}}  & \mrow{4}{*}{\rot{BPE}}  & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Recurrent Neural Network       &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           & \\
                            & \mrow{4}{*}{\rot{LIWC}} & Multi-Layered Perceptron       &          &          &           & \\
                            &                         & Convolutional Neural Network   &          &          &           & \\
                            &                         & Recurrent Neural Network       &          &          &           & \\
                            &                         & Long-Short Term Memory Network &          &          &           &
\end{tabular}%
}
\caption{In-domain scores on validation set by index encoded neural models.}
\label{tab:redux_index_neural_dev}
\end{table}
\end{landscape}

\section{Results}

\zw{UPDATE: Change F1, Accuracy, precision, recall names to full names from acc, prec, rec}
\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{ccl|llll|llll|llll|llll|llll}
                                     &                         &                                & \multicolumn{4}{c|}{Davidson} & \multicolumn{4}{c}{Wulczyn}   & \multicolumn{4}{c}{Waseem}    & \multicolumn{4}{c}{Waseem-Hovy} & \multicolumn{4}{c}{Garcia} \\
 Training data                       & Representation          & Model                          & F1    & acc    & prec & rec   & F1    & acc   & prec  & rec   & F1    & acc   & prec  & rec   & F1    & acc   & prec  & rec     & F1    & acc    & prec & rec   \\ \hline
\multirow{12}{*}{\rot{Davidson}}         & \mrow{6}{*}{\rot{BPE}}  & Support Vector Machine         &$92.19$&$95.60$&$91.91$&$92.47$&$71.26$&$86.83$&$67.87$&$92.47$&$49.06$&$68.64$&$49.56$&$49.41$&$58.49$&$65.66$&$49.41$&$58.25$  &$60.62$&$60.66$&$60.71$&$60.66$\\
                                     &                         & Logistic Regression            &$91.39$&$95.19$&$91.51$&$91.27$&$71.18$&$86.40$&$67.68$&$79.85$&$48.34$&$66.04$&$49.35$&$49.03$&$58.12$&$64.36$&$58.37$&$57.98$  &$58.15$&$58.15$&$58.16$&$58.15$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Recurrent Neural Network       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     & \mrow{6}{*}{\rot{LIWC}} & Support Vector Machine         &$86.85$&$92.01$&$84.41$&$90.12$&$75.32$&$90.02$&$72.41$&$79.78$&$53.29$&$71.24$&$53.27$&$54.42$&$55.89$&$65.36$&$57.59$&$55.93$  &$44.92$&$49.58$&$49.36$&$49.58$\\
                                     &                         & Logistic Regression            &$87.19$&$92.41$&$85.37$&$89.41$&$74.91$&$89.69$&$71.85$&$79.81$&$51.90$&$68.78$&$52.30$&$53.36$&$56.43$&$64.83$&$57.51$&$56.32$  &$45.23$&$48.74$&$48.31$&$48.74$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Recurrent Neural Network       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\\hline

\mrow{12}{*}{\rot{Wulczyn}}          & \mrow{6}{*}{\rot{BPE}}  & Support Vector Machine         &$63.89$&$70.55$&$66.24$&$78.47$&$86.42$&$95.59$&$89.11$&$84.15$&$53.45$&$80.34$&$55.30$&$53.23$&$51.95$&$67.90$&$59.86$&$54.19$  &$52.89$&$58.99$&$68.66$&$58.99$\\
                                     &                         & Logistic Regression            &$59.86$&$64.30$&$64.25$&$75.47$&$86.24$&$95.63$&$89.94$&$83.31$&$51.22$&$80.92$&$53.43$&$51.64$&$50.56$&$68.14$&$60.38$&$53.64$  &$50.04$&$57.53$&$68.78$&$57.53$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Recurrent Neural Network       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     & \mrow{6}{*}{\rot{LIWC}} & Support Vector Machine         &$76.06$&$81.72$&$73.73$&$88.34$&$83.20$&$95.05$&$90.72$&$78.42$&$54.49$&$82.51$&$59.19$&$54.12$&$49.82$&$68.02$&$59.77$&$53.23$  &$39.70$&$51.67$&$58.13$&$51.67$\\
                                     &                         & Logistic Regression            &$72.62$&$78.17$&$71.52$&$86.30$&$82.67$&$94.95$&$90.88$&$77.65$&$54.26$&$82.94$&$59.99$&$53.99$&$49.11$&$67.78$&$58.88$&$52.81$  &$39.41$&$51.67$&$58.77$&$51.67$\\
                                     &                         & Multi-Layered Perceptron       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Convolutional Neural Network   &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Recurrent Neural Network       &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $\\
                                     &                         & Long-Short Term Memory network &$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $&$     $  &$     $&$     $&$     $&$     $
\end{tabular}%
}
\caption{Performance of linear baseline models and index-encoded models across in-domain and out-of-domain evaluation sets.}
\label{tab:redux_embedding_davidson}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}[]
\centering
\resizebox{1.4\textheight}{!}{%
\begin{tabular}{ccl|llll|llll|llll|llll|llll}
                                     &                         &                                 & \multicolumn{4}{c|}{Davidson} & \multicolumn{4}{c}{Wulczyn} & \multicolumn{4}{c}{Waseem} & \multicolumn{4}{c}{Waseem-Hovy} & \multicolumn{4}{c}{Garcia} \\
 Training data                       & Representation          & Model                           & F1 & acc & prec & rec         & F1 & acc & prec & rec       & F1 & acc & prec & rec      & F1 & acc & prec & rec           & F1 & acc & prec & rec      \\ \hline
\mrow{12}{*}{\rot{Davidson}}         & \mrow{6}{*}{\rot{BPE}}  & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Recurrent Neural Network        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     & \mrow{6}{*}{\rot{LIWC}} & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Recurrent Neural Network        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\\hline

\mrow{12}{*}{\rot{Wulczyn}}          & \mrow{6}{*}{\rot{BPE}}  & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Recurrent Neural Network        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     & \mrow{6}{*}{\rot{LIWC}} & \textit{Support Vector Machine} &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & \textit{Logistic Regression}    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Multi-Layered Perceptron        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Convolutional Neural Network    &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Recurrent Neural Network        &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
                                     &                         & Long-Short Term Memory network  &    &     &      &             &    &     &      &           &    &     &      &          &    &     &      &               &    &     &      &          \\
\end{tabular}%
}
\caption{Performance of onehot-encoded models across in-domain and out-of-domain datasets (\textit{italic} denotes baseline models).}
\label{tab:redux_embedding_davidson}
\end{table}
\end{landscape}
\zw{Add results and start by rough analysis of just numbers}
\zw{Add plots for development of loss over each epoch}
\zw{Add plots for F1 score during the evaluation set}
\zw{Look at predictions in detail, try to identify where they still fail}
\zw{Do logistic regression and to find out what clear patterns there are}

While the onehot and index encoded tensors should functionally be equal to one another, we see a direct influence of the input layers on the classification scores; with all models showing stronger performance using linear input layers. We propose that the reason for such discrepancies lie in the simpler training procedure of linear layers which don't seek to find relationships between different tokens but instead simply provide a linear function, and embedding layers that seek to identify the relationships between each all tokens in the dataset.

\section{Conclusions and future work}

\zw{Some concluding remarks}
While functionally this limits the vocabulary, there is also loss of information. Future work, could then employ both simple and complex mappings of different forms of words to single tokens that cohere with the LIWC dictionary, thus limiting information loss while retaining the predictive power.

\subsection{Limitations}
Although such lack of recognition can have positive effects, such as lower false positive rate, the politics of not being recognised, as argued by \citet{Benjamin:2019} are not straightforward and the lack of recognition does not provide a guarantee that systemic harm will not occur. For instance, if systems developed to detect abuse did not recognise Multi-cultural London English due to vocabulary reductions, any abuse that was written in that dialect would not be recognised, leaving those users in harms way. Given that LIWC was developed using ``dictionaries, thesauruses, questionnaires, and lists made by research assistants'' \citep{Tauscik:2010} in a North American context, it is highly unlikely that word forms that differ from mainstream usage were included. For instance, the commonly used `brotha' and `bruva' in North American and British contexts, respectively, are absent from the dictionary.
