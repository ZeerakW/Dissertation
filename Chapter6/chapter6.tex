% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter6/Figs/Raster/}{Chapter6/Figs/PDF/}{Chapter6/Figs/}}
\else
    \graphicspath{{Chapter6/Figs/Vector/}{Chapter6/Figs/}}
\fi

\chapter{Multi-Task Learning}\label{chap:mtl}
\zw{Joint work with Joachim Bingel}

\begin{quote}
  ``So hate speech detection is kind of like sentiment analysis++'' -- Anders Johannsen
\end{quote}

\section{Introduction}
One of the frequently made assumptions is that hate speech detection, and in general abusive language detection, shares many similarities to other tasks that similarly seek to deal with identifying subjective human experiences such as sentiment analysis, sarcasm detection, and emotion detection. Indeed, each of these tasks share the characteristic that the identification of each of these on the basis of text is a task of linguistic pragmatics. While they share this unifying characteristic, hate and humour, for instance occupy different processes in \zw{INSERT: Generation/Identification/Whatever} \cite{CITE: Cite papers that find things about humour and hate}. It stands to reason then to ask if there are overlaps in patterns learned by machine learning models, in spite of their dissimilarities of the different tasks.

In recognition that tasks like identifying hate or humour are distinct in spite of their similarities and overlaps, it is not appropriate to use a single multi-class model, where data from all different tasks are simply combined to form a single large dataset with a number of different labels, some that indicate abuse while others do not. Indeed, several issues exist with such a methodology, as the collection periods, populations that are represented, and the collection rationales may differ to such a degree that combining datasets may introduce more confounding factors that the models may learn. On the other hand, as only one of the datasets that we investigate contains labels from multiple tasks, a multi-class prediction model, that is a model that predicts one or more labels for each document it processes is also a less-than perfect solution due to the limitation of only being given space to investigate one possible avenue for influence. Finally, one could also choose to investigate ensemble methods, that rely on a set of models that are each trained for a specific subtask, and are weighted by a model that treats the outputs of the models for each subtask as features. However, a drawback of this method is that it relies on training multiple models that are each optimised for their own subtask. In light of these issues, we choose to investigate multi-task learning as a framework for considering how a number of different tasks influence the modelling of abuse detection.

\zw{CODING: Code an ensemble method as a baseline.}

The use of multi-task learning affords remits from the issues of each of these different approaches and provides several benefits through the ability of multi-task learning models to leverage the information from each task separately while retaining a focus on identifying patterns that can be learned through the different tasks and datasets in question (please see \autoref{sec:model_background} for more detail).

In this work, we implement multi-task learning models that use hard parameter sharing. We choose this strategy due to its relative simplicity in comparison to the soft parameter sharing strategy.

\section{Previous work}

While multi-task learning has been applied for a number of tasks, including language specific tasks such as multi-word expression identification \cite{Bingel-Bjerva:2018}, machine translation \cite{Dong:2015}, and sequence labelling \cite{Rei:2017}. Multi-task learning has also been used in tasks that produce social outcomes such as predicting mental health conditions \cite{Benton:2017},hate speech detecting \cite{Waseem:2018,Rajamanickam:2020,Farha:2020,Djandji:2019}, and rumour verification \cite{Kochkina:2018}.

Considering the sparse previous work on different forms of abusive language detection that employ multi-task learning, two of the approaches consider English text \cite{Waseem:2018,Rajamanickam:2020} and two operate on Arabic \cite{Farha:2020,Djandji:2019}. Focusing on the models applied to English, both works approach the task using hard parameter sharing \cite{Caruana:1997} and randomly choose which task is being trained on. \citet{Waseem:2018} apply a Multi-Layered Perceptron network while \citet{Rajamanickam:2020} use a Long-Short Term Memory network. \citet{Rajamanickam:2020} also implemented a double encoder model in which the primary and auxiliary share an encoder and each have a stacked Bi-directional Long-Short Memory Network that generate another encoding. The primary and auxiliary task models diverge at this point, where the model for the auxiliary task directly passes the second encoding to a Bi-directional Long-Short Memory Network, the output of which is subject to an attention layer and finally passed through to a linear layer and subject to an activation function before producing the prediction of the model. The primary task model sums the encodings from the stacked Bi-directional Long-Short Term Memory networks for the primary and auxiliary task, passing this on to a Bi-directional Long Short Term Memory network. The resulting representation is then passed through an attention layer and passed through an output layer generating the prediction. 

A key difference between the hard parameter sharing models of \citet{Waseem:2018} and \citet{Rajamanickam:2020} is that \citet{Rajamanickam:2020} use a weighting parameter to distinguish between the primary and auxiliary task. \citet{Waseem:2018} only distinguish between the primary and auxiliary tasks through the validation set. The reason for this discrepancy is that \citet{Waseem:2018} seek to use multi-task learning to train a model that is capable of dealing with cross-cultural data while \citet{Rajamanickam:2020} seek to improve classification performance on the primary task. 

The work described in this chapter follows \citet{Rajamanickam:2020}, as we choose auxiliary tasks that we hypothesise will be beneficial to the primary task. While \citet{Rajamanickam:2020} use emotion detection as their only auxiliary task, we use the tasks described in \autoref{sec:mtl_tasks}.

% Indeed, using a multi-task learning framework (please see \autoref{sec:model_background} for more details) \citet{Rajamanickam:2020} find that that there may be some benefits in to machine learning models trained to detect hate if they also learn how to predict emotions. Building on this work, we further explore the use of multi-task learning for abusive language detection. We examine a range of tasks that function as auxiliary tasks including humour detection \cite{CITE: HUMOUR DETECTION}, sarcasm detection \cite{Oraby_sarcasm:2017}, different forms of abuse detection \cite{Waseem-Hovy:2016,Waseem:2016,Davidson:2017,Wulczyn:2017}, detecting whether an argument is fact-based or feeling-based \cite{Oraby_fact_feel:2017}, and the moral foundations of a document \cite{Hoover:2019}.

\section{Learning tasks}\label{sec:mtl_tasks}

For our tasks we choose a mix of tasks that have been identified as challenges in previous work, that relate to findings in previous work \cite{Waseem-Hovy:2016,Davidson:2017,Schmidt:2017} and datasets for distinct forms of abuse \cite{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Wulczyn:2017}.

Following \citet{Waseem:2018}, who train a multi-task learning model trained on \citet{Waseem-Hovy:2016} and \citet{Davidson:2017}. Beyond these two datasets, we also train in \citet{Waseem:2016}, as they examine intersection of racist and sexist hate speech and the influence of political perspectives on annotations. Finally, we include \citet{Wulczyn:2017} as this dataset considers what comments drive conversations to become toxic. Due to the differing sizes of the datasets, we experiment with using \citet{Wulczyn:2017} and \citet{Davidson:2017} as our main tasks and use \citet{Waseem:2016} and \citet{Waseem-Hovy:2016} as auxiliary tasks.

Beyond the datasets annotated for abuse, we identify several other dataset to use as auxiliary tasks. These datasets fall into two categories, those that relate to previously identified issues for abusive language detection models and datasets that address seemingly related issues. In the former category, we choose sarcasm detection as an auxiliary task, using the dataset provided by \citet{Oraby_sarcasm:2017}; for the latter we choose a dataset labelled for moral sentiments \citet{Hoover:2019} and a dataset tagged for whether an argument is based in facts or feelings \citet{Oraby_fact_feel:2017}. Please refer to \autoref{sec:datasets} for more a comprehensive view on each of the datasets that we use in our model.

\section{Modelling}

\subsection{Baseline Models}

\subsection{Experimental Models}



\zw{Describe Multi-Task learning models here}
\zw{Describe the models and experimental settings}

\section{Results}

\zw{Add results and start by rough analysis of just numbers}
\zw{Look at predictions in detail, try to identify where they still fail}
\zw{Do logistic regression and find out if there are things that the models might overfit on}

\section{Conclusions}

