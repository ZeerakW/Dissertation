% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter6/Figs/Raster/}{Chapter6/Figs/PDF/}{Chapter6/Figs/}}
\else
    \graphicspath{{Chapter6/Figs/Vector/}{Chapter6/Figs/}}
\fi

\chapter{Multi-Task Learning}\label{chap:mtl}
\zw{Joint work with Joachim Bingel}

\begin{quote}
  ``So hate speech detection is kind of like sentiment analysis++'' -- Anders Johannsen
\end{quote}

\section{Introduction}
One of the frequently made assumptions is that hate speech detection, and in general abusive language detection, shares many similarities to other tasks that similarly seek to deal with identifying subjective human experiences such as sentiment analysis, sarcasm detection, and emotion detection. Indeed, each of these tasks share the characteristic that the identification of each of these on the basis of text is a task of linguistic pragmatics. While they share this unifying characteristic, hate and humour, for instance occupy different processes in \zw{INSERT: Generation/Identification/Whatever} \cite{CITE: Cite papers that find things about humour and hate}. It stands to reason then to ask if there are overlaps in patterns learned by machine learning models, in spite of their dissimilarities of the different tasks.

In recognition that tasks like identifying hate or humour are distinct in spite of their similarities and overlaps, it is not appropriate to use a single multi-class model, where data from all different tasks are simply combined to form a single large dataset with a number of different labels, some that indicate abuse while others do not. Indeed, several issues exist with such a methodology, as the collection periods, populations that are represented, and the collection rationales may differ to such a degree that combining datasets may introduce more confounding factors that the models may learn. On the other hand, as only one of the datasets that we investigate contains labels from multiple tasks, a multi-class prediction model, that is a model that predicts one or more labels for each document it processes is also a less-than perfect solution due to the limitation of only being given space to investigate one possible avenue for influence. Finally, one could also choose to investigate ensemble methods, that rely on a set of models that are each trained for a specific subtask, and are weighted by a model that treats the outputs of the models for each subtask as features. However, a drawback of this method is that it relies on training multiple models that are each optimised for their own subtask. In light of these issues, we choose to investigate multi-task learning as a framework for considering how a number of different tasks influence the modelling of abuse detection.

\zw{CODING: Code an ensemble method as a baseline.}

The use of multi-task learning affords remits from the issues of each of these different approaches and provides several benefits through the ability of multi-task learning models to leverage the information from each task separately while retaining a focus on identifying patterns that can be learned through the different tasks and datasets in question (please see \autoref{sec:model_background} for more detail).

In this work, we implement multi-task learning models that use hard parameter sharing. We choose this strategy due to its relative simplicity in comparison to the soft parameter sharing strategy.

\section{Previous work}

While multi-task learning has been applied for a number of tasks, including language specific tasks such as multi-word expression identification \cite{Bingel-Bjerva:2018}, machine translation \cite{Dong:2015}, and sequence labelling \cite{Rei:2017}. Multi-task learning has also been used in tasks that produce social outcomes such as predicting mental health conditions \cite{Benton:2017},hate speech detecting \cite{Waseem:2018,Rajamanickam:2020,Farha:2020,Djandji:2019}, and rumour verification \cite{Kochkina:2018}.

Considering the sparse previous work on different forms of abusive language detection that employ multi-task learning, two of the approaches consider English text \cite{Waseem:2018,Rajamanickam:2020} and two operate on Arabic \cite{Farha:2020,Djandji:2019}. Focusing on the models applied to English, both works approach the task using hard parameter sharing \cite{Caruana:1997} and randomly choose which task is being trained on. \citet{Waseem:2018} apply a Multi-Layered Perceptron network while \citet{Rajamanickam:2020} use a Long-Short Term Memory network. \citet{Rajamanickam:2020} also implemented a double encoder model in which the primary and auxiliary share an encoder and each have a stacked Bi-directional Long-Short Memory Network that generate another encoding. The primary and auxiliary task models diverge at this point, where the model for the auxiliary task directly passes the second encoding to a Bi-directional Long-Short Memory Network, the output of which is subject to an attention layer and finally passed through to a linear layer and subject to an activation function before producing the prediction of the model. The primary task model sums the encodings from the stacked Bi-directional Long-Short Term Memory networks for the primary and auxiliary task, passing this on to a Bi-directional Long Short Term Memory network. The resulting representation is then passed through an attention layer and passed through an output layer generating the prediction.

A key difference between the hard parameter sharing models of \citet{Waseem:2018} and \citet{Rajamanickam:2020} is that \citet{Rajamanickam:2020} use a weighting parameter to distinguish between the primary and auxiliary task. \citet{Waseem:2018} only distinguish between the primary and auxiliary tasks through the validation set. The reason for this discrepancy is that \citet{Waseem:2018} seek to use multi-task learning to train a model that is capable of dealing with cross-cultural data while \citet{Rajamanickam:2020} seek to improve classification performance on the primary task.

The work described in this chapter follows \citet{Rajamanickam:2020}, as we choose auxiliary tasks that we hypothesise will be beneficial to the primary task. While \citet{Rajamanickam:2020} use emotion detection as their only auxiliary task, we use the tasks described in \autoref{sec:mtl_tasks}.

% Indeed, using a multi-task learning framework (please see \autoref{sec:model_background} for more details) \citet{Rajamanickam:2020} find that that there may be some benefits in to machine learning models trained to detect hate if they also learn how to predict emotions. Building on this work, we further explore the use of multi-task learning for abusive language detection. We examine a range of tasks that function as auxiliary tasks including humour detection \cite{CITE: HUMOUR DETECTION}, sarcasm detection \cite{Oraby_sarcasm:2017}, different forms of abuse detection \cite{Waseem-Hovy:2016,Waseem:2016,Davidson:2017,Wulczyn:2017}, detecting whether an argument is fact-based or feeling-based \cite{Oraby_fact_feel:2017}, and the moral foundations of a document \cite{Hoover:2019}.

\section{Learning tasks}\label{sec:mtl_tasks}

For our tasks we choose a mix of tasks that have been identified as challenges in previous work, that relate to findings in previous work \cite{Waseem-Hovy:2016,Davidson:2017,Schmidt:2017} and datasets for distinct forms of abuse \cite{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Wulczyn:2017}.

Following \citet{Waseem:2018}, who train a multi-task learning model trained on \citet{Waseem-Hovy:2016} and \citet{Davidson:2017}. Beyond these two datasets, we also train in \citet{Waseem:2016}, as they examine intersection of racist and sexist hate speech and the influence of political perspectives on annotations. Finally, we include \citet{Wulczyn:2017} as this dataset considers what comments drive conversations to become toxic. Due to the differing sizes of the datasets, we experiment with using \citet{Wulczyn:2017} and \citet{Davidson:2017} as our primary tasks and use \citet{Waseem:2016} and \citet{Waseem-Hovy:2016} as auxiliary tasks.

Beyond the datasets annotated for abuse, we identify several other dataset to use as auxiliary tasks. These datasets fall into two categories, those that relate to previously identified issues for abusive language detection models and datasets that address seemingly related issues. In the former category, we choose sarcasm detection as an auxiliary task, using the dataset provided by \citet{Oraby_sarcasm:2017}; for the latter we choose a dataset labelled for moral sentiments \citet{Hoover:2019} and a dataset tagged for whether an argument is based in facts or feelings \citet{Oraby_fact_feel:2017}. Please refer to \autoref{sec:datasets} for more a comprehensive view on each of the datasets that we use in our model.

Finally, considering \citet{Waseem:2018,Davidson:2019} and \citet{Sap:2019} who all highlight issues of dialectal biases along racial lines in hate speech datasets, we considered using \citet{Preotiuc-Unger:2018} as an auxiliary task to predict demographic attributes. However, as this dataset is annotated on a user-level we collect the Twitter timelines for all users and represent the user as the concatenation of all tweets. For modelling purposes, this poses an issue as each user is represented by up to $3,200$ tweets of $280$ characters per tweet. We use the entire Twitter histories, rather than sample from them as people who communicate in multiple dialects, particularly those that are marginalised, are likely to code-switch \cite{CITE: AAE code switching paper}. This, in combination with Mainstream American English being the largest dialect represented in the dataset as it is used by both people who only speak that dialect as well as those that speak other dialects, not using the full histories disproportionately risks erasing the exact dialects we seek to examine. On the other hand, using the full histories presents issues in fitting the into GPU memory during the training phase of the model.

\section{Modelling}

Following the same methodology as in \autoref{chap:liwc} we pre-process all documents using 200 dimensional Byte-Pair Encoding \cite{Heinzerling:2018}. Given the positive results obtained in \autoref{chap:liwc}, we also conduct experiments pre-processing all documents using the Linguistic Inquiry and Word Count (LIWC) dictionary. Through processing all documents through the Byte-Pair Encodings, we minimise the vocabulary as tokens are decomposed to their sub-words, moreover this approach allows us to reduce out-of-vocabulary tokens. Conversely, by preprocessing our documents using LIWC, we increase the number of out-of-vocabulary tokens, however, by reducing the number of unique tokens remaining after only using tokens in the LIWC dictionary. These tokens are then reduced to the smaller set of the combinations of LIWC categories. It is only through the use of LIWC categories that we limit the size of the vocabulary.\vspace{5mm}

We train two variants of all models, one using the Byte-Pair encoded documents and one using the LIWC encoded documents. For our baseline models, we train a Support Vector Machine and a single task Multi-Layered Perceptron. Our experimental models are then all multi-task learning models; we train a multi-task Multi-Layered Perceptron and a Long-Short Term Memory network. Please see \autoref{sec:model_background} for more detail on the models.

% \zw{Describe hyper parameter tuning}
For all models, we seek to optimise their parameters and hyper-parameters, respectively. For the neural models, we use the Optuna library \cite{Optuna:2019} to perform Bayesian hyper-parameter optimisation. To identify the best hyper-parameter configuration, we run $100$ iterations of training our model with different hyper-parameters. For the linear model, we use randomised grid-search as implemented in the library Scikit-Learn \cite{scikit-learn:2019}.

\subsection{Neural Models}

We train two different multi-task model architectures, a Multi-Layered Perceptron network model and a Long-Short Term Memory network model, and a single task Multi-Layered Perceptron network. For all models, we use index-encoded tensors and use an embedding layer as the input layer. All neural models are implemented using PyTorch \cite{CITE: Pytorch paper}.

In order to show the utility of multi-task learning for abuse detection, we use basic architectures and avoid things such as attention layers. Further, each model is subject to dropout for another measure against over-fitting.  While we do subject our model to early stopping, we find that in practice early stopping is not triggered due to the variable performances on the different tasks.

\begin{figure}
  \begin{minipage}{0.5\linewidth}
    \centering
    \includegraphics[scale=0.75]{mlp_mtl_embedding.jpg}
    \label{fig:mlp_mtl_embedding}
  \end{minipage}
  \begin{minipage}{0.5\linewidth}
    \centering
    \includegraphics[scale=0.75]{lstm_mtl_embedding.jpg}
    \label{fig:lstm_mtl_embedding}
  \end{minipage}
  \caption{Multi-task Learning models}
  \label{fig:mtl_models}
\end{figure}

\zw{CODING: Rerun MLP hyper-parameter search.}
\subsubsection{Multi-task Multi-Layered Perceptron}

\zw{Add mathematical function definition}
The multi-task Multi-Layered Perceptron that we implement consists of an input embedding layer, which is trainable. The output of this is then passed to a linear layer that is shared by all tasks. The resulting representation of a document, or batch of documents, is then passed on the another linear layer, and finally it is passed through a linear output layer that maps to the classes for each task, respectively. The result of this final layer is then subject to a softmax layer. Between each layer in the model, softmax, input, and output layer aside, we subject the output of all layers to an activation function and dropout. Please refer to \autoref{fig:mlp_mtl_embedding} for a depiction of this model.

\zw{Insert hyper-parameter space that is searched}

\subsubsection{Multi-task Long-Short Term Memory Network}

\zw{Add mathematical function definition}
\zw{CODING: Run LSTM hyper-parameter search.}
The multi-task Long-Short Term Memory network, similarly initially has a trainable input embedding layer. The output is, similarly the multi-task Multi-Layered Perceptron, passed to a shared hidden layer. The obtained representation is then passed to a Long-Short Term Memory network, which feeds its output into an output-layered that is then subject to a linear layer, mapping to the possible output classes. Similarly to our multi-task Multi-Layered Perceptron, all layers in our model, save the output layer, softmax layer, output layer, and input layer are subject to dropout. Please see \autoref{fig:lstm_mtl_embedding} for a depiction of the network.

\subsection{Baseline Models}

\zw{CODING: Single task SVM model}
\zw{CODING: Single task MLP model}

\subsubsection{Support Vector Machines}

\subsubsection{Single-task Multi-Layered Perceptron}

\subsection{Experimental Models}

To train our experimental models, and ensure direct comparability of the models with one another, we develop a single training process that we subject all models to.\vspace{5mm}

To train our model robustly, we iterate over the dataset in a set number of epochs for each model, identified through the hyper-parameter search. Each epoch consists of $500$ batches of data passed through the model; the batch size, too, is identified through the hyper-parameter search. To avoid the issue of only selecting within the first $500$ batches from each dataset, we randomly shuffle all datasets at the top of each new epoch. Though several different strategies for selecting which task to select have been developed \cite{Waseem:2018}, we choose here to follow the selection process in previous work \cite{Waseem:2018,Rajamanickam:2020} in which the task is randomly selected. For our training procedure, this means that selecting the task to processed is a 6-sided fair die toss, where each task will be chosen approximately every $6$ iterations. We distinguish between the primary task and the auxiliary tasks by introducing a parameter $\beta$ that controls how much each tasks contributes to the loss function. To avoid an issue of vanishing losses, we set the $\beta_{aux} = 1$ for each auxiliary task and $beta_{primary} = \sum^{i}_{i \in #auxiliary_tasks} 1$ for the primary task. Thus, the resulting model emphasises the contributions of the primary tasks equal to the collective contributions of all auxiliary tasks. For each task, we compute the loss using the equation given in \autoref{eq:mtl_loss} on the training data for the task. The computed loss is computed after each batch and back-propagated through the network, weighted by $\beta$. At the end of each epoch, we track the development of our model on the validation set from the primary task.

\begin{equation}\label{eq:mtl_loss}
  loss = loss(\hat{y}, y) \times \beta
\end{equation}

Although we implement early stopping for our model, and set the early stopping criteria to be a minimisation task of loss with a patience of $15$ epochs, we see that early stopping is not activated.

\begin{table}[]
\centering
\begin{tabular}{lll|ll|ll}
                    & \multicolumn{2}{c|}{Davidson} & \multicolumn{2}{c|}{Wulczyn} & \multicolumn{2}{l}{Waseem} \\
                    & MLP           & LSTM          & MLP          & LSTM          & MLP         & LSTM         \\ \hline
Hidden Dimension    &               &               &              &               &             &              \\
Embedding Dimension &               &               &              &               &             &              \\
Activation Function &               &               &              &               &             &              \\
Batch Size          &               &               &              &               &             &              \\
Learning Rate       &               &               &              &               &             &              \\
# Epochs            &               &               &              &               &             &              \\
Dropout             &               &               &              &               &             &
\end{tabular}
\caption{Best parameter setting for each experimental model type.}
\label{tab:mtl_exp_model_parameters}
\end{table}

For each dataset used as a primary task, we perform hyper-parameter search for each type of model, including the baselines. In \autoref{tab:mtl_exp_model_parameters} we see the best performing hyper-parameters for our experimental models. \zw{Insert insights about model hyper-parameters}.

\zw{Describe Multi-Task learning models here}
\zw{Describe the models and experimental settings}
\zw{Add hyper parameter settings for each model}

\section{Results}

\zw{Add results and start by rough analysis of just numbers}
\zw{Add plots for development of loss over each epoch}
\zw{Add plots for F1 score during the evaluation set}
\zw{Look at predictions in detail, try to identify where they still fail}
\zw{Do logistic regression and find out if there are things that the models might overfit on}

\section{Conclusions}

