% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

\chapter[Tasks that Matter: Multi-task Learning for Abusive Language Detection]{Tasks that Matter: Multi-task Learning for Abusive Language Detection\footnotemark{}}\label{chap:mtl}
\footnotetext{This chapter contains elements of an ongoing collaboration with Joachim Bingel, Hero I/S. All contents of the chapter, are original work produced for this dissertation. The shared elements between the project and this chapter are the machine learning model designs.}

\begin{quote}
  ``So is hate speech detection kind of like sentiment analysis++'' -- ACL 2016 Conference Attendee\footnote{Check with AJ that I can attribute.}
\end{quote}

One of the frequently made assumptions is that hate speech detection, and in general abusive language detection, shares many similarities to other tasks that also take on the challenge of identifying and predicting subjective human experiences such as sentiment analysis, sarcasm detection, and emotion detection.
Indeed, each of these tasks share the characteristic that the identification of each of these on the basis of text is a task of linguistic pragmatics and that the interpretation of a given statement will vary on the basis of parties involved in the communicative act.
While they share this unifying characteristic, hate and humour, for instance occupy different but sometimes overlapping processes as highlighted by the NGO partners interviewed by \citet{Rottger:2021}.

Similarities between distinct related tasks pose several interesting questions.
First, is it best to create multiple annotations, either through re-annotating previously published data or creating an entirely dataset, such that each task is addressed in all of the data or should one try to develop modelling architectures that are overlapping?
Second, how much data from each task is necessary to annotate, in the case of creating multiple annotations for each document; or, if the task is approached in terms of developing modelling architectures, how much of the data from each task should be used in training the model, or alternatively, how should the data from each task be weighted to gain the largest modelling improvements?

In this chapter, I approach the question of overlapping data through a question of developing a modelling approach that aims to use potential overlaps between each task.
As each task may be related only in terms of the abstraction required to understand the meaning of a given text, creating mappings between different classes is a complex task that in some cases is not possible.
Moreover, data for each task may be collected from different sources, at different times, from different populations that use different vocabularies resulting in models that may learn spurious patterns in the data that are not trivial to identify and address.
Thus modelling can be approached in two distinct manners. Either all documents are collapsed into a single dataset without creating maps between the different classes or each task remains a distinct task and model architectures such as Multi-Task Learning (MTL) and Ensemble methods are explored.
Here I take the latter approach, developing a MTL model that considers each task simultaneously in tandem and distinctly from all other tasks (see \cref{sub:mtl} for more details on how MTL functions).
I select a MTL modelling approach over an ensemble approach as training an ensemble requires training and optimising $n$ entirely distinct models, one for each task, and a final model that considers the outputs of each model, MTL models on the other hand can be trained such that a models is optimised to perform on its primary task, treating all auxiliary tasks as secondary.
Moreover, as I use a hard-parameter sharing design for my MTL models, an additional benefit is that all auxiliary tasks act as regularisers, even if they are not directly beneficial to the primary task.

Through the use of of MTL models, I find that non-abusive tasks as auxiliary can be beneficial to detecting all forms of abuse examined.
In line with the results in \cref{chap:liwc}, there is a difference in how helpful different abusive language datasets are for each other.
However, in spite of benefits from using MTL over some single-task baselines, some baseline models still out-perform some of the MTL models.
% TODO Add here about how dataset combinations contribute.

% WH - benefits from all aux tasks, abusive and non-abusive
% Wulczyn - benefits from all non-abusive tasks; benefits from WH, Davidson, but not Waseem
% Davidson - Benefits from all non-abusive and abusive tasks;

Thus, in this chapter I ask the following research questions to gain a deeper understanding on how different tasks that may share relations to abuse impact model performance:

\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}
        \item{\textit{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}
        \item{\textit{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}
    \end{enumerate}
\end{minipage}

\section{Previous work}

MTL has previously been applied for a number of tasks in NLP, including language specific tasks such as multi-word expression identification \cite{Bingel-Bjerva:2018}, machine translation \cite{Dong:2015}, and sequence labelling \cite{Rei:2017}.
Further, MTL has also been used in tasks that produce social outcomes such as predicting mental health conditions \cite{Benton:2017}, hate speech detection \cite{Waseem:2018,Rajamanickam:2020,Farha:2020,Djandji:2019}, and rumour verification \cite{Kochkina:2018}.

\subsection{Modelling}
For hate speech detection, and abusive language detection in general, MTL has been applied to English \citep{Waseem:2018,Rajamanickam:2020} and Arabic \citep{Farha:2020,Djandji:2019}.
Considering that I use datasets that are entirely in English, I only consider the previous work for hate speech detection using MTL for English language data.

\citet{Waseem:2018} show that the cultural gaps that exist between different datasets, as a result of their collection strategies and annotation procedures, could be addressed.
Using a hard parameter sharing strategy, they develop a MTL model that uses two different tasks for training.
In their model, sampling of the batches is chosen at random, with one of the tasks set as the main task and a manual mapping between the distinct classes is performed.
The machine learning model chosen by \citet{Waseem:2018} for their MTL experiment is a back-propagated MLP with a tanh activation function and Adam as their optimisation function.
For the input representations \citet{Waseem:2018} experiment with a Bag-of-Words model that uses the $5,000$ most frequent terms and model that uses Byte-Pair encoded input data.
Similarly to \citet{Waseem:2018}, \citet{Rajamanickam:2020} show that using hard parameter sharing strategy for MTL with an auxiliary task can aid in the detection of hate speech.
Rather than using a different task coded for abuse as \citet{Waseem:2018} do, \citet{Rajamanickam:2020} instead ask whether jointly learning which emotions are invoked in a given task can aid in the detection of abuse.
Moreover, the architectures of the two different approaches diverge from one another.
\citet{Rajamanickam:2020} implement a double encoder model in which the primary and auxiliary share an encoder and each have a stacked Bi-directional Long-Short Memory Network that generate a second encoding.
The primary and auxiliary task models developed by \citet{Rajamanickam:2020} diverge at this point. The auxiliary task model directly passes the second encoding to a Bi-directional Long-Short Memory Network, the output of which is subject to an attention layer and finally passed through to a linear layer and subject to an activation function before producing the prediction of the model.
The primary task model sums the encodings obtained from the stacked Bi-directional Long-Short Term Memory networks for the primary and auxiliary task, passing this on to a Bi-directional Long Short Term Memory network.
The resulting representation is then passed through an attention layer and passed through an output layer generating the prediction.
A key difference between the hard parameter sharing models of \citet{Waseem:2018} and \citet{Rajamanickam:2020} is that the latter use a weighting parameter to distinguish between the primary and auxiliary task.
\citet{Waseem:2018} only distinguish between the primary and auxiliary tasks through the validation set.
The reason for this discrepancy is that \citet{Waseem:2018} seek to use multi-task learning to train a model that is capable of dealing with cross-cultural data, that is a model that is able to perform on both tasks. 
\citet{Rajamanickam:2020} on the other hand seek to improve classification performance on the primary task, thus considering any performance gains on the auxiliary a side-benefit.
This discrepancy is the result of a natural prioritisation question, as the goal of \citet{Rajamanickam:2020} is to improve classification performance for abuse on a single dataset whereas \citet{Waseem:2018} seek to identify a classifier that can generalise beyond beyond the single dataset.

The work described in this chapter follows \citet{Rajamanickam:2020} in their focus on improving classification performances on the primary task.
For this reason, I choose auxiliary tasks (see \cref{sec:mtl_data} for an overview of the auxiliary datasets) that have been hypothesised as relevant to the question of detecting different forms of online abuse.

\subsection{Learning Tasks}\label{sec:mtl_tasks}

Multi-task learning, as the name of the framework implies, requires distinct tasks for learning, where each unique auxiliary task asks how learning representations from that task influences model performance on the primary task.
We saw in \cref{chap:liwc}, the trained on each abusive dataset has different applications onto other datasets in the case of binary classification. Therefore, I choose to use three different tasks for abusive language as main tasks.
In contrast to the method in \cref{chap:liwc}, I do not binarise, or otherwise modify the classes in from those proposed by the authors of the datasets.
This provides for the more challenging tasks of predicting the type of abuse in addition to whether content is abusive or not.
A further consequence of not binarising the label sets for the main tasks is that the classes don't directly map onto other datasets.
This means that I preclude considerations of generalisability onto other datasets for abuse without further reduction of the predicted labels into a binarised label space.
Here I provide brief descriptions of the different datasets and the rationale for their inclusion, for more detail please refer to \cref{sec:datasets} and \cref{sub:liwc_datasets}.

\subsubsection{Main Task Datasets}
For the main tasks, I choose to use the \textit{Toxicity} dataset \citep{Wulczyn:2017}, the \textit{Offence} dataset \citep{Davidson:2017}, and the \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016}. I choose these three datasets in part due to their size and in part due to their examining of three different aspects of abuse.
Through this choice, I aim to identify which auxiliary tasks can improve performance for each type of abuse.
Each main task dataset is also used as an auxiliary task when it is not the used as a main task.

\paragraph{Hate Speech}
The \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} as proposed consists of $3,383$ comments labelled as sexist, $1,972$ labelled as racist and $11,559$ labelled as neither sexist or racist.
This dataset was proposed as a first step towards modelling racialised and gendered hate speech.
I use this dataset to show that the MTL framework can be used to distinguish between different targets of hate, as this dataset seeks to identify different forms of hate speech.
Beyond using this dataset to show the ability of MTL models to learn how to distinguish different forms of hate speech, this dataset also provides the largest source of hate speech, which otherwise is vanishingly small in other other main task datasets.

\paragraph{Offence}
The \textit{Offence} dataset \citep{Davidson:2017} was proposed to distinguish `offensive' content from `hateful' and content that is neither `hateful' or `offensive'.
In the class distribution proposed by \citet{Davidson:2017}, the `offensive' class occupies the vast majority of the dataset, with $19,190$ documents labelled into the class, followed by the negative class which consists of $4,163$ documents, and finally the `hateful' class which contains only $1,430$ documents.
As such, the class distribution for this dataset varies strongly from the \textit{Hate Speech} and the \textit{Toxicity} dataset, with the majority class being one of the two positive classes.
In using this dataset for the main task, I show that MTL models can provide a viable modelling approach in spite of a significantly different class distribution.

\paragraph{Toxicity}
The \textit{Toxicity} dataset \citep{Wulczyn:2017} provides a special case.
For one, it is the largest dataset consisting of $159,686$ labelled comments split into a training set of $95,692$ comments, a validation set of $32,128$ comments, and an evaluation set of $31,866$ comments.
In total, this dataset has more than $100,000$ more comments than either the \textit{Toxicity} or \textit{Hate Speech} datasets.
Second, the dataset proposes a binary classification of `toxic' and `not toxic'.
Thus, the results from the MTL models trained on this dataset can be directly compared with the results obtained in \cref{chap:liwc}, unlike the models where the main task is \textit{Offence} or \textit{Hate Speech}.
Thus, I use this dataset to anchor the performances of the MTL models within the context of the preceding chapter and to show the impact of using a large scale dataset for abuse for MTL modelling.

\subsubsection{Auxiliary Task Datasets}

I choose the auxiliary task datasets for two different purposes: 1) to investigate the impact of using other datasets for abusive language as auxiliary tasks and 2) to examine how datasets that are labelled for other tasks can influence modelling for abuse.
To answer the first question, I use the three main task datasets in turn as auxiliary datasets when they are not serving as the main task. Moreover, to address the issue of the poor representation of content labelled within as hateful, I also use the \textit{Hate Expert} dataset.
Addressing the second motivation, I use a dataset labelled for sarcasm \citep{Oraby_sarcasm:2016}, a dataset labelled for the moral sentiment invoked by the text \citep{Hoover:2019}, and finally a dataset where documents are labelled for whether arguments are primarily based in emotion or in facts \citep{Oraby_factfeel:2015}.

\paragraph{Sarcasm}

\paragraph{Argument Basis}

\paragraph{Moral Sentiment}

\paragraph{Hate Expert}



% TODO Introduce datasets

\zw{Use Davidson, Waseem-Hovy, and Wulczyn as main tasks.}
\zw{Use oraby factfeel, hoover, oraby sarcasm as aux tasks.}
% TODO Expand
Sarcasm has been identified as one of the challenges for hate speech detection by both technical experts and civil society alike \citep{Rottger:2021}.

% TODO WOrk in about Oraby_factfeel.
A number of previous works on hate speech detection have suggested that many users who utter hate speech do so infrequently, suggesting that discriminatory speech may be produced in moments of carelessness and high emotional basis. Moreover, as is apparent from the motivations used by \citet{Garcia:2019} for using StormFront as a data source, white supremacists may seek to mask their discrimination behind the use and distortions of fact. Thus, whether hate is produced in the spur of the moment or is a part of a longer pattern, the basis upon which the argument is made, whether it is fact-based or based on emotion, may provide useful signals for learning to predict hate speech and abuse.

For our tasks we choose a mix of tasks that have been identified as challenges in previous work, that relate to findings in previous work \cite{Waseem-Hovy:2016,Davidson:2017,Schmidt:2017} and datasets for distinct forms of abuse \cite{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Wulczyn:2017}.

Following \citet{Waseem:2018}, who train a multi-task learning model trained on \citet{Waseem-Hovy:2016} and \citet{Davidson:2017}. Beyond these two datasets, we also train in \citet{Waseem:2016}, as they examine intersection of racist and sexist hate speech and the influence of political perspectives on annotations. Finally, we include \citet{Wulczyn:2017} as this dataset considers what comments drive conversations to become toxic. Due to the differing sizes of the datasets, we experiment with using \citet{Wulczyn:2017}, \citet{Davidson:2017}, and \citet{Waseem:2016} as our primary tasks and use \citet{Waseem-Hovy:2016} as an auxiliary task.

Beyond the datasets annotated for abuse, we identify several other dataset to use as auxiliary tasks. These datasets fall into two categories, those that relate to previously identified issues for abusive language detection models and datasets that address seemingly related issues. In the former category, we choose sarcasm detection as an auxiliary task, using the dataset provided by \citet{Oraby_sarcasm:2016}; for the latter we choose a dataset labelled for moral sentiments \citet{Hoover:2019} and a dataset tagged for whether an argument is based in facts or feelings \citet{Oraby_factfeel:2015}. Please refer to \autoref{sec:datasets} for more a comprehensive view on each of the datasets that we use in our model.

Finally, considering \citet{Waseem:2018,Davidson:2019} and \citet{Sap:2019} who all highlight issues of dialectal biases along racial lines in hate speech datasets, we considered using \citet{Preotiuc-Unger:2018} as an auxiliary task to predict demographic attributes. However, as this dataset is annotated on a user-level we collect the Twitter timelines for all users and represent the user as the concatenation of all tweets. For modelling purposes, this poses an issue as each user is represented by up to $3,200$ tweets of $280$ characters per tweet. We use the entire Twitter histories, rather than sample from them as people who communicate in multiple dialects, particularly those that are marginalised, are likely to code-switch \cite{CITE: AAE code switching paper}. This, in combination with Mainstream American English being the largest dialect represented in the dataset as it is used by both people who only speak that dialect as well as those that speak other dialects, not using the full histories disproportionately risks erasing the exact dialects we seek to examine. On the other hand, using the full histories presents issues in fitting the into GPU memory during the training phase of the model.

\zw{INSERT: Table of BPE and LIWC vocabularies}
\zw{INSERT: Short paragraph on vocabularies.}

\section{Modelling}

% TODO Use MLP without pre-trained layer to keep consistent with prior chapter.

Following the same methodology as in \autoref{chap:liwc} we pre-process all documents using 200 dimensional Byte-Pair Encoding \cite{Heinzerling:2018}. Given the positive results obtained in \autoref{chap:liwc}, we also conduct experiments pre-processing all documents using the Linguistic Inquiry and Word Count (LIWC) dictionary. Through processing all documents through the Byte-Pair Encodings, we minimise the vocabulary as tokens are decomposed to their sub-words, moreover this approach allows us to reduce out-of-vocabulary tokens. Conversely, by preprocessing our documents using LIWC, we increase the number of out-of-vocabulary tokens, however, by reducing the number of unique tokens remaining after only using tokens in the LIWC dictionary. These tokens are then reduced to the smaller set of the combinations of LIWC categories. It is only through the use of LIWC categories that we limit the size of the vocabulary.\vspace{5mm}

We train two variants of all models, one using the Byte-Pair encoded documents and one using the LIWC encoded documents. For our baseline models, we train a Support Vector Machine and a single task Multi-Layered Perceptron. Our experimental models are then all multi-task learning models; we train a multi-task Multi-Layered Perceptron and a Long-Short Term Memory network. Please see \autoref{sec:model_background} for more detail on the models.

% \zw{Describe hyper parameter tuning}
For all models, we seek to optimise their parameters and hyper-parameters, respectively. For the neural models, we use the Weights and Biases \cite{Wandb} to perform Bayesian hyper-parameter optimisation. To identify the best hyper-parameter configuration, we run $100$ iterations of training our model with different hyper-parameters. For the linear model, we use randomised grid-search as implemented in the library Scikit-Learn \cite{Pedregosa:2011}.

\subsection{Neural Models}

We train two different multi-task model architectures, a Multi-Layered Perceptron network model and a Long-Short Term Memory network model, and a single task Multi-Layered Perceptron network. For all models, we use index-encoded tensors and use an embedding layer as the input layer. All neural models are implemented using PyTorch \cite{Paszke:2019}.

In order to show the utility of multi-task learning for abuse detection, we use basic architectures and avoid things such as attention layers. Further, each model is subject to dropout for another measure against over-fitting.  While we do subject our model to early stopping, we find that in practice early stopping is not triggered due to the variable performances on the different tasks.

\begin{figure}
  \begin{minipage}{0.5\linewidth}
    \centering
    \includegraphics[scale=0.75]{mlp_mtl_embedding.jpg}
    \label{fig:mlp_mtl_embedding}
  \end{minipage}
  \begin{minipage}{0.5\linewidth}
    \centering
    \includegraphics[scale=0.75]{lstm_mtl_embedding.jpg}
    \label{fig:lstm_mtl_embedding}
  \end{minipage}
  \caption{Multi-task Learning models}
  \label{fig:mtl_models}
\end{figure}


\subsubsection{Multi-task Multi-Layered Perceptron}

The multi-task Multi-Layered Perceptron that we implement consists of an input embedding layer, which is trainable. The output of this is then passed to a linear layer that is shared by all tasks. The resulting representation of a document, or batch of documents, is then passed on the another linear layer, and finally it is passed through a linear output layer that maps to the classes for each task, respectively. The result of this final layer is then subject to a softmax layer. Between each layer in the model, softmax, input, and output layer aside, we subject the output of all layers to an activation function and dropout. Please refer to \autoref{fig:mlp_mtl_embedding} for a depiction of this model.

\zw{Insert hyper-parameter space that is searched}

\subsubsection{Multi-task Long-Short Term Memory Network}

\zw{Add mathematical function definition}
\zw{CODING: Run LSTM hyper-parameter search.}
The multi-task Long-Short Term Memory network, similarly initially has a trainable input embedding layer. The output is, similarly the multi-task Multi-Layered Perceptron, passed to a shared hidden layer. The obtained representation is then passed to a Long-Short Term Memory network, which feeds its output into an output-layered that is then subject to a linear layer, mapping to the possible output classes. Similarly to our multi-task Multi-Layered Perceptron, all layers in our model, save the output layer, softmax layer, output layer, and input layer are subject to dropout. Please see \autoref{fig:lstm_mtl_embedding} for a depiction of the network.

\subsection{Baseline Models}

For our baseline models, we train all single-task models using Support Vector Machines (see \autoref{sec:model_background}) while we use a Logistic Regression classifier for on top of Support Vector Machines for the ensemble baseline. For all model types, we train two variants, one that is trained on the BPE tokenised text and one that is trained on the LIWC encoded text. For our Support Vector Machines and ensemble classifiers, we perform a grid-search over the parameters of the models. For each training dataset, we fit a vectoriser to the training set and pass all other datasets through the fitted vectoriser. While this allows for every datasets to be passed through the model, by design this method limits the vocabulary to that which exists in the training dataset. For this reason, we do not perform any operations that limit the occurrence of tokens in the vocabulary. Further, for the byte-pair encoded documents out-of-vocabulary tokens are likely to appear with smaller frequency as tokens unknown to the pre-trained Byte-Pair Embeddings are broken down into subword units that are likely to be repeated across the different datasets.

For the grid search for the linear models, we explore the regularisation strength and the penalty provided for the models. For the regularisation strength we experiment with $C\in \{0.1, 0.2, 0.3, \ldots 1.0\}$, where the value of $C$ is inversely proportional to $C$, and we explore using an $L1$ and $L2$ penalty. The grid search is performed by training our linear baseline model trained on the training data for the primary task, using five-fold cross-validation. To arrive at the best performing parameters for a given model, a held-out subset of the training data is used to evaluate on. Following the conclusion of the parameter search, we evaluate the model with the optimal parameters on a held-out validation set, and finally we apply the model on the test data.

\subsubsection{Linear Single Task Models}

Following previous work \cite{Waseem:2016,Davidson:2017}, we use a Support Vector Machine with linear kernels for our single-task models. Please consult \autoref{tab:mtl_svm_parameters} for the optimal setting for parameters for the Support Vector Machine for each training set.

\begin{table}[]
\centering
\begin{tabular}{l|ll|ll}
                     & \multicolumn{2}{c}{BPE} & \multicolumn{2}{c}{LIWC} \\ \hline
                     &         & Penalty       & $C$      & Penalty       \\ \hline
\cite{Waseem:2016}   & $0.8$   & L2            & $0.8$    & L2            \\
\cite{Davidson:2017} & $0.2$   & L2            & $0.3$    & L2            \\
\cite{Wulczyn:2017}  & $0.1$   & L2            & $1.0$    & L2
\end{tabular}
\caption{Optimal parameters for single task Support Vector Machines with linear kernels.}
\label{tab:mtl_svm_parameters}
\end{table}

As we note in \autoref{tab:mtl_svm_parameters}, all models prefer a L2-loss. Regardless of the model, we note that the Byte-Pair encoded data prefers stronger regularisation while the LIWC encoded data prefers weaker regularisation. This trend is most notable on the models trained on \citet{Wulczyn:2017}, where the strongest possible regularisation is used for the Byte-Pair encoded dataset and the weakest possible for the LIWC encoded dataset. This difference in regularisation strength depending on the input data is likely strongly influenced by the size of the vocabularies, with $95,364$ tokens for the BPE encoded data and $1007$ for the LIWC encoded data (see \autoref{tab:mtl_vocab_sizes}). Interestingly, a difference in vocabulary sizes does not explain the lack of change in regularisation strength for the \citet{Waseem:2016}. However, we note that while the training data for \citet{Wulczyn:2017} consists of $95,364$ unique tokens over $95,692$ documents \citet{Waseem:2016} consists of $11,921$ unique tokens over only $5,526$ training documents. The vocabulary for the LIWC encoded \citet{Waseem:2016} dataset consists of $743$ tokens, where the least frequently used token \textit{`RELATIV\_COGPROC\_POWER\_CAUSE\_MOTION\_WORK\_DRIVES\_ACHIEVE'} occurs $6$ times. In comparison, only $2187$ out of the $11,921$ tokens occur $6$ or more times. In the more than $5,000$ tokens that are unique to a single document, and by extension class, allowing for a model to over-fit on such tokens requiring less regularisation of those tokens.
\zw{Check for logic: Does this argument make sense?}

\subsubsection{Ensemble Classifier}

As multi-task learning shares some resemblance with the ensemble training paradigm, we implement an ensemble as another baseline. Unlike the Support Vector Machines that use a single vectoriser for all datasets, in our ensemble model we train a vectoriser for each dataset and train a model to predict that task exclusively. To ensure that the model for each task performs as best possible, we perform a grid-search over each task model with a similar five-fold cross validation to the single-task Support Vector Machines.
\begin{figure}
  \centering
%  \includegraphics[scale=0.75]{ensemble_mtl.jpg}
  \label{fig:ensemble_mtl}
\end{figure}

Once each component model has been trained, we train a Logistic Regression Classifier on the main task training data and evaluate on the corresponding test set. The predictions from each single task model serve as features for the Logistic Regression Classifier to be trained on. Thus, the logistic regression classifier never sees any of the input text of the model, as that is only provided to the models whose predictions it relies on. To allow for this, each document must be vectorised several times, once for each task model in the ensemble. Finally, a vectoriser is fitted on the predicted labels from each task classifier, and presented to the main task model to be trained on. Please see \autoref{fig:ensemble_mtl} for a depiction of the training and prediction procedure the ensemble classifier.

\begin{table}[]
\centering
\begin{tabular}{l|ll|ll}
                     & \multicolumn{2}{c}{BPE} & \multicolumn{2}{c}{LIWC} \\ \hline
                     & $C$     & Penalty       & $C$      & Penalty       \\ \hline
\cite{Waseem:2016}   & $0.1$   & L2            & $0.2$    & L2             \\
\cite{Davidson:2017} & $0.4$   & L2            & $0.6$    & L2             \\
\cite{Wulczyn:2017}  & $0.2$   & L2            & $0.4$    & L2
\end{tabular}
\caption{Optimal parameters for the Logistic Regression ensemble classifier.}
\label{tab:mtl_ensemble_parameters}
\end{table}

Considering the values of $C$ in \autoref{tab:mtl_ensemble_parameters}, we similarly note a similarity to \autoref{tab:mtl_svm_parameters} that the strength of regularisation drops as the input to the task models moves from Byte-Pair Encoding to LIWC encoded documents. In this case, as we are performing the parameter search on the same feature set, namely a prediction from each classifier, the vocabulary sizes and the specific vocabularies are unlikely to directly provide an impact. Instead we interpret these changes in regularisation strength as a downstream effect of the lower regularisation strength for each model.

\subsubsection{Single Task Multi-Layered Perceptron}

\zw{CODING: Single task MLP model}

\subsection{Experimental Models}

Three different scenarios
\begin{enumerate}
  \item Aux tasks are also abusive language tasks
  \item Aux tasks are not abuse tasks
  \item Aux tasks are abuse and not abuse tasks
\end{enumerate}

To train our experimental models, and ensure direct comparability of the models with one another, we develop a single training process that we subject all models to.\vspace{5mm}

To train our model robustly, we iterate over the dataset in a set number of epochs for each model, identified through the hyper-parameter search. Each epoch consists of $500$ batches of data passed through the model; the batch size, too, is identified through the hyper-parameter search. To avoid the issue of only selecting within the first $500$ batches from each dataset, we randomly shuffle all datasets at the top of each new epoch. Though several different strategies for selecting which task to select have been developed \cite{Waseem:2018}, we choose here to follow the selection process in previous work \cite{Waseem:2018,Rajamanickam:2020} in which the task is randomly selected. For our training procedure, this means that selecting the task to processed is a 6-sided fair die toss, where each task will be chosen approximately every $6$ iterations. We distinguish between the primary task and the auxiliary tasks by introducing a parameter $\beta$ that controls how much each tasks contributes to the loss function. To avoid an issue of vanishing losses, we set the $\beta_{aux} = 1$ for each auxiliary task and $beta_{primary} = \sum^{i}_{i \in \#auxiliary_tasks} 1$ for the primary task. Thus, the resulting model emphasises the contributions of the primary tasks equal to the collective contributions of all auxiliary tasks. For each task, we compute the loss using the equation given in \autoref{eq:mtl_loss} on the training data for the task. The computed loss is computed after each batch and back-propagated through the network, weighted by $\beta$. At the end of each epoch, we track the development of our model on the validation set from the primary task.

\begin{equation}\label{eq:mtl_loss}
  loss = loss(\hat{y}, y) \times \beta
\end{equation}

For each dataset used as a primary task, we perform hyper-parameter search for each type of model, including the baselines. In \autoref{tab:mtl_exp_model_parameters} we see the best performing hyper-parameters for our experimental models. For all models, we see that they prefer a shared dimensionality of $256$ units, beyond this commonality which we set for our model, no other hyper-parameters are shared across all models. We do note however, that most models prefer $ReLU$ as their activation function over $tanh$. Similarly, most models prefer a batch size of $64$ over $32$. On the other hand, the number of epochs, the dropout rate, and the learning rate are variable across the models.
\zw{Insert insights about model hyper-parameters}.

\begin{landscape}
\begin{table}[]
\centering
\begin{tabular}{lllll|llll|llll}
                    & \multicolumn{4}{c|}{Davidson}   & \multicolumn{4}{c|}{Wulczyn}  & \multicolumn{4}{c}{Waseem}          \\ \hline
                    & \multicolumn{2}{c}{BPE} & \multicolumn{2}{c|}{LIWC} &  \multicolumn{2}{c}{BPE} & \multicolumn{2}{c|}{LIWC} &  \multicolumn{2}{c}{BPE} &  \multicolumn{2}{c}{LIWC} \\
                    & MLP       & LSTM & MLP      & LSTM & MLP      & LSTM & MLP      & LSTM & MLP      & LSTM     & MLP      & LSTM \\
Shared Dimension    & $256$     &      & $256$    &      & $256$    &      & $256$    &      & $256$    & $256$    & $256$    &      \\
Hidden Dimension    & -         &      & -        &      & -        &      & -        &      & -        & $300$    & -        &      \\
Embedding Dimension & $100$     &      & $100$    &      & $100$    &      & $100$    &      & $300$    & $100$    & $100$    &      \\
Activation Function & $ReLU$    &      & $ReLU$   &      & $ReLU$   &      & $ReLU$   &      & $Tanh$   & $ReLU$   & $ReLU$   &      \\
Batch Size          & $64$      &      & $64$     &      & $32$     &      & $32$     &      & $64$     & $64$     & $64$     &      \\
Learning Rate       & $0.001$   &      & $0.001$  &      & $0.01$   &      & $0.01$   &      & $0.001$  & $0.0001$ & $0.001$  &      \\
\# Epochs           & $200$     &      & $200$    &      & $100$    &      & $100$    &      & $100$    & $200$    & $200$    &      \\
Dropout             & $0.3$     &      & $0.2$    &      & $0.1$    &      & $0.2$    &      & $0.3$    & $0.1$    & $0.1$    &      \\
Validation Score    & $0.5249$  &      & $0.5410$ &      & $0.6885$ &      & $0.5304$ &      & $0.4054$ & $0.3336$ & $0.2995$ &
\end{tabular}
\caption{Best parameter setting for each experimental model type.}
\label{tab:mtl_exp_model_parameters}
\end{table}
\end{landscape}

\zw{Identify which models will be experimental models once good results come out.}

\zw{Describe the models and experimental settings}
\zw{CODING: Do Ablation test}

\section{Results}

\zw{INSERT: Baseline scores}
\zw{Add results and start by rough analysis of just numbers}
\zw{Add baseline experiment results}
\zw{Show which models contribute most to ensemble models}
\zw{Add plots for development of loss over each epoch}
\zw{Add plots for F1 score during the evaluation set}
\zw{Look at predictions in detail, try to identify where they still fail}
\zw{Do logistic regression and find out if there are things that the models might overfit on}

\section{Conclusions}

