% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

\chapter[Tasks that Matter: Multi-task Learning for Abusive Language Detection]{Tasks that Matter: Multi-task Learning for Abusive Language Detection\footnotemark{}}\label{chap:mtl}
\footnotetext{This chapter contains elements of an ongoing collaboration with Joachim Bingel, Hero I/S. All contents of the chapter, are original work produced for this dissertation. The shared elements between the project and this chapter are the machine learning model designs.}

\begin{quote}
  ``So is hate speech detection kind of like sentiment analysis++'' -- ACL 2016 Conference Attendee\footnote{Check with AJ that I can attribute.}
\end{quote}

One of the frequently made assumptions is that hate speech detection, and in general abusive language detection, shares many similarities to other tasks that also take on the challenge of identifying and predicting subjective human experiences such as sentiment analysis, sarcasm detection, and emotion detection.
Indeed, each of these tasks share the characteristic that the identification of each of these on the basis of text is a task of linguistic pragmatics and that the interpretation of a given statement will vary on the basis of parties involved in the communicative act.
While they share this unifying characteristic, hate and humour, for instance occupy different but sometimes overlapping processes as highlighted by the NGO partners interviewed by \citet{Rottger:2021}.

Similarities between distinct related tasks pose several interesting questions.
First, is it best to create multiple annotations, either through re-annotating previously published data or creating an entirely dataset, such that each task is addressed in all of the data or should one try to develop modelling architectures that are overlapping?
Second, how much data from each task is necessary to annotate, in the case of creating multiple annotations for each document; or, if the task is approached in terms of developing modelling architectures, how much of the data from each task should be used in training the model, or alternatively, how should the data from each task be weighted to gain the largest modelling improvements?

In this chapter, I approach the question of overlapping data through a question of developing a modelling approach that aims to use potential overlaps between each task.
As each task may be related only in terms of the abstraction required to understand the meaning of a given text, creating mappings between different classes is a complex task that in some cases is not possible.
Moreover, data for each task may be collected from different sources, at different times, from different populations that use different vocabularies resulting in models that may learn spurious patterns in the data that are not trivial to identify and address.
Thus modelling can be approached in two distinct manners. Either all documents are collapsed into a single dataset without creating maps between the different classes or each task remains a distinct task and model architectures such as Multi-Task Learning (MTL) and Ensemble methods are explored.
Here I take the latter approach, developing a MTL model that considers each task simultaneously in tandem and distinctly from all other tasks (see \cref{sub:mtl} for more details on how MTL functions).
I select a MTL modelling approach over an ensemble approach as training an ensemble requires training and optimising $n$ entirely distinct models, one for each task, and a final model that considers the outputs of each model, MTL models on the other hand can be trained such that a models is optimised to perform on its primary task, treating all auxiliary tasks as secondary.
Moreover, as I use a hard-parameter sharing design for my MTL models, an additional benefit is that all auxiliary tasks act as regularisers, even if they are not directly beneficial to the primary task.

Through the use of of MTL models, I find that non-abusive tasks as auxiliary can be beneficial to detecting all forms of abuse examined.
In line with the results in \cref{chap:liwc}, there is a difference in how helpful different abusive language datasets are for each other.
However, in spite of benefits from using MTL over some single-task baselines, some baseline models still out-perform some of the MTL models.
% TODO Add here about how dataset combinations contribute.

% WH - benefits from all aux tasks, abusive and non-abusive
% Wulczyn - benefits from all non-abusive tasks; benefits from WH, Davidson, but not Waseem
% Davidson - Benefits from all non-abusive and abusive tasks;

Thus, in this chapter I ask the following research questions to gain a deeper understanding on how different tasks that may share relations to abuse impact model performance:

\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}
        \item{\textit{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}
        \item{\textit{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}
    \end{enumerate}
\end{minipage}

\section{Previous work}

MTL has previously been applied for a number of tasks in NLP, including language specific tasks such as multi-word expression identification \cite{Bingel-Bjerva:2018}, machine translation \cite{Dong:2015}, and sequence labelling \cite{Rei:2017}.
Further, MTL has also been used in tasks that produce social outcomes such as predicting mental health conditions \cite{Benton:2017}, hate speech detection \cite{Waseem:2018,Rajamanickam:2020,Farha:2020,Djandji:2019}, and rumour verification \cite{Kochkina:2018}.

\subsection{Modelling}
For hate speech detection, and abusive language detection in general, MTL has been applied to English \citep{Waseem:2018,Rajamanickam:2020} and Arabic \citep{Farha:2020,Djandji:2019}.
Considering that I use datasets that are entirely in English, I only consider the previous work for hate speech detection using MTL for English language data.

\citet{Waseem:2018} show that the cultural gaps that exist between different datasets, as a result of their collection strategies and annotation procedures, could be addressed.
Using a hard parameter sharing strategy, they develop a MTL model that uses two different tasks for training.
In their model, sampling of the batches is chosen at random, with one of the tasks set as the main task and a manual mapping between the distinct classes is performed.
The machine learning model chosen by \citet{Waseem:2018} for their MTL experiment is a back-propagated MLP with a tanh activation function and Adam as their optimisation function.
For the input representations \citet{Waseem:2018} experiment with a Bag-of-Words model that uses the $5,000$ most frequent terms and model that uses Byte-Pair encoded input data.
Similarly to \citet{Waseem:2018}, \citet{Rajamanickam:2020} show that using hard parameter sharing strategy for MTL with an auxiliary task can aid in the detection of hate speech.
Rather than using a different task coded for abuse as \citet{Waseem:2018} do, \citet{Rajamanickam:2020} instead ask whether jointly learning which emotions are invoked in a given task can aid in the detection of abuse.
Moreover, the architectures of the two different approaches diverge from one another.
\citet{Rajamanickam:2020} implement a double encoder model in which the primary and auxiliary share an encoder and each have a stacked Bi-directional Long-Short Memory Network that generate a second encoding.
The primary and auxiliary task models developed by \citet{Rajamanickam:2020} diverge at this point. The auxiliary task model directly passes the second encoding to a Bi-directional Long-Short Memory Network, the output of which is subject to an attention layer and finally passed through to a linear layer and subject to an activation function before producing the prediction of the model.
The primary task model sums the encodings obtained from the stacked Bi-directional Long-Short Term Memory networks for the primary and auxiliary task, passing this on to a Bi-directional Long Short Term Memory network.
The resulting representation is then passed through an attention layer and passed through an output layer generating the prediction.
A key difference between the hard parameter sharing models of \citet{Waseem:2018} and \citet{Rajamanickam:2020} is that the latter use a weighting parameter to distinguish between the primary and auxiliary task.
\citet{Waseem:2018} only distinguish between the primary and auxiliary tasks through the validation set.
The reason for this discrepancy is that \citet{Waseem:2018} seek to use multi-task learning to train a model that is capable of dealing with cross-cultural data, that is a model that is able to perform on both tasks. 
\citet{Rajamanickam:2020} on the other hand seek to improve classification performance on the primary task, thus considering any performance gains on the auxiliary a side-benefit.
This discrepancy is the result of a natural prioritisation question, as the goal of \citet{Rajamanickam:2020} is to improve classification performance for abuse on a single dataset whereas \citet{Waseem:2018} seek to identify a classifier that can generalise beyond beyond the single dataset.

The work described in this chapter follows \citet{Rajamanickam:2020} in their focus on improving classification performances on the primary task.
For this reason, I choose auxiliary tasks (see \cref{sec:mtl_data} for an overview of the auxiliary datasets) that have been hypothesised as relevant to the question of detecting different forms of online abuse.

\subsection{Learning Tasks}\label{sec:mtl_tasks}

Multi-task learning, as the name of the framework implies, requires distinct tasks for learning, where each unique auxiliary task asks how learning representations from that task influences model performance on the primary task.
We saw in \cref{chap:liwc}, the trained on each abusive dataset has different applications onto other datasets in the case of binary classification. Therefore, I choose to use three different tasks for abusive language as main tasks.
In contrast to the method in \cref{chap:liwc}, I do not binarise, or otherwise modify the classes in from those proposed by the authors of the datasets.
This provides for the more challenging tasks of predicting the type of abuse in addition to whether content is abusive or not.
A further consequence of not binarising the label sets for the main tasks is that the classes don't directly map onto other datasets.
This means that I preclude considerations of generalisability onto other datasets for abuse without further reduction of the predicted labels into a binarised label space.
Here I provide brief descriptions of the different datasets and the rationale for their inclusion, for more detail please refer to \cref{sec:datasets} and \cref{sub:liwc_datasets}.

\subsubsection{Main Task Datasets}
For the main tasks, I choose to use the \textit{Toxicity} dataset \citep{Wulczyn:2017}, the \textit{Offence} dataset \citep{Davidson:2017}, and the \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016}. I choose these three datasets in part due to their size and in part due to their examining of three different aspects of abuse.
Through this choice, I aim to identify which auxiliary tasks can improve performance for each type of abuse.
Each main task dataset is also used as an auxiliary task when it is not the used as a main task.

\paragraph{Hate Speech}
The \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} as proposed consists of $3,383$ comments labelled as sexist, $1,972$ labelled as racist and $11,559$ labelled as neither sexist or racist.
This dataset was proposed as a first step towards modelling racialised and gendered hate speech.
I use this dataset to show that the MTL framework can be used to distinguish between different targets of hate, as this dataset seeks to identify different forms of hate speech.
Beyond using this dataset to show the ability of MTL models to learn how to distinguish different forms of hate speech, this dataset also provides the largest source of hate speech, which otherwise is vanishingly small in other other main task datasets.

\paragraph{Offence}
The \textit{Offence} dataset \citep{Davidson:2017} was proposed to distinguish `offensive' content from `hateful' and content that is neither `hateful' or `offensive'.
In the class distribution proposed by \citet{Davidson:2017}, the `offensive' class occupies the vast majority of the dataset, with $19,190$ documents labelled into the class, followed by the negative class which consists of $4,163$ documents, and finally the `hateful' class which contains only $1,430$ documents.
As such, the class distribution for this dataset varies strongly from the \textit{Hate Speech} and the \textit{Toxicity} dataset, with the majority class being one of the two positive classes.
In using this dataset for the main task, I show that MTL models can provide a viable modelling approach in spite of a significantly different class distribution.

\paragraph{Toxicity}
The \textit{Toxicity} dataset \citep{Wulczyn:2017} provides a special case.
For one, it is the largest dataset consisting of $159,686$ labelled comments split into a training set of $95,692$ comments, a validation set of $32,128$ comments, and an evaluation set of $31,866$ comments.
In total, this dataset has more than $100,000$ more comments than either the \textit{Toxicity} or \textit{Hate Speech} datasets.
Second, the dataset proposes a binary classification of `toxic' and `not toxic'.
Thus, the results from the MTL models trained on this dataset can be directly compared with the results obtained in \cref{chap:liwc}, unlike the models where the main task is \textit{Offence} or \textit{Hate Speech}.
Thus, I use this dataset to anchor the performances of the MTL models within the context of the preceding chapter and to show the impact of using a large scale dataset for abuse for MTL modelling.

\subsubsection{Auxiliary Task Datasets}

I choose the auxiliary task datasets for two different purposes: 1) to investigate the impact of using other datasets for abusive language as auxiliary tasks and 2) to examine how datasets that are labelled for other tasks can influence modelling for abuse.
To answer the first question, I use the three main task datasets in turn as auxiliary datasets when they are not serving as the main task. Moreover, to address the issue of the poor representation of content labelled within as hateful, I also use the \textit{Hate Expert} dataset.
Addressing the second motivation, I use a dataset labelled for sarcasm \citep{Oraby_sarcasm:2016}, a dataset labelled for the moral sentiment invoked by the text \citep{Hoover:2019}, and finally a dataset where documents are labelled for whether arguments are primarily based in emotion or in facts \citep{Oraby_factfeel:2015}.
With the exception of the \textit{Moral Sentiments} dataset, all auxiliary task datasets contain between $~5,000$ and $~16,000$ labelled documents, while the \textit{Moral Sentiments} consists of $~35,000$ labelled documents.
This spread of sizes of auxiliary task datasets allows for considering how auxiliary task dataset size impact the main task.

\paragraph{Sarcasm}
Previous work on hate speech detection \citep{Rottger:2021} has identified that sarcasm and irony can be contributing factors to misclassification from machine learning models as they take literally things that are communicated to be understood figuratively.
In efforts to better understand dialogue in online debate forums, \citet{Oraby_sarcasm:2016} develop a balanced dataset of $6520$ comments labelled for the occurrence of sarcasm.
Through this auxiliary task, MTL models learn representations of how sarcasm is constituted within \textit{Sarcasm} dataset, in addition to the other auxiliary tasks, and the main task.
Finally, through the use of this dataset, I explore how learning representations for sarcasm detection influences prediction of each operationalisation of detecting abuse.

\paragraph{Argument Basis}
Previous work on hate speech detection have suggested that many users who utter hate speech do so infrequently, suggesting that discriminatory speech may be produced in moments of carelessness and high emotionality \citep{Waseem:2016}. 
Moreover, as is apparent from the motivations used by \citet{Garcia:2019} for using StormFront as a data source, white supremacists may seek to mask their discrimination behind the use and distortions of fact.
Thus, whether hate is produced in the spur of the moment or is a part of a longer pattern, the basis upon which the argument is made, whether it is fact-based or based on emotion, may provide useful signals for learning to predict hate speech and abuse.
To model the hypothesis that high emotionality may influence the production of abuse, I include the \textit{Argument Basis} dataset \citep{Oraby_factfeel:2015}.
This dataset was developed using the same underlying data source as the \textit{Sarcasm} dataset, however rather than annotating the dataset for the occurrence of sarcasm, \citet{Oraby_factfeel:2015} annotate $5,848$ comments as being based in either fact or emotion.
The dataset is slightly imbalanced with $59\%$ of the dataset labelled as primarily fact-based and $41\%$ labelled as primarily based in feelings.
MTL models for abuse can take advantage of this dataset by learning a joint representation of the basis of an argument along with the main task in question.
Thus, this auxiliary task can provide insight into the question of whether learning such a joint representation is beneficial to detecting abuse and implicitly provide another signal into the feasibility of more deeply considering the emotional and mental state of the author when writing, e.g. through the use of LIWC in \cref{chap:liwc}.

\paragraph{Moral Sentiments}
The \textit{Moral Sentiments} dataset is annotated for the vices and virtues represented along five different `factors': \texttt{Loyalty/betrayal}, \texttt{care/harm},\\ \texttt{fairness/cheating}, \texttt{authority/subversion} and \texttt{purity/degradation} \citep{Hoover:2019}.
The authors of the dataset suggest that these five factors are likely to be represented in data that contains abuse, through their use of a subset of the \textit{Offence} dataset for annotation.
To explore this further, I use this dataset as an auxiliary task.
While the impacts of moral sentiments on the \textit{Offence} dataset are likely beneficial, using this auxiliary task on other datasets allows for examining whether representing moral sentiments has a positive impact on other datasets labelled for abuse.

\paragraph{Hate Expert}
Finally, as learned from \cref{chap:liwc}, some datasets for abusive language appear to be more closely related to others.
For this reason, I include the \textit{Hate Expert} dataset as an auxiliary task dataset to verify this finding and to provide a dataset to help address the poor representation of hate speech in the classes.
In \cref{chap:liwc} I binarise this dataset, here on the other hand I retain all $4$ classes in the dataset. 
In the expert annotated data, the four classes have a highly imbalanced distribution, the largest class being the negative class consuming $84\%$ of the data, while the second largest class `sexism' consumes $13\%$ of the data, the `racist' class consuming $1.4\%$ of the data and the final class, `both racist and sexist' contributing with $0.7\%$ of the data.

Focusing our attention on the smallest class for a moment, $0.7\%$ of $6,909$ documents means less than $50$ documents are labelled for the minority class, and given that I create stratified splits for the training ($80\%$), validation ($10\%$), and evaluation ($10\%$) sets, less than $40$ documents remain in the training set.
Thus there is not enough data for a machine learning model to learn patterns of abuse in the intersection between racist and sexist speech.
However, I choose to keep this data in the dataset to provide more instance of hate speech and to complicate, albeit only slightly, the question of what constitutes hate for the machine learning systems that use this dataset in the training process.

\section{Modelling}
\zw{Update kernel information if kernel is updated}
For the experiments conducted, I only use one form of tokens to allow for an examination of the impact of the auxiliary tasks rather than the impact of tokenisation.
I choose to represent all documents as their Byte-Pair encoded representations as these minimise the number of out-of-vocabulary tokens as this representation retains competitive performances in \cref{chap:liwc}.
To this end, I pre-process all documents using a $200$ dimensional Byte-Pair Embedding \citep{Heinzerling:2018}.
The pre-processing here follows the same method as in \cref{chap:liwc}, that is each document is lower-cased, all hyper-links are replaced with a `<URL>' token, all usernames are replaced with a `<USER>' token, and all hashtags are replaced with a `<HASHTAG>' token.
Then each document is passed through the Byte-Pair Embeddings to produce the Byte-Pair encoded representations, that is their sub-word units.

I train three different types of baseline models: a linear single-task model where the model is trained and evaluated on the same task, a non-linear single-task model, and a linear ensemble model where a model is trained on the basis of outputs from the auxiliary task models.
In terms of experimental models, I follow \citet{Waseem:2018} in designing a multi-task Multi-Layered Perceptron implemented in PyTorch \citep{Paszke:2019}. I select an MLP over more complex neural networks architectures like Convolutional Neural Networks and Long-Short Term Memory networks due to the speed with which MLPs are trained along with their general performance in \cref{chap:liwc}.

I perform parameter and hyper-parameter optimisation for the linear and non-linear models, respectively.
For the non-linear models I use the Weights and Biases library \citep{Wandb} to perform Bayesian Hyper-Parameter Optimisation.
For the linear models, I use grid-search as implemented in the Scikit-Learn library \citep{Pedregosa:2011}.

Once models have been trained, they are each evaluated on the validation data and the evaluation data.
For non-linear models, the performance on the validation data guides the decision on which parameter configurations are chosen for analysis while for linear models, cross-validation is applied during the grid-search which aids in determining which parameter configuration performs best.

\subsection{Baseline Models}
I train three baseline models: a linear single-task model, a neural network single-task model, and a linear ensemble model.
I choose to use a linear single-task model as a baseline as these often provide a strong baseline against neural network approaches for abuse detection \citep{Cite someone who says that here} while also being fast and efficient to train.
The non-linear neural network baseline is chosen as a counter-point to the linear baseline, using a MLP to more directly be able to consider the influence of the multi-task architecture for the experimental models.
Finally, I choose to an ensemble classifier that is trained on the outputs of linear single-task models for each of the auxiliary task models as an ensemble, similarly to a multi-task model, can take advantage of learned representations for each auxiliary task for producing a prediction for the main task.

\paragraph{Single-Task Baselines}
Following prior work \citep{Waseem:2016,Davidson:2017}, I train all single-task models using a Support Vector Machine with a linear kernel (see \cref{sec:model_background} for more details on SVMs).
All linear single-task models are trained on unigram counts of the Byte-Pair encoded tokens and are subject to parameter-optimisation of the regularisation type ($L1$ and $L2$) and the strength of regularisation (using values $0.1, 0.2, \ldots, 1.0$).
See \cref{tab:linear_singletask_params} for the best parameters for the linear single-task models for each dataset.

\begin{table}[h]
\centering
\begin{tabular}{lll}
                                       & Regulariser & Regularisation Strength \\
\textit{Offence} Linear Single task     & L2          & 0.2                     \\
\textit{Hate Speech} Linear Single task & L2          & 0.1                     \\
\textit{Toxicity} Linear Single task    & L2          & 0.1
\end{tabular}
\caption{Best model parameters for linear single-task models.}
\label{tab:linear_singletask_params}
\end{table}

\paragraph{MLP Single-Task Baseline}
I train a MLP as a non-linear counter-part to the linear single-task models to provide a baseline of the performance of a neural network approach that only relies on the Byte-Pair unigrams for the main task to optimise for the main task.
To ensure that the baseline model is also tuned for optimal performance, I perform a hyper-parameter sweep  over the batch size (${16, 32, 64}$), the dropout value ($[0.0, 0.5]$), the dimensionality of the embedding layers (${64, 100, 200, 300}$), the number of training epochs (${50, 100, 200}$), the dimensionality of the hidden layers (${64, 100, 200, 300}$), the learning rate ($[1^{-5}, 1.0]$), the Non-linearity to apply ($[Tanh, ReLU]$), and the optimiser function (${SGD, ASGD, Adam, AdamW}$).
I conduct at least $50$ independent trials of distinct hyper parameter settings which identify the best hyper parameter configuration as detailed in \cref{tab:mlp_singletask_params}.

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllllll}
                            & Batch Size & Dropout & Embedding Dimension & Epochs & Hidden Dimension & Learning Rate & Non-linearity & Optimiser \\
Offence MLP Single Task     & 64         & 0.318   & 300                 & 200    & 100              & 0.003586      & ReLU         & SGD       \\
Hate Speech MLP Single Task & 64         & 0.1458  & 300                 & 100    & 100              & 0.0007246     & Tanh         & AdamW     \\
Toxicity MLP Single Task    & 64         & 0.1978  & 200                 & 50     & 200              & 0.006056      & ReLU         & Adam     
\end{tabular}%
}
\caption{Best hyper parameters for non-linear single task model for each main task dataset.}
\label{tab:mlp_singletask_params}
\end{table}

\paragraph{Ensemble Baseline}
The ensemble baselines require a different training scheme that relies on a classifier that is trained for each auxiliary task and an ensemble classifier that relies on the outputs of the auxiliary task classifiers, by virtue of the nature of ensemble classifiers.
For this reason, I first train a linear SVM for each auxiliary task and perform a grid search over the type of regulariser ({$L1$, $L2$}) and the strength of the regularisation (${0.1, 0.2,\ldots, 1.0}$) (see \cref{tab:aux_ensemble_params} for the parameter settings for each auxiliary task).
Once all auxiliary task classifiers have been trained, I train a Logistic Regression model on the outputs of the auxiliary task classifiers on the main task training data.
During this training procedure, the ensemble is provided with the training data for the main task, which is vectorised to the vocabulary of each auxiliary task and a prediction is obtained for each task.
For each document, predictions of all auxiliary task classifiers are vectorised and a classifier is trained on the auxiliary task predictions.
While this method allows for every datasets to be passed through the model, by design this method limits the vocabulary to that which exists in the training datasets for the auxiliary tasks, rather than the main task.
This risk however is mitigated by the use of subwords obtained by preprocessing all data through the Byte-Pair embeddings.

\subsubsection{Linear Single Task Models}

Following previous work \cite{Waseem:2016,Davidson:2017}, we use a Support Vector Machine with linear kernels for our single-task models. Please consult \autoref{tab:mtl_svm_parameters} for the optimal setting for parameters for the Support Vector Machine for each training set.

\begin{table}[]
\centering
\begin{tabular}{l|ll|ll}
                     & \multicolumn{2}{c}{BPE} & \multicolumn{2}{c}{LIWC} \\ \hline
                     &         & Penalty       & $C$      & Penalty       \\ \hline
\cite{Waseem:2016}   & $0.8$   & L2            & $0.8$    & L2            \\
\cite{Davidson:2017} & $0.2$   & L2            & $0.3$    & L2            \\
\cite{Wulczyn:2017}  & $0.1$   & L2            & $1.0$    & L2
\end{tabular}
\caption{Optimal parameters for single task Support Vector Machines with linear kernels.}
\label{tab:mtl_svm_parameters}
\end{table}

As we note in \autoref{tab:mtl_svm_parameters}, all models prefer a L2-loss. Regardless of the model, we note that the Byte-Pair encoded data prefers stronger regularisation while the LIWC encoded data prefers weaker regularisation. This trend is most notable on the models trained on \citet{Wulczyn:2017}, where the strongest possible regularisation is used for the Byte-Pair encoded dataset and the weakest possible for the LIWC encoded dataset. This difference in regularisation strength depending on the input data is likely strongly influenced by the size of the vocabularies, with $95,364$ tokens for the BPE encoded data and $1007$ for the LIWC encoded data (see \autoref{tab:mtl_vocab_sizes}). Interestingly, a difference in vocabulary sizes does not explain the lack of change in regularisation strength for the \citet{Waseem:2016}. However, we note that while the training data for \citet{Wulczyn:2017} consists of $95,364$ unique tokens over $95,692$ documents \citet{Waseem:2016} consists of $11,921$ unique tokens over only $5,526$ training documents. The vocabulary for the LIWC encoded \citet{Waseem:2016} dataset consists of $743$ tokens, where the least frequently used token \textit{`RELATIV\_COGPROC\_POWER\_CAUSE\_MOTION\_WORK\_DRIVES\_ACHIEVE'} occurs $6$ times. In comparison, only $2187$ out of the $11,921$ tokens occur $6$ or more times. In the more than $5,000$ tokens that are unique to a single document, and by extension class, allowing for a model to over-fit on such tokens requiring less regularisation of those tokens.
\zw{Check for logic: Does this argument make sense?}

\subsubsection{Ensemble Classifier}

As multi-task learning shares some resemblance with the ensemble training paradigm, we implement an ensemble as another baseline. Unlike the Support Vector Machines that use a single vectoriser for all datasets, in our ensemble model we train a vectoriser for each dataset and train a model to predict that task exclusively. To ensure that the model for each task performs as best possible, we perform a grid-search over each task model with a similar five-fold cross validation to the single-task Support Vector Machines.
\begin{figure}
  \centering
%  \includegraphics[scale=0.75]{ensemble_mtl.jpg}
  \label{fig:ensemble_mtl}
\end{figure}

Once each component model has been trained, we train a Logistic Regression Classifier on the main task training data and evaluate on the corresponding test set. The predictions from each single task model serve as features for the Logistic Regression Classifier to be trained on. Thus, the logistic regression classifier never sees any of the input text of the model, as that is only provided to the models whose predictions it relies on. To allow for this, each document must be vectorised several times, once for each task model in the ensemble. Finally, a vectoriser is fitted on the predicted labels from each task classifier, and presented to the main task model to be trained on. Please see \autoref{fig:ensemble_mtl} for a depiction of the training and prediction procedure the ensemble classifier.

\begin{table}[]
\centering
\begin{tabular}{l|ll|ll}
                     & \multicolumn{2}{c}{BPE} & \multicolumn{2}{c}{LIWC} \\ \hline
                     & $C$     & Penalty       & $C$      & Penalty       \\ \hline
\cite{Waseem:2016}   & $0.1$   & L2            & $0.2$    & L2             \\
\cite{Davidson:2017} & $0.4$   & L2            & $0.6$    & L2             \\
\cite{Wulczyn:2017}  & $0.2$   & L2            & $0.4$    & L2
\end{tabular}
\caption{Optimal parameters for the Logistic Regression ensemble classifier.}
\label{tab:mtl_ensemble_parameters}
\end{table}

Considering the values of $C$ in \autoref{tab:mtl_ensemble_parameters}, we similarly note a similarity to \autoref{tab:mtl_svm_parameters} that the strength of regularisation drops as the input to the task models moves from Byte-Pair Encoding to LIWC encoded documents. In this case, as we are performing the parameter search on the same feature set, namely a prediction from each classifier, the vocabulary sizes and the specific vocabularies are unlikely to directly provide an impact. Instead we interpret these changes in regularisation strength as a downstream effect of the lower regularisation strength for each model.

\subsubsection{Single Task Multi-Layered Perceptron}

\zw{CODING: Single task MLP model}

\subsection{Neural Models}
% TODO Use MLP without pre-trained layer to keep consistent with prior chapter.

We train two different multi-task model architectures, a Multi-Layered Perceptron network model and a Long-Short Term Memory network model, and a single task Multi-Layered Perceptron network. For all models, we use index-encoded tensors and use an embedding layer as the input layer. All neural models are implemented using PyTorch \cite{Paszke:2019}.

In order to show the utility of multi-task learning for abuse detection, we use basic architectures and avoid things such as attention layers. Further, each model is subject to dropout for another measure against over-fitting.  While we do subject our model to early stopping, we find that in practice early stopping is not triggered due to the variable performances on the different tasks.

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{mlp_mtl_embedding.jpg}
  \caption{Multi-task Learning MLP.}
  \label{fig:mlp_mtl}
\end{figure}


\subsubsection{Multi-task Multi-Layered Perceptron}

The multi-task Multi-Layered Perceptron that we implement consists of an input embedding layer, which is trainable. The output of this is then passed to a linear layer that is shared by all tasks. The resulting representation of a document, or batch of documents, is then passed on the another linear layer, and finally it is passed through a linear output layer that maps to the classes for each task, respectively. The result of this final layer is then subject to a softmax layer. Between each layer in the model, softmax, input, and output layer aside, we subject the output of all layers to an activation function and dropout. Please refer to \autoref{fig:mlp_mtl_embedding} for a depiction of this model.

\zw{Insert hyper-parameter space that is searched}


\subsection{Experimental Models}

Three different scenarios
\begin{enumerate}
  \item Aux tasks are also abusive language tasks
  \item Aux tasks are not abuse tasks
  \item Aux tasks are abuse and not abuse tasks
\end{enumerate}

To train our experimental models, and ensure direct comparability of the models with one another, we develop a single training process that we subject all models to.\vspace{5mm}

To train our model robustly, we iterate over the dataset in a set number of epochs for each model, identified through the hyper-parameter search. Each epoch consists of $500$ batches of data passed through the model; the batch size, too, is identified through the hyper-parameter search. To avoid the issue of only selecting within the first $500$ batches from each dataset, we randomly shuffle all datasets at the top of each new epoch. Though several different strategies for selecting which task to select have been developed \cite{Waseem:2018}, we choose here to follow the selection process in previous work \cite{Waseem:2018,Rajamanickam:2020} in which the task is randomly selected. For our training procedure, this means that selecting the task to processed is a 6-sided fair die toss, where each task will be chosen approximately every $6$ iterations. We distinguish between the primary task and the auxiliary tasks by introducing a parameter $\beta$ that controls how much each tasks contributes to the loss function. To avoid an issue of vanishing losses, we set the $\beta_{aux} = 1$ for each auxiliary task and $beta_{primary} = \sum^{i}_{i \in \#auxiliary_tasks} 1$ for the primary task. Thus, the resulting model emphasises the contributions of the primary tasks equal to the collective contributions of all auxiliary tasks. For each task, we compute the loss using the equation given in \autoref{eq:mtl_loss} on the training data for the task. The computed loss is computed after each batch and back-propagated through the network, weighted by $\beta$. At the end of each epoch, we track the development of our model on the validation set from the primary task.

\begin{equation}\label{eq:mtl_loss}
  loss = loss(\hat{y}, y) \times \beta
\end{equation}

For each dataset used as a primary task, we perform hyper-parameter search for each type of model, including the baselines. In \autoref{tab:mtl_exp_model_parameters} we see the best performing hyper-parameters for our experimental models. For all models, we see that they prefer a shared dimensionality of $256$ units, beyond this commonality which we set for our model, no other hyper-parameters are shared across all models. We do note however, that most models prefer $ReLU$ as their activation function over $tanh$. Similarly, most models prefer a batch size of $64$ over $32$. On the other hand, the number of epochs, the dropout rate, and the learning rate are variable across the models.
\zw{Insert insights about model hyper-parameters}.

\begin{landscape}
\begin{table}[]
\centering
\begin{tabular}{lllll|llll|llll}
                    & \multicolumn{4}{c|}{Davidson}   & \multicolumn{4}{c|}{Wulczyn}  & \multicolumn{4}{c}{Waseem}          \\ \hline
                    & \multicolumn{2}{c}{BPE} & \multicolumn{2}{c|}{LIWC} &  \multicolumn{2}{c}{BPE} & \multicolumn{2}{c|}{LIWC} &  \multicolumn{2}{c}{BPE} &  \multicolumn{2}{c}{LIWC} \\
                    & MLP       & LSTM & MLP      & LSTM & MLP      & LSTM & MLP      & LSTM & MLP      & LSTM     & MLP      & LSTM \\
Shared Dimension    & $256$     &      & $256$    &      & $256$    &      & $256$    &      & $256$    & $256$    & $256$    &      \\
Hidden Dimension    & -         &      & -        &      & -        &      & -        &      & -        & $300$    & -        &      \\
Embedding Dimension & $100$     &      & $100$    &      & $100$    &      & $100$    &      & $300$    & $100$    & $100$    &      \\
Activation Function & $ReLU$    &      & $ReLU$   &      & $ReLU$   &      & $ReLU$   &      & $Tanh$   & $ReLU$   & $ReLU$   &      \\
Batch Size          & $64$      &      & $64$     &      & $32$     &      & $32$     &      & $64$     & $64$     & $64$     &      \\
Learning Rate       & $0.001$   &      & $0.001$  &      & $0.01$   &      & $0.01$   &      & $0.001$  & $0.0001$ & $0.001$  &      \\
\# Epochs           & $200$     &      & $200$    &      & $100$    &      & $100$    &      & $100$    & $200$    & $200$    &      \\
Dropout             & $0.3$     &      & $0.2$    &      & $0.1$    &      & $0.2$    &      & $0.3$    & $0.1$    & $0.1$    &      \\
Validation Score    & $0.5249$  &      & $0.5410$ &      & $0.6885$ &      & $0.5304$ &      & $0.4054$ & $0.3336$ & $0.2995$ &
\end{tabular}
\caption{Best parameter setting for each experimental model type.}
\label{tab:mtl_exp_model_parameters}
\end{table}
\end{landscape}

\zw{Identify which models will be experimental models once good results come out.}

\zw{Describe the models and experimental settings}
\zw{CODING: Do Ablation test}

\section{Results}

\zw{INSERT: Baseline scores}
\zw{Add results and start by rough analysis of just numbers}
\zw{Add baseline experiment results}
\zw{Show which models contribute most to ensemble models}
\zw{Add plots for development of loss over each epoch}
\zw{Add plots for F1 score during the evaluation set}
\zw{Look at predictions in detail, try to identify where they still fail}
\zw{Do logistic regression and find out if there are things that the models might overfit on}

\section{Conclusions}

