% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

\chapter[Tasks that Matter: Multi-task Learning for Abusive Language Detection]{Tasks that Matter: Multi-task Learning for Abusive Language Detection\footnotemark{}}\label{chap:mtl}
\footnotetext{This chapter contains elements of an ongoing collaboration with Joachim Bingel, Hero I/S. All contents of the chapter, are original work produced for this dissertation. The shared elements between the project and this chapter are the machine learning model designs.}

\begin{quote}
  ``So is hate speech detection kind of like sentiment analysis++'' -- ACL 2016 Conference Attendee\footnote{Check with AJ that I can attribute.}
\end{quote}

One of the frequently made assumptions is that hate speech detection, and in general abusive language detection, shares many similarities to other tasks that also take on the challenge of identifying and predicting subjective human experiences such as sentiment analysis, sarcasm detection, and emotion detection.
Indeed, each of these tasks share the characteristic that the identification of each of these on the basis of text is a task of linguistic pragmatics and that the interpretation of a given statement will vary on the basis of parties involved in the communicative act.
While they share this unifying characteristic, hate and humour, for instance occupy different but sometimes overlapping processes as highlighted by the NGO partners interviewed by \citet{Rottger:2021}.

Similarities between distinct related tasks pose several interesting questions.
First, is it best to create multiple annotations, either through re-annotating previously published data or creating an entirely dataset, such that each task is addressed in all of the data or should one try to develop modelling architectures that are overlapping?
Second, how much data from each task is necessary to annotate, in the case of creating multiple annotations for each document; or, if the task is approached in terms of developing modelling architectures, how much of the data from each task should be used in optimising the model, or alternatively, how should the data from each task be weighted to gain the largest modelling improvements?

In this chapter, I approach the question of overlapping data through a question of developing a modelling approach that aims to use potential overlaps between each task.
As each task may be related only in terms of the abstraction required to understand the meaning of a given text, creating mappings between different classes is a complex task that in some cases is not possible.
Moreover, data for each task may be collected from different sources, at different times, from different populations that use different vocabularies resulting in models that may learn spurious patterns in the data that are not trivial to identify and address.
Thus modelling can be approached in two distinct manners. Either all documents are collapsed into a single dataset without creating maps between the different classes or each task remains a distinct task and model architectures such as Multi-Task Learning (MTL) and Ensemble methods are explored.
Here I take the latter approach, developing a MTL model that considers each task simultaneously in tandem and distinctly from all other tasks (see \cref{sub:mtl} for more details on how MTL functions).
I select a MTL modelling approach over an ensemble approach as optimising an ensemble requires optimising $n$ entirely distinct models, one for each task, and a final model that considers the outputs of each model, MTL models on the other hand can be optimised such that a models is optimised to perform on its primary task, treating all auxiliary tasks as secondary.
Moreover, as I use a hard-parameter sharing design for my MTL models, an additional benefit is that all auxiliary tasks act as regularisers, even if they are not directly beneficial to the primary task.

Through the use of of MTL models, I find that non-abusive tasks as auxiliary can be beneficial to detecting all forms of abuse examined.
In line with the results in \cref{chap:liwc}, there is a difference in how helpful different abusive language datasets are for each other.
However, in spite of benefits from using MTL over some single-task baselines, some baseline models still out-perform some of the MTL models.
% TODO Add here about how dataset combinations contribute.

% WH - benefits from all aux tasks, abusive and non-abusive
% Wulczyn - benefits from all non-abusive tasks; benefits from WH, Davidson, but not Waseem
% Davidson - Benefits from all non-abusive and abusive tasks;

Thus, in this chapter I ask the following research questions to gain a deeper understanding on how different tasks that may share relations to abuse impact model performance:

\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}
        \item{\textit{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}
        \item{\textit{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}
    \end{enumerate}
\end{minipage}

\section{Previous work}

MTL has previously been applied for a number of tasks in NLP, including language specific tasks such as multi-word expression identification \cite{Bingel-Bjerva:2018}, machine translation \cite{Dong:2015}, and sequence labelling \cite{Rei:2017}.
Further, MTL has also been used in tasks that produce social outcomes such as predicting mental health conditions \cite{Benton:2017}, hate speech detection \cite{Waseem:2018,Rajamanickam:2020,Farha:2020,Djandji:2019}, and rumour verification \cite{Kochkina:2018}.

\subsection{Modelling}
For hate speech detection, and abusive language detection in general, MTL has been applied to English \citep{Waseem:2018,Rajamanickam:2020} and Arabic \citep{Farha:2020,Djandji:2019}.
Considering that I use datasets that are entirely in English, I only consider the previous work for hate speech detection using MTL for English language data.

\citet{Waseem:2018} show that the cultural gaps that exist between different datasets, as a result of their collection strategies and annotation procedures, could be addressed.
Using a hard parameter sharing strategy, they develop a MTL model that uses two different tasks for optimisation.
In their model, sampling of the batches is chosen at random, with one of the tasks set as the main task and a manual mapping between the distinct classes is performed.
The machine learning model chosen by \citet{Waseem:2018} for their MTL experiment is a back-propagated MLP with a tanh activation function and Adam as their optimisation function.
For the input representations \citet{Waseem:2018} experiment with a Bag-of-Words model that uses the $5,000$ most frequent terms and model that uses Byte-Pair encoded input data.
Similarly to \citet{Waseem:2018}, \citet{Rajamanickam:2020} show that using hard parameter sharing strategy for MTL with an auxiliary task can aid in the detection of hate speech.
Rather than using a different task coded for abuse as \citet{Waseem:2018} do, \citet{Rajamanickam:2020} instead ask whether jointly learning which emotions are invoked in a given task can aid in the detection of abuse.
Moreover, the architectures of the two different approaches diverge from one another.
\citet{Rajamanickam:2020} implement a double encoder model in which the primary and auxiliary share an encoder and each have a stacked Bi-directional Long-Short Memory Network that generate a second encoding.
The primary and auxiliary task models developed by \citet{Rajamanickam:2020} diverge at this point. The auxiliary task model directly passes the second encoding to a Bi-directional Long-Short Memory Network, the output of which is subject to an attention layer and finally passed through to a linear layer and subject to an activation function before producing the prediction of the model.
The primary task model sums the encodings obtained from the stacked Bi-directional Long-Short Term Memory networks for the primary and auxiliary task, passing this on to a Bi-directional Long Short Term Memory network.
The resulting representation is then passed through an attention layer and passed through an output layer generating the prediction.
A key difference between the hard parameter sharing models of \citet{Waseem:2018} and \citet{Rajamanickam:2020} is that the latter use a weighting parameter to distinguish between the primary and auxiliary task.
\citet{Waseem:2018} only distinguish between the primary and auxiliary tasks through the validation set.
The reason for this discrepancy is that \citet{Waseem:2018} seek to use multi-task learning to optimise a model that is capable of dealing with cross-cultural data, that is a model that is able to perform on both tasks.
\citet{Rajamanickam:2020} on the other hand seek to improve classification performance on the primary task, thus considering any performance gains on the auxiliary a side-benefit.
This discrepancy is the result of a natural prioritisation question, as the goal of \citet{Rajamanickam:2020} is to improve classification performance for abuse on a single dataset whereas \citet{Waseem:2018} seek to identify a classifier that can generalise beyond beyond the single dataset.

The work described in this chapter follows \citet{Rajamanickam:2020} in their focus on improving classification performances on the primary task.
For this reason, I choose auxiliary tasks (see \cref{sec:mtl_data} for an overview of the auxiliary datasets) that have been hypothesised as relevant to the question of detecting different forms of online abuse.

\subsection{Learning Tasks}\label{sec:mtl_tasks}

Multi-task learning, as the name of the framework implies, requires distinct tasks for learning, where each unique auxiliary task asks how learning representations from that task influences model performance on the primary task.
We saw in \cref{chap:liwc}, the optimised for each abusive dataset has different applications onto other datasets in the case of binary classification. Therefore, I choose to use three different tasks for abusive language as main tasks.
In contrast to the method in \cref{chap:liwc}, I do not binarise, or otherwise modify the classes in from those proposed by the authors of the datasets.
This provides for the more challenging tasks of predicting the type of abuse in addition to whether content is abusive or not.
A further consequence of not binarising the label sets for the main tasks is that the classes don't directly map onto other datasets.
This means that I preclude considerations of generalisability onto other datasets for abuse without further reduction of the predicted labels into a binarised label space.
Here I provide brief descriptions of the different datasets and the rationale for their inclusion, for more detail please refer to \cref{sec:datasets} and \cref{sub:liwc_datasets}.

\subsubsection{Main Task Datasets}
For the main tasks, I choose to use the \textit{Toxicity} dataset \citep{Wulczyn:2017}, the \textit{Offence} dataset \citep{Davidson:2017}, and the \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016}. I choose these three datasets in part due to their size and in part due to their examining of three different aspects of abuse.
Through this choice, I aim to identify which auxiliary tasks can improve performance for each type of abuse.
Each main task dataset is also used as an auxiliary task when it is not the used as a main task.

\paragraph{Hate Speech}
The \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} as proposed consists of $3,383$ comments labelled as sexist, $1,972$ labelled as racist and $11,559$ labelled as neither sexist or racist.
This dataset was proposed as a first step towards modelling racialised and gendered hate speech.
I use this dataset to show that the MTL framework can be used to distinguish between different targets of hate, as this dataset seeks to identify different forms of hate speech.
Beyond using this dataset to show the ability of MTL models to learn how to distinguish different forms of hate speech, this dataset also provides the largest source of hate speech, which otherwise is vanishingly small in other other main task datasets.

\paragraph{Offence}
The \textit{Offence} dataset \citep{Davidson:2017} was proposed to distinguish `offensive' content from `hateful' and content that is neither `hateful' or `offensive'.
In the class distribution proposed by \citet{Davidson:2017}, the `offensive' class occupies the vast majority of the dataset, with $19,190$ documents labelled into the class, followed by the negative class which consists of $4,163$ documents, and finally the `hateful' class which contains only $1,430$ documents.
As such, the class distribution for this dataset varies strongly from the \textit{Hate Speech} and the \textit{Toxicity} dataset, with the majority class being one of the two positive classes.
In using this dataset for the main task, I show that MTL models can provide a viable modelling approach in spite of a significantly different class distribution.

\paragraph{Toxicity}
The \textit{Toxicity} dataset \citep{Wulczyn:2017} provides a special case.
For one, it is the largest dataset consisting of $159,686$ labelled comments split into a training set of $95,692$ comments, a validation set of $32,128$ comments, and an evaluation set of $31,866$ comments.
In total, this dataset has more than $100,000$ more comments than either the \textit{Toxicity} or \textit{Hate Speech} datasets.
Second, the dataset proposes a binary classification of `toxic' and `not toxic'.
Thus, the results from the MTL models optimised for this dataset can be directly compared with the results obtained in \cref{chap:liwc}, unlike the models where the main task is \textit{Offence} or \textit{Hate Speech}.
Thus, I use this dataset to anchor the performances of the MTL models within the context of the preceding chapter and to show the impact of using a large scale dataset for abuse for MTL modelling.

\subsubsection{Auxiliary Task Datasets}

I choose the auxiliary task datasets for two different purposes: 1) to investigate the impact of using other datasets for abusive language as auxiliary tasks and 2) to examine how datasets that are labelled for other tasks can influence modelling for abuse.
To answer the first question, I use the three main task datasets in turn as auxiliary datasets when they are not serving as the main task. Moreover, to address the issue of the poor representation of content labelled within as hateful, I also use the \textit{Hate Expert} dataset.
Addressing the second motivation, I use a dataset labelled for sarcasm \citep{Oraby_sarcasm:2016}, a dataset labelled for the moral sentiment invoked by the text \citep{Hoover:2019}, and finally a dataset where documents are labelled for whether arguments are primarily based in emotion or in facts \citep{Oraby_factfeel:2015}.
With the exception of the \textit{Moral Sentiments} dataset, all auxiliary task datasets contain between $~5,000$ and $~16,000$ labelled documents, while the \textit{Moral Sentiments} consists of $~35,000$ labelled documents.
This spread of sizes of auxiliary task datasets allows for considering how auxiliary task dataset size impact the main task.

\paragraph{Sarcasm}
Previous work on hate speech detection \citep{Rottger:2021} has identified that sarcasm and irony can be contributing factors to misclassification from machine learning models as they take literally things that are communicated to be understood figuratively.
In efforts to better understand dialogue in online debate forums, \citet{Oraby_sarcasm:2016} develop a balanced dataset of $6520$ comments labelled for the occurrence of sarcasm.
Through this auxiliary task, MTL models learn representations of how sarcasm is constituted within \textit{Sarcasm} dataset, in addition to the other auxiliary tasks, and the main task.
Finally, through the use of this dataset, I explore how learning representations for sarcasm detection influences prediction of each operationalisation of detecting abuse.

\paragraph{Argument Basis}
Previous work on hate speech detection have suggested that many users who utter hate speech do so infrequently, suggesting that discriminatory speech may be produced in moments of carelessness and high emotionality \citep{Waseem:2016}. 
Moreover, as is apparent from the motivations used by \citet{Garcia:2019} for using StormFront as a data source, white supremacists may seek to mask their discrimination behind the use and distortions of fact.
Thus, whether hate is produced in the spur of the moment or is a part of a longer pattern, the basis upon which the argument is made, whether it is fact-based or based on emotion, may provide useful signals for learning to predict hate speech and abuse.
To model the hypothesis that high emotionality may influence the production of abuse, I include the \textit{Argument Basis} dataset \citep{Oraby_factfeel:2015}.
This dataset was developed using the same underlying data source as the \textit{Sarcasm} dataset, however rather than annotating the dataset for the occurrence of sarcasm, \citet{Oraby_factfeel:2015} annotate $5,848$ comments as being based in either fact or emotion.
The dataset is slightly imbalanced with $59\%$ of the dataset labelled as primarily fact-based and $41\%$ labelled as primarily based in feelings.
MTL models for abuse can take advantage of this dataset by learning a joint representation of the basis of an argument along with the main task in question.
Thus, this auxiliary task can provide insight into the question of whether learning such a joint representation is beneficial to detecting abuse and implicitly provide another signal into the feasibility of more deeply considering the emotional and mental state of the author when writing, e.g. through the use of LIWC in \cref{chap:liwc}.

\paragraph{Moral Sentiments}
The \textit{Moral Sentiments} dataset is annotated for the vices and virtues represented along five different `factors': \texttt{Loyalty/betrayal}, \texttt{care/harm},\\ \texttt{fairness/cheating}, \texttt{authority/subversion} and \texttt{purity/degradation} \citep{Hoover:2019}.
The authors of the dataset suggest that these five factors are likely to be represented in data that contains abuse, through their use of a subset of the \textit{Offence} dataset for annotation.
To explore this further, I use this dataset as an auxiliary task.
While the impacts of moral sentiments on the \textit{Offence} dataset are likely beneficial, using this auxiliary task on other datasets allows for examining whether representing moral sentiments has a positive impact on other datasets labelled for abuse.

\paragraph{Hate Expert}
Finally, as learned from \cref{chap:liwc}, some datasets for abusive language appear to be more closely related to others.
For this reason, I include the \textit{Hate Expert} dataset as an auxiliary task dataset to verify this finding and to provide a dataset to help address the poor representation of hate speech in the classes.
In \cref{chap:liwc} I binarise this dataset, here on the other hand I retain all $4$ classes in the dataset. 
In the expert annotated data, the four classes have a highly imbalanced distribution, the largest class being the negative class consuming $84\%$ of the data, while the second largest class `sexism' consumes $13\%$ of the data, the `racist' class consuming $1.4\%$ of the data and the final class, `both racist and sexist' contributing with $0.7\%$ of the data.

Focusing our attention on the smallest class for a moment, $0.7\%$ of $6,909$ documents means less than $50$ documents are labelled for the minority class, and given that I create stratified splits for the training ($80\%$), validation ($10\%$), and evaluation ($10\%$) sets, less than $40$ documents remain in the training set.
Thus there is not enough data for a machine learning model to learn patterns of abuse in the intersection between racist and sexist speech.
However, I choose to keep this data in the dataset to provide more instance of hate speech and to complicate, albeit only slightly, the question of what constitutes hate for the machine learning systems that use this dataset in the optimisation process.

\section{Modelling}\label{sec:mtl_modelling}
For the experiments conducted, I only use one form of tokens to allow for an examination of the impact of the auxiliary tasks rather than the impact of tokenisation.
I choose to represent all documents as their Byte-Pair encoded representations as these minimise the number of out-of-vocabulary tokens as this representation retains competitive performances in \cref{chap:liwc}.
To this end, I pre-process all documents using a $200$ dimensional Byte-Pair Embedding \citep{Heinzerling:2018}.
The pre-processing here follows the same method as in \cref{chap:liwc}, that is each document is lower-cased, all hyper-links are replaced with a `<URL>' token, all usernames are replaced with a `<USER>' token, and all hashtags are replaced with a `<HASHTAG>' token.
Then each document is passed through the Byte-Pair Embeddings to produce the Byte-Pair encoded representations, that is their sub-word units.

I develop three different types of baseline models: a linear single-task model where the model is optimised and evaluated on the same task, a non-linear single-task model, and a linear ensemble model where a model is optimised on the basis of outputs from the auxiliary task models.
In terms of experimental models, I follow \citet{Waseem:2018} in designing a multi-task Multi-Layered Perceptron implemented in PyTorch \citep{Paszke:2019}. I select an MLP over more complex neural networks architectures like Convolutional Neural Networks and Long-Short Term Memory networks due to the speed with which MLPs are optimised along with their general performance in \cref{chap:liwc}.

I perform parameter and hyper-parameter optimisation for the linear and non-linear models, respectively.
For the non-linear models I use the Weights and Biases library \citep{Wandb} to perform Bayesian Hyper-Parameter Optimisation.
For the linear models, I use grid-search as implemented in the Scikit-Learn library \citep{Pedregosa:2011}.

Once models have been optimised, they are each evaluated on the validation data and the evaluation data.
For non-linear models, the performance on the validation data guides the decision on which parameter configurations are chosen for analysis while for linear models, cross-validation is applied during the grid-search which aids in determining which parameter configuration performs best.

\subsection{Baseline Models}\label{sub:mtl_baselines}
I develop three baseline models: a linear single-task model, a neural network single-task model, and a linear ensemble model.
I choose to use a linear single-task model as a baseline as these often provide a strong baseline against neural network approaches for abuse detection \citep{Cite someone who says that here} while also being fast and efficient to optimise.
The non-linear neural network baseline is chosen as a counter-point to the linear baseline, using a MLP to more directly be able to consider the influence of the multi-task architecture for the experimental models.
Finally, I choose to an ensemble classifier that is optimised on the outputs of linear single-task models for each of the auxiliary task models as an ensemble, similarly to a multi-task model, can take advantage of learned representations for each auxiliary task for producing a prediction for the main task.

\paragraph{Single-Task Baselines}
Following prior work \citep{Waseem:2016,Davidson:2017}, I optimise all single-task models using a Support Vector Machine with a linear kernel (see \cref{sec:model_background} for more details on SVMs).
All linear single-task models are optimised on unigram counts of the Byte-Pair encoded tokens and are subject to parameter-optimisation of the regularisation type ($L1$ and $L2$) and the strength of regularisation (using values $0.1, 0.2, \ldots, 1.0$).

\paragraph{MLP Single-Task Baseline}
I develop a MLP as a non-linear counter-part to the linear single-task models to provide a baseline of the performance of a neural network approach that only relies on the Byte-Pair unigrams for the main task to optimise for the main task.
To ensure that the baseline model is also tuned for optimal performance, I perform a hyper-parameter sweep  over the batch size ($\{16, 32, 64\}$), the dropout value ($[0.0, 0.5]$), the dimensionality of the embedding layers ($\{64, 100, 200, 300\}$), the number of epochs for optimisation ($\{50, 100, 200\}$), the dimensionality of the hidden layers ($\{64, 100, 200, 300\}$), the learning rate ($[1^{-5}, 1.0]$), the Non-linearity to apply ($\{Tanh, ReLU\}$), and the optimiser function ($\{SGD, ASGD, Adam, AdamW\}$).
I conduct at least $50$ independent trials of distinct hyper parameter settings which identify the best hyper parameter configuration.

\paragraph{Ensemble Baseline}
The ensemble baselines require a different optimisation scheme that relies on a classifier that is optimised for each auxiliary task and an ensemble classifier that relies on the outputs of the auxiliary task classifiers, by virtue of the nature of ensemble classifiers.
For this reason, I first optimise a linear SVM for each auxiliary task and perform a grid search over the type of regulariser ({$L1$, $L2$}) and the strength of the regularisation (${0.1, 0.2,\ldots, 1.0}$) (see \cref{tab:aux_ensemble_params} for the parameter settings for each auxiliary task).
Once all auxiliary task classifiers have been optimised, I optimise a Logistic Regression model on the outputs of the auxiliary task classifiers on the main task training data, similarly subject to a grid-search over the same parameter values as the auxiliary task models
.
During this opitimisation procedure, the ensemble is provided with the training data for the main task, which is vectorised to the vocabulary of each auxiliary task and a prediction is obtained for each task.
For each document, predictions of all auxiliary task classifiers are vectorised and a classifier is optimised on the auxiliary task predictions.
While this method allows for every datasets to be passed through the model, by design this method limits the vocabulary to that which exists in the training datasets for the auxiliary tasks, rather than the main task.
This risk however is mitigated by the use of sub-words obtained by pre-processing all data through the Byte-Pair embeddings.

\subsection{Experimental Models}
For the experimental models, I follow \citet{Waseem:2018} in using a Multi-Layered Perceptron model.
The Multi-task MLP architecture that I design (see \cref{fig:mlp_mtl} for a depiction of the model architecture) consists of an input embedding layer which is unique to each task, a shared linear hidden layer, followed by another linear hidden layer that is specific to each task, a linear output layer for each task, and finally the \texttt{softmax} is computed on the model representation.
I also include a dropout layer and a non-linear activation function, where I treat the decision of activation function as an hyper-parameter optimising between the choice of \texttt{ReLU} and \texttt{Tanh} activation functions.

My architecture of the Multi-task MLP deviates from the architecture proposed by \citet{Waseem:2018} in two ways: the choice of input layer and the choice of activation function.
Where I use an embedding layer as the input layer for each task, \citet{Waseem:2018} use a onehot encoded input layer and they use a \texttt{Tanh} activation function for all of their experiments.
Following the experimental approach in \cref{chap:liwc}, I keep the embedding layer randomly initialised rather than using a pre-trained embedding layer.
The motivation for training the embedding layer, even with sparse data, is that pre-trained embeddings have been shown to harbour significant social biases against marginalised communities, a behaviour that is directly oppositional to the aims of abuse detection.

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{mlp_mtl_embedding.jpg}
  \caption{Multi-task Learning MLP.}
  \label{fig:mlp_mtl}
\end{figure}

The optimisation procedure for the Multi-task MLP deviates significantly from the optimisation procedures associated with the baseline models.
For the Multi-task MLP, I optimise my models by giving all tasks an equal weight but distinguish between the main task and the auxiliary tasks by the probability with which a batch from task is chosen.
A task is chosen each time a batch is to be selected, where the primary task is chosen with a probability of $0.6$ when there are two or more auxiliary tasks and $0.7$ when there is only one auxiliary task.
As each task is chosen probabilistically, it is necessary for the probabilities to sum to $1.0$, thus the weight of each auxiliary task is $\frac{1.0-P(M)}{N}$, where $N$ is the total number of auxiliary tasks and $P(M)$ is the probability of the main task being chosen.
Given that I choose the task to be optimised probabilistically, I do not weight the loss as in \citet{Rajamanickam:2020}.
Once a task has been chosen, a batch is selected from the data associated with the task and is passed through the model and the loss on the batch is computed and back-propagated through the network, a process which is repeated for a number of epochs, where the exact number of epochs is a hyper-parameter that I tune.
For single-task models, it is common to iterate over the entire dataset, obtaining a batch count given the size of the dataset and the batch size.
MTL models however are optimised for a number of datasets, including auxiliary task datasets where obtaining a high performance on the auxiliary task may not be of concern, rather learning inductive biases from the data are.
For this reason, I limit the number of batches that are selected in each epoch, setting a global value of $300$ batches per epoch.
Through the use of the probabilities with batches are chosen from each task in conjunction with the number of epochs and the batches being shuffled between each epoch, I ensure that my models gain a representative perspective of each dataset and their labelled data.
These representations of the datasets afford the models the ability to jointly learn representations based on the auxiliary tasks and the primary task.

For my hyper-parameter exploration, I explore the hyper-parameters listed above, that is the number of epochs ($\{50, 100, 200\}$) and the activation function ($\{Tanh, ReLU\}$).
I also perform a hyper-parameter optimisation of the choice of optimisation algorithm ($\{Adam, AdamW,$\\$SGD,ASGD\}$); the dimensionality of the shared layer (${64, 128, 256}$); the learning rate ($[1^{-5}, 1.0]$); the dimensionality of the task-specific hidden layers (${64, 100, 200, 300}$); the dimensionality of the task-specific input layers ($\{64, 100, 200, 300\}$); the value of dropout $[0.0, 0.5]$; and lastly the batch size ($\{16, 32, 64\}$).
Note, that the batch size can have an influence over how much of each dataset is exposed to the model at training time as the number of batches selected per epoch does not scale with the variation in the batch size.

\subsection{Auxiliary Task Configurations}\label{sub:aux_task_selection}
In order to select the auxiliary tasks and their combinations that contribute most towards the performances of the primary, I add auxiliary tasks as they prove useful to the main task in terms of performance boosts.
To perform this selection, I design three different scenarios of auxiliary task configurations:

\begin{enumerate}
  \item Auxiliary tasks consist only of abusive language detection tasks,
  \item auxiliary tasks consist only of non-abusive language detection tasks, and
  \item auxiliary tasks are a combination of abusive language detection tasks and tasks that are not abusive language detection tasks.
\end{enumerate}

I initially experiment with only one auxiliary task and select those that either outperform all baseline models or obtain the highest performances, in the case where some baseline models outperform all experimental models with one auxiliary task.
I then construct experiments with all combinations of the selected auxiliary tasks.

\section{Results}
\zw{Maybe add some introductory words here?}

\subsection{Baseline Models}

In \cref{tab:linear_singletask_params,tab:mlp_singletask_params,tab:ensemble_params} I present the best identified hyper-parameters for each model type.
Focusing on the regulariser, all linear models prefer an $L2$ regulariser, likely because it redistributes the weights of equally important features rather than zeroing any of them out.
Moreover, as observed in \cref{tab:linear_singletask_params} all models prefer a low regularisation strength when a linear single-task classifier is optimised.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cc}
                                            & Regulariser & Regularisation Strength \\\hline
    \textit{Offence} Linear Single task     & L2          & 0.2                     \\
    \textit{Hate Speech} Linear Single task & L2          & 0.1                     \\
    \textit{Toxicity} Linear Single task    & L2          & 0.1
  \end{tabular}
  \caption{Best model parameters for linear single-task models.}
  \label{tab:linear_singletask_params}
\end{table}

For the ensemble classifier a different picture emerges (see \cref{tab:ensemble_params}.
As this model is optimised on a very sparse feature set that consists only of the predictions of the auxiliary task classifiers, it is no surprise that an $L2$ regulariser is preferred.
Moreover, there are indications of a correlation between the dataset size and the strength of the regularisation, with the smallest dataset requiring the greatest regularisation strength ($0.5$ for \textit{Hate Speech}) and the largest dataset requiring the lowest regularisation strength ($0.1$ for \textit{Toxicity}).

\begin{table}[]
  \centering
  \begin{tabular}{l|ccc}
    Dataset                  & Vocabulary Size & Training Documents & \# Classes\\\hline
    \textit{Offence}         & 23263           & 19826              & 3\\
    \textit{Hate Speech}     & 19981           & 13525              & 3\\
    \textit{Toxicity}        & 95739           & 95692              & 2\\
    \textit{Hate Expert}     & 12005           & 5527               & 4\\
    \textit{Sarcasm}         & 21159           & 7508               & 2\\
    \textit{Argument Basis}  & 22275           & 8433               & 2\\
    \textit{Moral Sentiment} & 31779           & 27989              & 11
  \end{tabular}
  \caption{Vocabulary sizes for each of the datasets used.}
  \label{tab:aux_vocab_sizes}
\end{table}

This narrative however is complicated by the best parameters found in \cref{tab:ensemble_aux_params}.
Here, the smallest abusive language dataset requires the largest regularisation power while other, in line with the observation on \cref{tab:ensemble_params}.
However, classifiers optimised for the larger \textit{Offence} dataset require more regularisation strength than classifiers optimised the smaller \textit{Hate Speech}.
In tandem, these observations suggest that beyond the size of the dataset in terms of numbers, other factors may influence the strength of the regularisation.
One such potential factor may be the vocabulary size.
Observing the vocabulary sizes in \cref{tab:aux_vocab_sizes}, it appears that vocabulary sizes in conjunction with the dataset sizes may be causes for the regularisation strength for models optimised for the different datasets.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cc}
                                            & Regulariser & Regularisation Strength \\\hline
    \textit{Offence} Aux Classifier         & L2          & 0.2                     \\
    \textit{Hate Speech} Aux Classifier     & L2          & 0.1                     \\
    \textit{Toxicity} Aux Classifier        & L2          & 0.1                     \\
    \textit{Hate Expert} Aux Classifier     & L2          & 0.5                     \\
    \textit{Sarcasm} Aux Classifier         & L2          & 0.1                     \\
    \textit{Argument Basis} Aux Classifier  & L2          & 0.1                     \\
    \textit{Moral Sentiment} Aux Classifier & L2          & 0.1
  \end{tabular}
  \caption{Auxiliary task parameters for ensemble classifier.}
  \label{tab:ensemble_aux_params}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l|cc}
                                             & Regulariser & Regularisation Strength \\\hline
    \textit{Offence} Ensemble Classifier     & L2          & 0.2                     \\
    \textit{Hate Speech} Ensemble Classifier & L2          & 0.5                     \\
    \textit{Toxicity} Ensemble Classifier    & L2          & 0.1
  \end{tabular}
  \caption{Parameters for the ensemble classifiers.}
  \label{tab:ensemble_params}
\end{table}

Turning to the hyper-parameters for the non-linear baseline in \cref{tab:mlp_singletask_params}, the number of similar and shared values across models optimised for each dataset decreases to share only one parameter, the batch size.
The models optimised for the larger datasets, the \textit{Offence} and \textit{Toxicity} dataset also share a preference for using ReLU as their non-linearity.
Moreover, the baseline models optimised for these two datasets also prefer a higher learning rate compared to the model optimised for the smaller \textit{Hate Speech} dataset.

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccccccc}
                                          & Batch Size & Dropout & Embedding Dim & Epochs & Hidden Dim & Learning Rate & Non-linearity & Optimiser \\\hline
    \textit{Offence} MLP Single Task      & 64         & 0.318   & 300           & 200    & 100        & 0.003586      & ReLU          & SGD       \\
    \textit{Hate Speech} MLP Single Task  & 64         & 0.1458  & 300           & 100    & 100        & 0.0007246     & Tanh          & AdamW     \\
    \textit{Toxicity MLP} Single Task     & 64         & 0.1978  & 200           & 50     & 200        & 0.006056      & ReLU          & Adam
  \end{tabular}%
  }
  \caption{Best hyper parameters for non-linear single task model for each main task dataset.}
  \label{tab:mlp_singletask_params}
\end{table}

\subsubsection{Validation Data Performances}
\zw{Update baseline results}
Prior to an analysis of the baseline model performances on the evaluation set, I examine their performances on the validation set to gain an insight in the viability of the modelling approach and expected outcomes on the evaluation data.

Considering the results for all baseline models in \cref{tab:baseline_dev_wh,tab:baseline_dev_wulczyn,tab:baseline_dev_davidson} it is immediately clear that ensemble models provide a poor method for identifying each form of abuse.
Additionally, the linear SVM baseline models provide for good baselines to compare the experimental models with, as the SVM baselines tend to out-perform the non-linear baselines.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cccc}
               & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    Linear SVM & \textbf{0.8515} & \textbf{0.8239} & \textbf{0.7741} & \textbf{0.7962} \\
    Ensemble   & 0.8493          & 0.2123          & 0.2500          & 0.2296          \\
    MLP        & 0.8117          & 0.7117          & 0.7737          & 0.7378
  \end{tabular}
  \caption{Baseline validation scores on the \textit{Hate Speech} dataset.}
  \label{tab:baseline_dev_wh}
\end{table}

The exception to this pattern is provided by the MLP optimised for the \textit{Toxicity} dataset, where the MLP baseline outperforms the SVM baseline in terms of recall and F1-score.
For all datasets, the MLP classifiers show a drop in precision on the development set, suggesting that while they may be comparable in terms of recall, the MLP models tend to misclassify into the positive class at a greater rate than the negative classes.

Observing the results for the baseline models optimised for the \textit{Hate Speech} dataset in \cref{tab:baseline_dev_wh}, the largest drop in performance occurs for the positive classes when using the MLP.
The relatively smaller drop in accuracy is aligned with that the positive classes are minority classes, thus a performance drop in the positive classes has a small impact as the relative number of misclassification remains small.
For the ensemble, the negligible drop in accuracy, in comparison to precision and recall, suggests that although the performance on precision and recall are abysmal, the largest performance drop happens into the positive classes.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cccc}
               & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    Linear SVM & \textbf{0.8898} & \textbf{0.7224} & \textbf{0.7107} & \textbf{0.8661} \\
    Ensemble   & 0.7744          & 0.2581          & 0.3333          & 0.2910          \\
    MLP        & 0.8708          & 0.6581          & 0.7024          & 0.6773
  \end{tabular}
  \caption{Baseline validation scores on the \textit{Offence} dataset.}
  \label{tab:baseline_dev_davidson}
\end{table}

For the baseline models optimised for the \textit{Offence} dataset on the other hand, the accuracy score reveals a different performance drop.
In this dataset, the `offence' class is the majority class, thus the accuracy obtained provides insight into how well the models predict into that class.
The drop in precision is therefore likely to primarily occur in the other two classes in the dataset, one of which, the `hate' class, also being a positive class.
The model scores here tell a story of misclassifications primarily in the negative class and the positive `hate' class.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cccc}
                & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    Linear SVM  & \textbf{0.9570} & \textbf{0.8967} & 0.8411          & 0.7151          \\
    Ensemble    & 0.9045          & 0.4522          & 0.5000          & 0.4749          \\
    MLP         & 0.9480          & 0.8145          & \textbf{0.8671} & \textbf{0.8382}
  \end{tabular}
  \caption{Baseline validation scores on the \textit{Toxicity} dataset.}
  \label{tab:baseline_dev_wulczyn}
\end{table}

Finally, the models optimised for the \textit{Toxicity} dataset are the only ones where the non-linear baseline outperforms the linear SVM.
This dataset is developed for binary classification on an imbalanced dataset, where the minority class is the positive class.
Thus, the negligible drops in accuracy provide information into the ability of the models to predict into negative class.
The MLP baseline optimised for the \textit{Toxicity} classifier provides a stronger performance on the recall, meaning that it has an improved ability in correctly identifying the data that does not belong in the positive class compared to the linear SVM.

On the basis of the model performances on the validation sets, we can expect that the ensemble models will uniformly under-perform on the evaluation data while the MLP models provide a competitive, but lower, performance than the linear SVM baselines.
The primary performance drop for the MLP models is likely to be in their precision, that is their ability to classify into the positive classes.\vspace{5mm}

\subsubsection{Evaluation Data Performances}
Turning to the performances of the baseline models on the evaluation set, the model performances and the patterns remain mostly stable between the validation and the evaluation set across the datasets: the linear SVM out-perform all other models in most cases and the ensemble models mostly post poor classification performances.
In addition, the linear SVM models tend to perform best in terms of precision, with the MLP models obtaining a lower precision score.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll}
             & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    Linear   & \textbf{0.8440} & \textbf{0.8182} & 0.7671          & \textbf{0.7892} \\
    Ensemble & 0.6803          & 0.2268          & 0.3333          & 0.2699          \\
    MLP      & 0.8056          & 0.6686          & \textbf{0.7894} & 0.7132
  \end{tabular}
  \caption{Baseline model evaluation set performances on the \textit{Hate Speech} dataset.}
  \label{tab:baseline_test_wh}
\end{table}

Within the performances for each dataset there are some discrepancies between the performances on the validation and the evaluation sets.
Unlike for the validation set, the MLP models optimised for the \textit{Hate Speech} dataset obtain a higher recall score on the evaluation set (see \cref{tab:baseline_test_wh}) than the linear SVM.
This suggests that the MLP baseline is better suited for correctly identifying the negative class while the linear SVM is better suited for identifying the positive classes.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll}
             & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    Linear   & \textbf{0.8871} & \textbf{0.6997} & 0.6789          & \textbf{0.6850} \\
    Ensemble & 0.7713          & 0.4115          & 0.3696          & 0.3622          \\
    MLP      & 0.8790          & 0.5625          & \textbf{0.9163} & 0.5721
  \end{tabular}
  \caption{Baseline model evaluation set performances on the \textit{Offence} dataset.}
  \label{tab:baseline_test_davidson}
\end{table}

Further discrepancies are found for the models optimised for the \textit{Offence} dataset.
On the validation set, the linear SVM outperformed all other models across all metrics.
On the evaluation data however, the MLP baseline outperforms the linear SVM in terms of recall with a $0.2$ increase (see \cref{tab:baseline_test_davidson}).
This increase is obtained while there is a decrease in precision of $0.09$.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll}
                          & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    Linear                & \textbf{0.9582} & \textbf{0.9008} & 0.8450          & \textbf{0.8702} \\
    Ensemble\footnotemark & 0.9582          & 0.9008          & 0.8448          & 0.8701          \\
    MLP                   & 0.9397          & 0.6979          & \textbf{0.9359} & 0.7632
  \end{tabular}
  \caption{Baseline model evaluation set performances on the \textit{Toxicity} dataset.}
  \label{tab:baseline_test_wulczyn}
\end{table}
\footnotetext{The accuracy and precision for the ensemble classifiers have a lower performance, however rounding up to represent the performances by 4 decimal points creates the illusion of identical performance.}

The largest discrepancy between the validation set and evaluation set (see \cref{tab:baseline_test_wulczyn}) however is found in the ensemble baseline optimised for the \textit{Toxicity} dataset.
Here the ensemble classifier obtains a competitive classification performance across metrics to the linear SVM.
While the ensemble baseline breaks with the pattern observed on the validation set, the MLP baseline does not.
Similarly to its' performances on the validation data, the MLP is competitive with the linear SVM in terms of accuracy and posts poorer performance in terms of precision.
Meanwhile, the MLP baseline outperforms all other models in terms of recall however the poor performance in terms of precision results in a F1-score that is not competitive with the other baseline models.

\subsection{Experimental Model Performances}
As the experimental models are MLP models that have been adapted for Multi-Task Learning, the experimental models are expected to \textit{at least} out-perform the MLP baselines as they share a similar architecture.
The best hyper-parameter settings for each of the experimental settings for each main task are shown in \cref{tab:mtl_params_wh,tab:mtl_params_davidson,tab:mtl_params_wulczyn}.\footnote{Note that the `Aux Task Weight' column only contains a single value as each auxiliary task is given the same weight.}
In these tables, I show the best hyper-parameter settings for the models with one auxiliary task and each of the subsequent dataset configurations selected based.
The dataset configurations are chosen on the basis of their F1-score performance on the validation data where only the main task and a single auxiliary task (see further detail on dataset combination selection in \cref{sub:aux_task_selection}).
For all configurations of the datasets, I perform at least $50$ distinct trials of potential hyper-parameters to identify the best-performing hyper-parameters.
The selection of each hyper-parameter setting to trial is performed through Bayesian Hyper-Parameter Optimisation which selects a candidate set of parameters for trial given the results of past trials and an objective.
For these experiments, I set the objective to maximise the F1-score on the validation data as the models optimise for minimisation on the training loss.

\subsubsection{Validation Set Performances}

Observing the best hyper-parameters identified for each dataset in \cref{tab:mtl_params_wh,tab:mtl_params_davidson,tab:mtl_params_wulczyn} three salient attributes are immediately clear.
First, the vast majority of models prefer ReLU as a non-linearity.
Second, most models prefer the largest batch size that I experiment with, namely $64$.
Finally, Some version of the stochastic gradient descent optimisation algorithm is preferred by all but two models, the vast majority of models preferring averaged stochastic gradient descent.

Turning to the performances of the best performing models on the validation data.
The results presented in the first 6 rows of \cref{tab:mtl_dev_wh,tab:mtl_dev_davidson,tab:mtl_dev_wulczyn} guide decision for which auxiliary task datasets to experiment with.
The decision in which auxiliary task datasets to select is guided by two different objectives: 1) selecting auxiliary task data that outperform the MLP baselines in terms of Macro F1 score and 2) selecting auxiliary tasks that can aid in outperforming the best-performing baseline.
I separate these two objectives as not all MLP baselines outperform the linear SVMs (please refer back to \cref{tab:baseline_dev_wh,tab:baseline_dev_davidson,tab:baseline_dev_wulczyn} for the results on the validation sets).
Thus, the first objective provides insight into the utility of using MTL as a modelling approach can be observed through the ability of MTL-based models to out-perform their single-task counterparts.
However, in the interest of identifying auxiliary tasks that contribute towards improved models for different forms of abusive language detection, I set up the second objective of identifying auxiliary tasks that most positively contribute towards the prediction of abusive language.

Selecting the auxiliary tasks for the models optimised for the \textit{Hate Speech} dataset poses the challenge that none of the model configurations with a single auxiliary task outperform the linear SVM baseline on the development set though they all outperform the MLP baseline (see \cref{tab:baseline_dev_wh} for the baseline models and the first six rows of \cref{tab:mtl_dev_wh} for the MTL models).
For this reason, I choose the best performing MTL auxiliary task configurations to continue further experiments with.
I select the top four auxiliary tasks as they are have a maximal difference of $0.01$ from one another.
The auxiliary task datasets that I proceed to experiment with are the \textit{Hate Expert} dataset, the \textit{Offence} dataset, the \textit{Moral Sentiment} dataset and the \textit{Sarcasm} dataset.

Similarly to the pattern observed for the baseline models optimised for the \textit{Hate Speech} data, most of the MTL MLP dataset configurations outperform the baselines in terms of recall while struggling in the precision score.
\afterpage{%
\begin{landscape}
  \begin{table}[]
    \centering
    \resizebox{0.75\paperheight}{!}{%
    \begin{tabular}{l|ccccccccccc}
      Aux                                               & Batch Size & Main Task Weight & Aux Task Weights & Dropout & Embedding Dim & Epochs & Hidden Dims     & Learning Rate & Non-linearity & Optimiser & Shared Dim \\ \hline
      Hate Expert                                       & 64         & 0.7              & 0.3              & 0.3705  & 200           & 50     & 200,200         & 0.9084        & ReLU          & ASGD      & 128        \\
      Toxicity                                          & 64         & 0.7              & 0.3              & 0.02884 & 100           & 100    & 64,64           & 0.3873        & Tanh          & ASGD      & 128        \\
      Offence                                           & 32         & 0.7              & 0.3              & 0.4568  & 64            & 100    & 200,200         & 0.2846        & ReLU          & ASGD      & 256        \\
      Moral Sentiment                                   & 64         & 0.7              & 0.3              & 0.1954  & 300           & 100    & 100,100         & 0.06402       & ReLU          & ASGD      & 256        \\
      Sarcasm                                           & 64         & 0.7              & 0.3              & 0.4534  & 300           & 100    & 300,300         & 0.3894        & ReLU          & ASGD      & 128        \\
      Argument Basis                                    & 64         & 0.7              & 0.3              & 0.1556  & 100           & 50     & 100,100         & 0.4948        & ReLU          & ASGD      & 256        \\ \hline
      Hate Expert | Offence                             & 64         & 0.6              & 0.2              & 0.3703  & 200           & 50     & 200,200,200     & 0.9429        & ReLU          & ASGD      & 128        \\
      Hate Expert | Moral Sentiment                     & 64         & 0.6              & 0.2              & 0.311   & 100           & 50     & 300,300,300     & 0.6185        & ReLU          & ASGD      & 128        \\
      Offence | Moral Sentiment                         & 16         & 0.6              & 0.2              & 0.1408  & 300           & 100    & 64,64,64        & 0.1237        & Tanh          & SGD       & 128        \\
      Sarcasm | Hate Expert                             & 64         & 0.6              & 0.2              & 0.3054  & 100           & 50     & 64,64,64        & 0.06252       & ReLU          & SGD       & 64         \\
      Sarcasm | Offence                                 & 16         & 0.6              & 0.2              & 0.4576  & 300           & 200    & 100,100,100     & 0.2276        & ReLU          & ASGD      & 256        \\
      Sarcasm | Moral Sentiment                         & 64         & 0.6              & 0.2              & 0.3586  & 300           & 50     & 100,100,100     & 0.3822        & ReLU          & ASGD      & 64         \\
      Hate Expert  | Offence | Moral Sentiment          & 16         & 0.6              & 0.133333333      & 0.414   & 200           & 100    & 300,300,300,300 & 0.8435        & ReLU          & ASGD      & 256        \\
      Sarcasm | Hate Expert | Moral Sentiment           & 32         & 0.6              & 0.133333333      & 0.152   & 64            & 100    & 200,200,200,200 & 0.3459        & ReLU          & ASGD      & 256        \\
      Sarcasm | Offence | Moral Sentiment               & 64         & 0.6              & 0.133333333      & 0.05853 & 300           & 100    & 64,64,64,64     & 0.04528       & ReLU          & ASGD      & 64         \\
      Sarcasm | Hate Expert | Offence | Moral Sentiment & 16         & 0.6              & 0.1              & 0.143   & 64            & 100    & 64,64,64,64,64  & 0.2368        & ReLU          & ASGD      & 256
    \end{tabular}%
    }
    \caption{Hyper-parameters for best performing MTL models optimised on the \textit{Hate Speech} dataset.}
    \label{tab:mtl_params_wh}
  \end{table}
  \vfill
  \begin{table}[]
    \centering
    \resizebox{0.75\paperheight}{!}{%
    \begin{tabular}{l|ccccccccccc}
      Aux                                               & Batch Size & Main Task Weight & Aux Task Weights & Dropout & Embedding Dim & Epochs & Hidden Dims     & Learning Rate & Non-linearity & Optimiser & Shared Dim \\ \hline
      Hate Expert                                       & 32         & 0.7              & 0.3              & 0.06554 & 100           & 200    & 300,300         & 0.2921        & ReLU          & ASGD      & 256        \\
      Hate Speech                                       & 64         & 0.7              & 0.3              & 0.1785  & 100           & 50     & 100,100         & 0.281         & ReLU          & SGD       & 128        \\
      Toxicity                                          & 64         & 0.7              & 0.3              & 0.4405  & 300           & 50     & 200,200         & 0.3469        & ReLU          & ASGD      & 256        \\
      Sarcasm                                           & 32         & 0.7              & 0.3              & 0.1476  & 300           & 50     & 200,200         & 0.9616        & ReLU          & ASGD      & 128        \\
      Argument Basis                                    & 16         & 0.7              & 0.3              & 0.2952  & 300           & 100    & 64,64           & 0.4306        & ReLU          & ASGD      & 64         \\
      Moral Sentiment                                   & 32         & 0.7              & 0.3              & 0.1953  & 200           & 100    & 300,300         & 0.1415        & ReLU          & SGD       & 64         \\ \hline
      Hate Speech | Toxicity                            & 64         & 0.6              & 0.2              & 0.1663  & 100           & 50     & 100,100,100     & 0.3764        & ReLU          & SGD       & 256        \\
      Sarcasm | Hate Speech                             & 32         & 0.6              & 0.2              & 0.1497  & 100           & 100    & 300,300,300     & 0.2229        & ReLU          & ASGD      & 256        \\
      Sarcasm | Toxicity                                & 32         & 0.6              & 0.2              & 0.166   & 64            & 100    & 300,300,300     & 0.511         & ReLU          & ASGD      & 64         \\
      Argument Basis | Hate Speech                      & 64         & 0.6              & 0.2              & 0.0265  & 100           & 200    & 200,200,200     & 0.4188        & ReLU          & ASGD      & 256        \\
      Argument Basis | Toxicity                         & 64         & 0.6              & 0.2              & 0.3497  & 300           & 100    & 200,200,200     & 0.3466        & ReLU          & ASGD      & 128        \\
      Argument Basis | Sarcasm                          & 64         & 0.6              & 0.2              & 0.4527  & 200           & 100    & 64,64,64        & 0.509         & ReLU          & ASGD      & 256        \\
      Sarcasm | Toxicity | Hate Speech                  & 64         & 0.6              & 0.133333333      & 0.4113  & 300           & 100    & 200,200,200,200 & 0.1113        & ReLU          & ASGD      & 256        \\
      Argument Basis | Hate Speech | Toxicity           & 16         & 0.6              & 0.133333333      & 0.2439  & 200           & 200    & 100,100,100,100 & 0.8852        & ReLU          & ASGD      & 64         \\
      Argument Basis | Hate Speech | Sarcasm            & 64         & 0.6              & 0.133333333      & 0.3725  & 200           & 200    & 200,200,200,200 & 0.3176        & ReLU          & ASGD      & 64         \\
      Argument Basis | Toxicity | Sarcasm               & 64         & 0.6              & 0.133333333      & 0.259   & 64            & 100    & 300,300,300,300 & 0.6679        & ReLU          & ASGD      & 128        \\
      Argument Basis | Hate Speech | Toxicity | Sarcasm & 64         & 0.6              & 0.1              & 0.0103  & 64            & 100    & 64,64,64,64,64  & 0.4785        & ReLU          & ASGD      & 64
    \end{tabular}%
    }
    \caption{Hyper-parameters for best performing MTL models optimised on the \textit{Offence} dataset.}
    \label{tab:mtl_params_davidson}
  \end{table}
  \vfill  
  \begin{table}[]
    \centering
    \resizebox{0.75\paperheight}{!}{%
    \begin{tabular}{l|ccccccccccc}
      Aux                                                                & Batch Size & Main Task Weight & Aux Task Weights & Dropout & Embedding Dim & Epochs & Hidden Dims             & Learning Rate & Non-linearity & Optimiser & Shared Dim \\ \hline
      Waseem                                                             & 64         & 0.7              & 0.3              & 0.1166  & 300           & 200    & 64,64                   & 0.4055        & ReLU          & ASGD      & 256        \\
      Hate Speech                                                        & 64         & 0.7              & 0.3              & 0.3346  & 100           & 50     & 300,300                 & 0.003051      & ReLU          & AdamW     & 256        \\
      Offence                                                            & 64         & 0.7              & 0.3              & 0.03044 & 300           & 200    & 200,200                 & 0.3432        & ReLU          & SGD       & 128        \\
      Moral Sentiment                                                    & 64         & 0.7              & 0.3              & 0.3558  & 300           & 200    & 64,64                   & 0.7853        & ReLU          & ASGD      & 256        \\
      Sarcasm                                                            & 64         & 0.7              & 0.3              & 0.3711  & 200           & 200    & 100,100                 & 0.9143        & ReLU          & ASGD      & 256        \\
      Argument Basis                                                     & 64         & 0.7              & 0.3              & 0.3995  & 300           & 50     & 200,200                 & 0.5104        & ReLU          & SGD       & 128        \\ \hline
      Hate Speech | Offence                                              & 64         & 0.6              & 0.2              & 0.03108 & 300           & 200    & 100,100,100             & 0.2465        & ReLU          & SGD       & 256        \\
      Moral Sentiment | Hate Speech                                      & 64         & 0.6              & 0.2              & 0.3406  & 300           & 200    & 200,200,200             & 0.2291        & ReLU          & SGD       & 64         \\
      Moral Sentiment | Offence                                          & 64         & 0.6              & 0.2              & 0.08389 & 200           & 200    & 64,64,64                & 0.8606        & ReLU          & ASGD      & 128        \\
      Sarcasm | Moral Sentiment                                          & 64         & 0.6              & 0.2              & 0.2051  & 200           & 200    & 300,300,300             & 0.8602        & ReLU          & ASGD      & 64         \\
      Sarcasm | Hate Speech                                              & 64         & 0.6              & 0.2              & 0.2315  & 300           & 200    & 300,300,300             & 0.2219        & ReLU          & SGD       & 128        \\
      Sarcasm | Offence                                                  & 64         & 0.6              & 0.2              & 0.02304 & 100           & 200    & 64,64,64                & 0.804         & ReLU          & ASGD      & 256        \\
      Moral Sentiment | Hate Speech | Offence                            & 64         & 0.6              & 0.133333333      & 0.3018  & 200           & 200    & 300,300,300,300         & 0.9543        & ReLU          & ASGD      & 256        \\
      Sarcasm | Moral Sentiment | Hate Speech                            & 64         & 0.6              & 0.133333333      & 0.2762  & 200           & 100    & 300,300,300,300         & 0.4007        & ReLU          & SGD       & 128        \\
      Sarcasm | Moral Sentiment | Offence                                & 64         & 0.6              & 0.133333333      & 0.2939  & 64            & 200    & 200,200,200,200         & 0.8591        & ReLU          & ASGD      & 64         \\
      Sarcasm | Moral Sentiment | Hate Speech | Offence                  & 32         & 0.6              & 0.1              & 0.1936  & 300           & 50     & 100,100,100,100,100     & 0.004907      & ReLU          & AdamW     & 64         \\
      % Argument Basis | Moral Sentiment                                   & 64         & 0.6              & 0.2              & 0.01288 & 200           & 200    & 300,300,300             & 0.968         & ReLU          & ASGD      & 256        \\
      % Argument Basis | Sarcasm                                           & 64         & 0.6              & 0.2              & 0.061   & 200           & 200    & 300,300,300             & 0.384         & ReLU          & SGD       & 256        \\
      % Argument Basis | Hate Speech                                       & 64         & 0.6              & 0.2              & 0.09094 & 200           & 200    & 300,300,300             & 0.8456        & ReLU          & ASGD      & 128        \\
      % Argument Basis | Offence                                           & 64         & 0.6              & 0.2              & 0.2733  & 300           & 100    & 100,100,100             & 0.3757        & ReLU          & SGD       & 256        \\
      % Argument Basis | Moral Sentiment | Sarcasm                         & 64         & 0.6              & 0.133333333      & 0.03792 & 300           & 200    & 300,300,300,300         & 0.934         & ReLU          & ASGD      & 64         \\
      % Argument Basis | Moral Sentiment | Hate Speech                     & 64         & 0.6              & 0.133333333      & 0.05325 & 300           & 200    & 300,300,300,300         & 0.4349        & ReLU          & SGD       & 64         \\
      % Argument Basis | Moral Sentiment | Offence                         & 64         & 0.6              & 0.133333333      & 0.1082  & 300           & 200    & 100,100,100,100         & 0.3236        & ReLU          & SGD       & 128        \\
      % Argument Basis | Moral Sentiment | Sarcasm | Hate Speech           & 64         & 0.6              & 0.1              & 0.06104 & 100           & 200    & 300,300,300,300,300     & 0.565         & ReLU          & SGD       & 256        \\
      % Argument Basis | Moral Sentiment | Sarcasm | Offence               & 64         & 0.6              & 0.1              & 0.2149  & 300           & 200    & 100,100,100,100,100     & 0.6336        & ReLU          & SGD       & 128        \\
      % Argument Basis | Moral Sentiment | Sarcasm | Offence | Hate Speech & 64         & 0.6              & 0.08             & 0.2348  & 100           & 100    & 200,200,200,200,200,200 & 0.546         & ReLU          & SGD       & 256
    \end{tabular}%
    }
    \caption{Hyper-parameters for best performing MTL models optimised on the \textit{Toxicity} dataset.}
    \label{tab:mtl_params_wulczyn}
  \end{table}
\end{landscape}
} %

Although none of the auxiliary task datasets outperform all baselines, they all outperform the MLP baseline thus providing early indication that learning a representation that all auxiliary tasks that I consider provide some beneficial information to the main task of detecting abuse.

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                      & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    \textit{Hate Expert}                              & 0.8171          & 0.7463          & 0.7776          & \textit{0.7608} \\
    \textit{Offence}                                  & 0.8141          & \textbf{0.7518} & 0.7609          & \textit{0.7558} \\
    Toxicity                                          & 0.8082          & 0.7433          & 0.7562          & 0.7495          \\
    \textit{Moral Sentiment}                          & 0.8242          & 0.7462          & 0.7865          & \textit{0.7644} \\
    Argument Basis                                    & 0.8094          & 0.7374          & 0.7638          & 0.7494          \\
    \textit{Sarcasm}                                  & 0.8194          & 0.7259          & 0.7944          & \textit{0.7551} \\\hline
    Hate Expert | Moral Sentiment                     & 0.8212          & 0.7220          & 0.7996          & 0.7540          \\
    Offence | Moral Sentiment                         & 0.7957          & 0.7552          & 0.7408          & 0.7476          \\
    Hate Expert | Offence                             & 0.8200          & 0.7521          & 0.7781          & 0.7623          \\
    Sarcasm | Hate Expert                             & 0.8171          & 0.7068          & 0.7994          & 0.7431          \\
    Sarcasm | Offence                                 & 0.8141          & 0.7284          & 0.7731          & 0.7449          \\
    Sarcasm | Moral Sentiment                         & \textbf{0.8289} & 0.7425          & 0.8009          & \textbf{0.7664} \\
    Hate Expert | Offence | Moral Sentiment           & 0.8165          & 0.7212          & 0.7913          & 0.7497          \\
    Sarcasm | Hate Expert | Moral Sentiment           & 0.8259          & 0.7279          & \textbf{0.8051} & 0.7566          \\
    Sarcasm | Offence | Moral Sentiment               & 0.8194          & 0.7204          & 0.7916          & 0.7497          \\
    Sarcasm | Hate Expert | Offence | Moral Sentiment & 0.8200          & 0.7389          & 0.7834          & 0.7561
  \end{tabular}%
  }
  \caption{Experimental model validation scores on the \textit{Hate Speech} dataset. Configurations written in \textit{italic} signify the auxiliary tasks chosen for further exploration and \textbf{bolded} scores indicate the highest performances.}
  \label{tab:mtl_dev_wh}
\end{table}

For the MTL models optimised on the \textit{Offence} dataset, a similar issue of the linear SVM baseline outperforming the MTL configurations with one auxiliary task.
Here, I set the same $0.01$ cut off in difference from the best performing auxiliary task as a criteria for inclusion.
This results in the \textit{Hate Speech}, \textit{Toxicity} and the \textit{Sarcasm} datasets to be selected for further experiments.

\begin{table}[h]
  \centering
  %\resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                          & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    \textit{Hate Speech}                                  & 0.8857          & 0.7422          & 0.7076          & \textit{0.7232} \\
    \textit{Toxicity}                                     & 0.8962          & 0.7124          & 0.7562          & \textit{0.7286} \\
    Hate Expert                                           & 0.8882          & 0.7122          & 0.7042          & 0.7069          \\
    \textit{Sarcasm}                                      & 0.8845          & \textbf{0.7503} & 0.7149          & \textit{0.7312} \\
    \textit{Argument Basis}                               & 0.8987          & 0.7039          & 0.7489          & \textit{0.7164}          \\
    Moral Sentiment                                       & 0.8853          & 0.7237          & 0.7020          & 0.7105          \\\hline
    Hate Speech | Toxicity                                & 0.8914          & 0.7381          & 0.7251          & 0.7309          \\
    Sarcasm | Hate Speech                                 & \textbf{0.9023} & 0.7005          & \textbf{0.7619} & 0.7167          \\
    Sarcasm | Toxicity                                    & 0.8954          & 0.7367          & 0.7361          & \textbf{0.7349} \\
    Sarcasm | Toxicity | Hate Speech                      & 0.8979          & 0.7338          & 0.7449          & 0.7345          \\
    Argument Basis | Hate Speech                          & 0.8862          & 0.7018          & 0.7136          & 0.7069          \\
    Argument Basis | Toxicity                             & 0.8958          & 0.7076          & 0.7449          & 0.7218          \\
    Argument Basis | Sarcasm                              & 0.8906          & 0.7126          & 0.7194          & 0.7098          \\
    Argument Basis | Hate Speech | Toxicity               & 0.8765          & 0.7226          & 0.6992          & 0.7103          \\
    Argument Basis | Hate Speech | Sarcasm                & 0.8950          & 0.6952          & 0.7354          & 0.7069          \\
    Argument Basis | Toxicity | Sarcasm                   & 0.8991          & 0.7122          & 0.7481          & 0.7211          \\
    Argument Basis | Hate Speech | Toxicity | Sarcasm     & 0.8902          & 0.6977          & 0.7281          & 0.7079
  \end{tabular}
  %%
  %}
  \caption{Experimental model validation scores on the \textit{Offence} dataset. Configurations written in \textit{italic} signify the auxiliary tasks chosen for further exploration and \textbf{bolded} scores indicate the highest performances.}
  \label{tab:mtl_dev_davidson}
\end{table}

For the \textit{Offence} dataset, the auxiliary tasks that are most beneficial turn out to be two other datasets for abusive language and one for sarcasm detection, unlike the \textit{Hate Speech} dataset, where the two best performing non-abusive tasks obtain scores that are competitive with the two best abusive language detection tasks.

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                                       & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    \textit{Hate Speech}                                               & 0.9513          & 0.8326          & 0.8726          & \textit{0.8511} \\
    \textit{Offence}                                                   & 0.9490          & \textbf{0.8523} & 0.8524          & \textit{0.8524} \\
    Hate Expert                                                        & 0.9499          & 0.8001          & 0.8881          & 0.8370          \\
    \textit{Moral Sentiment}                                           & 0.9528          & 0.8165          & 0.8918          & \textit{0.8491} \\
    \textit{Sarcasm}                                                   & 0.9511          & 0.8130          & 0.8846          & \textit{0.8441} \\
    \textit{Argument Basis}                                            & 0.9534          & 0.8195          & 0.8932          & \textit{0.8515} \\\hline
    Moral Sentiment | Offence                                          & 0.9517          & 0.8158          & 0.8862          & 0.8465          \\
    Moral Sentiment | Hate Speech                                      & 0.9555          & 0.8167          & 0.9081          & 0.8551          \\
    Hate Speech | Offence                                              & 0.9504          & 0.8386          & 0.8652          & 0.8512          \\
    Sarcasm | Moral Sentiment                                          & 0.9533          & 0.8008          & 0.9093          & \textbf{0.8575} \\
    Sarcasm | Hate Speech                                              & \textbf{0.9558} & 0.8191          & 0.9079          & 0.8566          \\
    Sarcasm | Offence                                                  & 0.9522          & 0.7957          & 0.9068          & 0.8469          \\
    Argument Basis | Hate Speech                                       & 0.9521          & 0.8148          & 0.8889          & 0.8421          \\
    Argument Basis | Moral Sentiment                                   & 0.9538          & 0.8101          & 0.9038          & 0.8492          \\
    Argument Basis | Sarcasm                                           & 0.9535          & 0.8376          & 0.8808          & \textit{0.8575} \\
    Argument Basis | Offence                                           & 0.9542          & 0.8093          & 0.9066          & 0.8496          \\
    Moral Sentiment | Hate Speech | Offence                            & 0.9529          & 0.7956          & 0.9121          & 0.8419          \\
    Sarcasm | Moral Sentiment | Hate Speech                            & 0.9533          & 0.7938          & 0.917           & 0.8402          \\
    Sarcasm | Moral Sentiment | Offence                                & 0.9523          & 0.7861          & \textbf{0.9184} & 0.8368          \\
    Argument Basis | Moral Sentiment | Sarcasm                         & 0.9526          & 0.8214          & 0.8869          & 0.8503          \\
    Argument Basis | Moral Sentiment | Hate Speech                     & 0.9516          & 0.8469          & 0.8661          & 0.8562          \\
    Argument Basis | Moral Sentiment | Offence                         & 0.953           & 0.8248          & 0.8867          & 0.8523          \\
    Sarcasm | Moral Sentiment | Hate Speech | Offence                  & 0.9548          & 0.8117          & 0.9086          & 0.8519          \\
    Argument Basis | Moral Sentiment | Sarcasm | Hate Speech           & 0.9514          & 0.8315          & 0.8740          & 0.8511          \\
    Argument Basis | Moral Sentiment | Sarcasm | Offence               & 0.9517          & 0.8098          & 0.8905          & 0.8442          \\
    Argument Basis | Moral Sentiment | Sarcasm | Offence | Hate Speech & 0.9534          & 0.8039          & 0.9067          & 0.8459
  \end{tabular}%
  }
  \caption{Experimental model validation scores on the \textit{Toxicity} dataset. Configurations written in \textit{italic} signify the auxiliary tasks chosen for further exploration and \textbf{bolded} scores indicate the highest performances.}
  \label{tab:mtl_dev_wulczyn}
\end{table}

Finally, turning to the \textit{Toxicity} dataset, several of the MTL models using a single auxiliary task outperform all linear baselines.
In fact, all auxiliary tasks, with the exception of the \textit{Hate Expert} dataset outperform all baseline models on the validation set in terms of F1-score.
Thus, all auxiliary tasks, asides from the \textit{Hate Expert} task, are selected for further experimentation.\vspace{5mm}

\subsubsection{Evaluation Data Performances}
Turning to the performance on the test sets, the results presented in \cref{tab:mtl_test_wh,tab:mtl_test_davidson,tab:mtl_test_wulczyn} are the means of five runs with different random seeds and the standard deviation for each metric.

\paragraph{Hate Speech Main Task}
Considering first the results in \cref{tab:mtl_test_wh} for the MTL models optimised for the \textit{Hate Speech} task.
All but one dataset configurations yield an improved accuracy over the MLP baseline, though none outperform the SVM baseline.
The best auxiliary task configuration for accuracy is one that uses the \textit{Hate Expert} and the \textit{Sarcasm} auxiliary tasks.
Similarly to the results on the validation set, most MTL MLP models yield an improvement in terms of recall over all baselines, while only one shows a strong decrease in performance.
The best auxiliary task configuration in terms of recall only uses the \textit{Toxicity} dataset for an auxiliary task.
In terms of precision, all task configurations provide an improvement over the MLP baseline, though similarly to the case with the accuracy score, none outperform the best-performing baseline in terms of precision, the linear SVM.
Although all task configurations yield an improvement over the MLP baseline, an interesting issue occurs where the use of all auxiliary tasks has a detrimental effect, resulting in the worst performance of the MTL models in terms of precision.
For the F1-score, no models outperform the SVM baseline, though all models outperform the MLP baseline.
The strongest performance here is obtained by an auxiliary task combination where two out of three auxiliary tasks are non-abusive in nature.
This performance gain is obtained through an increase in the experimental model's ability in precision at the cost of a slightly lower recall score.
Although, on their own, none of the non-abusive tasks obtain the highest performances out of all auxiliary task configurations, they frequently post highly competitive scores with the all other configurations.
This suggests that some of the improvements obtained when using multiple auxiliary tasks is obtained through the information of non-abusive information encoded into the shared layer of the model.

Aligning with the insights on dataset mappings identified in \cref{chap:liwc}, the \textit{Hate Expert} dataset provides for boosted predicted power when used in conjunction with a non-abusive auxiliary task.
Somewhat surprisingly, the \textit{Offence} auxiliary task equally provides boosts in performance.
These improvements in performance indicate that the source domain of the data offer greater inductive biases, that can be optimised, than the size, as evidenced by the \textit{Toxicity} dataset not being selected for further experimentation.
The selection of the \textit{Hate Expert} auxiliary task also suggests that an alignment of dataset goals, and the processes to achieve those goals, also have a positive factor.

\begin{table}[]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                      & Accuracy                          & Precision                         & Recall                            & F1-score                          \\\hline
    Hate Expert                                       & 0.8082 ($\sigma$ 0.0098)          & 0.7496 ($\sigma$ 0.0117)          & 0.7661 ($\sigma$ 0.0153)          & 0.7564 ($\sigma$ 0.0044)          \\
    Offence                                           & 0.8194 ($\sigma$ 0.0047)          & 0.7001 ($\sigma$ 0.0181)          & \textbf{0.8204} ($\sigma$ 0.0069) & 0.7430 ($\sigma$ 0.0124)          \\
    Toxicity                                          & 0.8105 ($\sigma$ 0.0075)          & 0.7236 ($\sigma$ 0.0244)          & 0.7779 ($\sigma$ 0.0184)          & 0.7424 ($\sigma$ 0.0165)          \\
    Moral Sentiment                                   & 0.8234 ($\sigma$ 0.0024)          & 0.7302 ($\sigma$ 0.0120)          & 0.8039 ($\sigma$ 0.0079)          & 0.7595 ($\sigma$ 0.0067)          \\
    Argument Basis                                    & 0.8141 ($\sigma$ 0.0090)          & 0.7380 ($\sigma$ 0.0185)          & 0.7744 ($\sigma$ 0.0228)          & 0.7489 ($\sigma$ 0.0146)          \\
    Sarcasm                                           & 0.8192 ($\sigma$ 0.0018)          & 0.7262 ($\sigma$ 0.0167)          & 0.7993 ($\sigma$ 0.0145)          & 0.7548 ($\sigma$ 0.0065)          \\
    Hate Expert | Moral Sentiment                     & 0.8173 ($\sigma$ 0.0071)          & 0.7366 ($\sigma$ 0.0089)          & 0.7882 ($\sigma$ 0.0191)          & 0.7523 ($\sigma$ 0.0100)          \\
    Offence | Moral Sentiment                         & 0.7985 ($\sigma$ 0.0090)          & 0.6994 ($\sigma$ 0.0240)          & 0.7628 ($\sigma$ 0.0155)          & 0.7244 ($\sigma$ 0.0163)          \\
    Hate Expert | Offence                             & 0.8147 ($\sigma$ 0.0053)          & 0.7294 ($\sigma$ 0.0222)          & 0.7845 ($\sigma$ 0.0184)          & 0.7505 ($\sigma$ 0.0070)          \\
    Sarcasm | Hate Expert                             & \textbf{0.8253} ($\sigma$ 0.0079) & 0.7333 ($\sigma$ 0.0277)          & 0.8096 ($\sigma$ 0.0167)          & 0.7618 ($\sigma$ 0.0171)          \\
    Sarcasm | Offence                                 & 0.8182 ($\sigma$ 0.0051)          & 0.7186 ($\sigma$ 0.0266)          & 0.8031 ($\sigma$ 0.0139)          & 0.7518 ($\sigma$ 0.0160)          \\
    Sarcasm | Moral Sentiment                         & 0.8215 ($\sigma$ 0.0071)          & 0.7209 ($\sigma$ 0.0231)          & 0.8108 ($\sigma$ 0.0101)          & 0.7548 ($\sigma$ 0.0164)          \\
    Hate Expert | Offence | Moral Sentiment           & 0.7901 ($\sigma$ 0.0176)          & 0.7488 ($\sigma$ 0.0057)          & 0.7366 ($\sigma$ 0.0227)          & 0.7407 ($\sigma$ 0.0153)          \\
    Sarcasm | Hate Expert | Moral Sentiment           & 0.8180 ($\sigma$ 0.0071)          & 0.7129 ($\sigma$ 0.0222)          & 0.8016 ($\sigma$ 0.0046)          & 0.7473 ($\sigma$ 0.0166)          \\
    Sarcasm | Offence | Moral Sentiment               & 0.8151 ($\sigma$ 0.0066)          & \textbf{0.7502} ($\sigma$ 0.0070) & 0.7762 ($\sigma$ 0.0103)          & \textbf{0.7623} ($\sigma$ 0.0051) \\
    Sarcasm | Hate Expert | Offence | Moral Sentiment & 0.8124 ($\sigma$ 0.0101)          & 0.6898 ($\sigma$ 0.0221)          & 0.8142 ($\sigma$ 0.0053)          & 0.7278 ($\sigma$ 0.0234)
  \end{tabular}%
  }
  \caption{Experimental model evaluation scores on the \textit{Hate Speech} dataset. \textbf{bolded} scores indicate the highest performances and $\sigma$ values indicate the standard deviation.}
  \label{tab:mtl_test_wh}
\end{table}

\paragraph{Offence Main Task}
When using \textit{Offence} detection as the main task, most models outperform all baselines in terms of F1-score (see \cref{tab:mtl_test_davidson}).
The exception to this pattern are two auxiliary task configurations: the first where only the \textit{Argument Basis} auxiliary task is used, and the second where the \textit{Sarcasm} auxiliary task is used in conjunction with one abusive language detection task.
In the setting where only one auxiliary task is used, the \textit{Hate Speech} auxiliary task somewhat surprisingly provides for slightly bigger improvements across all scores than the \textit{Toxicity} auxiliary task.
This further evidences that abusive tasks from the same data source provide for a good source of data.
When combining multiple auxiliary tasks however, the \textit{Toxicity} data provides for more stable improvements across different auxiliary task combinations.
This further lends credibility to the observation that a similarity of dataset goals is useful in a multi-task setting.

Focusing on the non-abusive auxiliary tasks, the \textit{Sarcasm} and \textit{Argument Basis} auxiliary tasks are selected for further experimentation given the results on the validation data.
The \textit{Sarcasm} auxiliary task, when used in isolation from other auxiliary tasks, offers competitive results with the abusive language detection auxiliary tasks used in isolation, and outperforms the closely related \textit{Toxicity} auxiliary task in terms of accuracy.
The \textit{Argument Basis} auxiliary task affords performance improvements along accuracy, precision, and F1-score and only when this task is used in conjunction with other auxiliary tasks, specifically the \textit{Sarcasm} and \textit{Hate Speech} auxiliary tasks, do the models outperform the best baseline model in terms of precision.

The experimental models optimised for the \textit{Offence} detection task post a poorer recall than the best performing baseline model, thus the increased performances in F1-score are driven by improvements in precision score.
The best performing baseline model, in terms of recall, obtains a score of $0.91$ but only obtains $0.57$ in terms of precision.
Thus, adding auxiliary tasks provides for models that are more balanced in terms of performance on precision and recall, yielding an improved performance in terms of F1-score as neither precision or recall are being neglected in favour of the other.
Comparing the MTL models to the SVM baseline, all auxiliary task settings models strongly outperform the baseline in terms of recall.

\begin{table}[]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                           & Accuracy                          & Precision                         & Recall                            & F1-score                          \\\hline
    Hate Speech                                            & 0.8970 ($\sigma$ 0.0028)          & 0.6952 ($\sigma$ 0.0240)          & 0.7685 ($\sigma$ 0.0339)          & 0.7119 ($\sigma$ 0.0138)          \\
    Toxicity                                               & 0.8916 ($\sigma$ 0.0036)          & 0.6829 ($\sigma$ 0.0176)          & 0.7659 ($\sigma$ 0.0074)          & 0.7099 ($\sigma$ 0.0159)          \\
    Hate Expert                                            & 0.8871 ($\sigma$ 0.0048)          & 0.6794 ($\sigma$ 0.0274)          & 0.7552 ($\sigma$ 0.0110)          & 0.7028 ($\sigma$ 0.0193)          \\
    Sarcasm                                                & 0.8933 ($\sigma$ 0.0029)          & 0.6790 ($\sigma$ 0.0303)          & 0.7594 ($\sigma$ 0.0141)          & 0.6987 ($\sigma$ 0.0271)          \\
    Argument Basis                                         & 0.8973 ($\sigma$ 0.0045)          & 0.6740 ($\sigma$ 0.0238)          & 0.7775 ($\sigma$ 0.0202)          & 0.6847 ($\sigma$ 0.0207)          \\
    Moral Sentiment                                        & 0.8956 ($\sigma$ 0.0045)          & 0.6860 ($\sigma$ 0.0344)          & 0.7541 ($\sigma$ 0.0267)          & 0.6938 ($\sigma$ 0.0356)          \\
    Hate Speech | Toxicity                                 & 0.8860 ($\sigma$ 0.0046)          & 0.6961 ($\sigma$ 0.0157)          & 0.7294 ($\sigma$ 0.0211)          & 0.7033 ($\sigma$ 0.0135)          \\
    Sarcasm | Hate Speech                                  & 0.8891 ($\sigma$ 0.0080)          & 0.6561 ($\sigma$ 0.0239)          & 0.7757 ($\sigma$ 0.0157)          & 0.6806 ($\sigma$ 0.0200)          \\
    Sarcasm | Toxicity                                     & 0.8977 ($\sigma$ 0.0011)          & 0.6723 ($\sigma$ 0.0126)          & \textbf{0.7899} ($\sigma$ 0.0063) & 0.6811 ($\sigma$ 0.0227)          \\
    Sarcasm | Toxicity | Hate Speech                       & 0.8923 ($\sigma$ 0.0028)          & 0.6877 ($\sigma$ 0.0213)          & 0.7601 ($\sigma$ 0.0079)          & 0.7091 ($\sigma$ 0.0151)          \\
    Argument Basis | Hate Speech                           & 0.8864 ($\sigma$ 0.0036)          & 0.7043 ($\sigma$ 0.0320)          & 0.7386 ($\sigma$ 0.0173)          & 0.7155 ($\sigma$ 0.0139)          \\
    Argument Basis | Toxicity                              & 0.8917 ($\sigma$ 0.0052)          & 0.6929 ($\sigma$ 0.0203)          & 0.7552 ($\sigma$ 0.0049)          & 0.7143 ($\sigma$ 0.0139)          \\
    Argument Basis | Sarcasm                               & 0.9010 ($\sigma$ 0.0022)          & 0.6939 ($\sigma$ 0.0170)          & 0.7791 ($\sigma$ 0.0110)          & 0.7105 ($\sigma$ 0.0190)          \\
    Argument Basis | Hate Speech | Toxicity                & 0.8972 ($\sigma$ 0.0036)          & 0.6861 ($\sigma$ 0.0089)          & 0.7723 ($\sigma$ 0.0085)          & 0.7064 ($\sigma$ 0.0083)          \\
    Argument Basis | Hate Speech | Sarcasm                 & \textbf{0.9025 ($\sigma$ 0.0007)} & \textbf{0.7107 ($\sigma$ 0.0090)} & 0.7820 ($\sigma$ 0.0093)          & \textbf{0.7291 ($\sigma$ 0.0072)} \\
    Argument Basis | Toxicity | Sarcasm                    & 0.8925 ($\sigma$ 0.0044)          & 0.6973 ($\sigma$ 0.0218)          & 0.7472 ($\sigma$ 0.0082)          & 0.7138 ($\sigma$ 0.0125)          \\
    Argument Basis | Hate Speech | Toxicity | Sarcasm      & 0.8948 ($\sigma$ 0.0016)          & 0.6979 ($\sigma$ 0.0048)          & 0.7652 ($\sigma$ 0.0102)          & 0.7100 ($\sigma$ 0.0049)
  \end{tabular}%
  }
  \caption{Experimental model evaluation scores on the \textit{Offence} dataset. \textbf{bolded} scores indicate the highest performances and $\sigma$ values indicate the standard deviation.}
  \label{tab:mtl_test_davidson}
\end{table}

\paragraph{Toxicity Main Task}
\afterpage{
\begin{landscape}
\begin{table}[]
  \centering
  \resizebox{0.75\paperheight}{!}{%
  \begin{tabular}{l|cccc}
                                                                        & Accuracy                          & Precision                         & Recall                            & F1-score                          \\\hline
    Hate Speech                                                         & 0.9523 ($\sigma$ 0.0006)          & 0.8265 ($\sigma$ 0.0184)          & 0.8836 ($\sigma$ 0.01505)         & 0.8513 ($\sigma$ 0.0052)          \\
    Offence                                                             & 0.9506 ($\sigma$ 0.0027)          & \textbf{0.8445 ($\sigma$ 0.0091)} & 0.8637 ($\sigma$ 0.0147)          & 0.8535 ($\sigma$ 0.0049)          \\
    Hate Expert                                                         & 0.9497 ($\sigma$ 0.0008)          & 0.7794 ($\sigma$ 0.0081)          & 0.9074 ($\sigma$ 0.0124)          & 0.8283 ($\sigma$ 0.0027)          \\
    Moral Sentiment                                                     & 0.9535 ($\sigma$ 0.0005)          & 0.8058 ($\sigma$ 0.0074)          & 0.9061 ($\sigma$ 0.0088)          & 0.8469 ($\sigma$ 0.0025)          \\
    Sarcasm                                                             & 0.9532 ($\sigma$ 0.0004)          & 0.8029 ($\sigma$ 0.0131)          & 0.9078 ($\sigma$ 0.0130)          & 0.8452 ($\sigma$ 0.0050)          \\
    Argument Basis                                                      & 0.9477 ($\sigma$ 0.0040)          & 0.7604 ($\sigma$ 0.0307)          & 0.9152 ($\sigma$ 0.0098)          & 0.8145 ($\sigma$ 0.0245)          \\
    Moral Sentiment | Offence                                           & 0.9518 ($\sigma$ 0.0005)          & 0.7897 ($\sigma$ 0.0125)          & 0.9118 ($\sigma$ 0.0144)          & 0.8370 ($\sigma$ 0.0048)          \\
    Moral Sentiment | Hate Speech                                       & 0.9530 ($\sigma$ 0.0013)          & 0.7984 ($\sigma$ 0.0163)          & 0.9109 ($\sigma$ 0.0114)          & 0.8430 ($\sigma$ 0.0088)          \\
    Hate Speech | Offence                                               & \textbf{0.9538 ($\sigma$ 0.0008)} & 0.8238 ($\sigma$ 0.0142)          & 0.8931 ($\sigma$ 0.0139)          & \textbf{0.8537 ($\sigma$ 0.0038)} \\
    Sarcasm | Moral Sentiment                                           & 0.9526 ($\sigma$ 0.0007)          & 0.7929 ($\sigma$ 0.0092)          & 0.9132 ($\sigma$ 0.0054)          & 0.8401 ($\sigma$ 0.0052)          \\
    Sarcasm | Hate Speech                                               & 0.8947 ($\sigma$ 0.0798)          & 0.7800 ($\sigma$ 0.0733)          & 0.8467 ($\sigma$ 0.0535)          & 0.8070 ($\sigma$ 0.0654)          \\
    Sarcasm | Offence                                                   & 0.9523 ($\sigma$ 0.0007)          & 0.7949 ($\sigma$ 0.0138)          & 0.9099 ($\sigma$ 0.0113)          & 0.8402 ($\sigma$ 0.0065)          \\
    Argument Basis | Hate Speech                                        & 0.9514 ($\sigma$ 0.0008)          & 0.8027 ($\sigma$ 0.0153)          & 0.8966 ($\sigma$ 0.0159)          & 0.8410 ($\sigma$ 0.0057)          \\
    Argument Basis | Moral Sentiment                                    & 0.9528 ($\sigma$ 0.0004)          & 0.8026 ($\sigma$ 0.0070)          & 0.9047 ($\sigma$ 0.0060)          & 0.8443 ($\sigma$ 0.0031)          \\
    Argument Basis | Sarcasm                                            & 0.9528 ($\sigma$ 0.0011)          & 0.8182 ($\sigma$ 0.0224)          & 0.8925 ($\sigma$ 0.0144)          & 0.8495 ($\sigma$ 0.0095)          \\
    Argument Basis | Offence                                            & 0.9511 ($\sigma$ 0.0014)          & 0.7813 ($\sigma$ 0.0132)          & 0.9162 ($\sigma$ 0.0139)          & 0.8321 ($\sigma$ 0.0079)          \\
    Moral Sentiment | Hate Speech | Offence                             & 0.9532 ($\sigma$ 0.0005)          & 0.7932 ($\sigma$ 0.0095)          & 0.9174 ($\sigma$ 0.0100)          & 0.8415 ($\sigma$ 0.0042)          \\
    Sarcasm | Moral Sentiment | Hate Speech                             & 0.9517 ($\sigma$ 0.0012)          & 0.7845 ($\sigma$ 0.0115)          & 0.9165 ($\sigma$ 0.0051)          & 0.8348 ($\sigma$ 0.0072)          \\
    Sarcasm | Moral Sentiment | Offence                                 & 0.9513 ($\sigma$ 0.0012)          & 0.7760 ($\sigma$ 0.0106)          & \textbf{0.9235 ($\sigma$ 0.0065)} & 0.8303 ($\sigma$ 0.0071)          \\
    Argument Basis | Moral Sentiment | Sarcasm                          & 0.9535 ($\sigma$ 0.0005)          & 0.8053 ($\sigma$ 0.0160)          & 0.9083 ($\sigma$ 0.0177)          & 0.8467 ($\sigma$ 0.0054)          \\
    Argument Basis | Moral Sentiment | Hate Speech                      & 0.9532 ($\sigma$ 0.0018)          & 0.8170 ($\sigma$ 0.0075)          & 0.8947 ($\sigma$ 0.0133)          & 0.8503 ($\sigma$ 0.0046)          \\
    Argument Basis | Moral Sentiment | Offence                          & 0.9537 ($\sigma$ 0.0007)          & 0.8194 ($\sigma$ 0.0142)          & 0.8958 ($\sigma$ 0.0102)          & 0.8520 ($\sigma$ 0.0057)          \\
    Sarcasm | Moral Sentiment | Hate Speech | Offence                   & 0.9526 ($\sigma$ 0.0018)          & 0.7883 ($\sigma$ 0.0228)          & 0.9197 ($\sigma$ 0.0131)          & 0.8379 ($\sigma$ 0.0128)          \\
    Argument Basis | Moral Sentiment | Sarcasm | Hate Speech            & 0.9526 ($\sigma$ 0.0003)          & 0.8242 ($\sigma$ 0.0190)          & 0.8869 ($\sigma$ 0.0155)          & 0.8512 ($\sigma$ 0.0060)          \\
    Argument Basis | Moral Sentiment | Sarcasm | Offence                & 0.9505 ($\sigma$ 0.0017)          & 0.8107 ($\sigma$ 0.0232)          & 0.8852 ($\sigma$ 0.0183)          & 0.8417 ($\sigma$ 0.0098)          \\
    Argument Basis | Moral Sentiment | Sarcasm | Offence | Hate Speech  & 0.9512 ($\sigma$ 0.0018)          & 0.7948 ($\sigma$ 0.0236)          & 0.9035 ($\sigma$ 0.0187)          & 0.8374 ($\sigma$ 0.0118)
  \end{tabular}%
  }
  \caption{Experimental model evaluation scores on the \textit{Toxicity} dataset. \textbf{bolded} scores indicate the highest performances and $\sigma$ values indicate the standard deviation.}
  \label{tab:mtl_test_wulczyn}
\end{table}
\end{landscape}
}

The final task that I explore experimentally through MTL is the \textit{Toxicity} detection task.
This task is the task that has the most auxiliary tasks that are explored experimentally with five out of six tasks explored, the only task that is note explored is the \textit{Hate Expert} task.
Moreover, this is also the task with the largest dataset available and the only binary task, meaning that this is the only task where the results are directly comparable with the results obtained in \cref{chap:liwc}.

Observing experimental model results in \cref{tab:mtl_test_wulczyn} with the performances of the baseline models in \cref{tab:baseline_test_wulczyn}, we see that no setting of the auxiliary tasks outperform the SVM baseline in terms of F1-score or accuracy.
However, all task configurations outperform the SVM in terms of recall and are competitive in terms of the accuracy score with the worst performing configuration, one that uses the \textit{Sarcasm} and the \textit{Hate Speech} auxiliary tasks scoring $0.0635$ below the SVM baseline and $0.045$ lower the MLP baseline.
On the other hand, the best performing model in terms of accuracy has a score that is $0.0044$ lower than the SVM baseline and $0.0141$ higher than the MLP baseline.
For precision, similarly none of the experimental settings outperform the SVM or the ensemble baselines.
The best auxiliary task configuration only uses the \textit{Offence} auxiliary task and obtains a precision score of $0.8445$, $0.0563$ lower than the SVM baseline, while the worst performing auxiliary task setting only uses the \textit{Argument Basis} with a score of $0.7604$ or $0.1404$ lower than the SVM baseline.
Here is an indication that using the F1-score as the only metric by which the auxiliary tasks are chosen is suboptimal as the \textit{Hate Expert} auxiliary task outperforms the \textit{Argument Basis} auxiliary task for all metrics asides from the recall score.
Although the performance in terms of precision is lower than the SVM baseline, all auxiliary task configurations outperform the MLP baseline with the worst performing task setting outperforming the MLP baseline by $0.0625$ and a $0.1466$ improvement over the baseline for the best-performing task setting.
For the recall score, the best performing baseline model is the ensemble baseline, closely followed by the MLP baseline.
Neither of these baselines are outperformed by the MTL models though all experimental models improve on the SVM baseline.
The best performing model here uses three different auxiliary tasks, namely the \textit{Sarcasm} task, the \textit{Moral Sentiment} task, and the \textit{Offence} task and only underperforms by $0.0124$ in comparison to the MLP baseline.
Finally, focusing on the F1-score.
Although no auxiliary task setting outperforms the best performing baseline, the SVM baseline, task configurations strongly outperform the MLP baseline.
The best performing auxiliary task setting uses the \textit{Hate Speech} and the \textit{Offence} auxiliary tasks, that is all of the abusive language detection tasks under consideration and it outperforms the MLP baseline by $0.0905$.
The worst performing auxiliary task setting outscores the MLP baseline by $0.0438$ and uses the \textit{Sarcasm} and the \textit{hate Speech} auxiliary tasks.

\zw{Add note on the best performing MTL model vs. the best performing model from the LIWC chapter}

\paragraph{Auxiliary Task Patterns}
When considering the performances of each main task in isolation, the larger patterns across the different main tasks are obscured by the details of the model performances.
Here I take a birds-eye view on all three main tasks and the patterns that emerge from across the different main tasks and their auxiliary task settings.

In the performances of all three main tasks, auxiliary task configurations that include abusive language detection tasks tend to post the high performances.
This may be because forms and phrasings in abusive, and non-abusive, text that are infrequently occurring in the main task can be aided by the data provided in the abusive auxiliary tasks.
This is particularly evidenced through the improvements along the precision score, meaning that using abusive language detection auxiliary tasks can minimise the number of false negatives that the model predicts.
These increases in the precision scores also positively affect the F1-scores, although these are often hampered by a lower recall score.
While there appears to be a lesser impact of dataset size, likely because the number of epochs is fixed, so the larger datasets are not afforded the ability to contribute more by virtue of their size, the data source and dataset goals appear to influence the performances.
This is apparent in all three main task settings but is particularly visible when the main task is \textit{Offence} detection.
Here, \textit{Hate Speech} shares the same data source but has different annotation goals whereas \textit{Toxicity} shares in the dataset goals but the data has a different source.
In the setting where only one auxiliary task is used, both post comparative performances.
When two auxiliary tasks are used and these auxiliary tasks are used together, they produce results that are lower across all metrics asides from recall than when only one of the tasks are used.
When both auxiliary abuse detection tasks are used, then adding a at least one more auxiliary task that is not abuse detection has a slightly beneficial impact on the model performance.

The non-abusive auxiliary tasks may at first seem to contribute less towards improved model performances than the abuse detection auxiliary tasks, however for two out of three different main tasks, the best F1-scores are obtained by the combination of abuse detection tasks and auxiliary tasks that are not addressing the question of abuse.
The non-abuse related auxiliary tasks, when used in conjunction with abuse detection tasks offer modelling improvements across all metrics.
However, some non-abusive tasks appear to be more applicable in general, specifically the \textit{Sarcasm} auxiliary task appears across all three main tasks while the \textit{Argument Basis} and \textit{Moral Sentiment} auxiliary tasks appear for two different main task settings.
Moreover, all three auxiliary tasks appear in the best performing auxiliary task configurations and in the setting where only one auxiliary task is used, they achieve competitive performances with the models that use an abuse detection auxiliary task.

\section{Conclusions}

Automated detection of abuse occurring in online spaces is a complex problem that is contingent on the ability to contextualise comments made by authors with the intentions of the authors, how the comments are perceived by readers, and situating the comments within the contexts of the authors.
In this chapter, I attempt to address the issue of contextualisation through the use of Multi-Task Learning.
Using MTL, I examine how different auxiliary tasks impact in-domain classification of abuse detection.
Specifically, I examine how the use of datasets developed for the purpose of optimising machine learning models for abuse as auxiliary tasks can impact the in-domain performances of different forms of abuse detection.
I contrast the use of these resources with resources that are developed for classifying the basis of an argument, the moral sentiments displayed in messages, and the use of sarcasm and examine how these impact the ability of MTL models to classify abuse, when the resources are used as auxiliary tasks.
Finally, I examine whether there are synergies between datasets developed for detecting abuse and datasets developed to predict other constructs that are conducive towards improved in-domain classification performance for the main task.

Through the use of MTL, I show that machine learning models developed for detecting abuse can benefit from using auxiliary task datasets.
In studying the first question, how other abuse detection datasets impact performance on the main task, when they are used as auxiliary tasks, I find that datasets that 1) share in the goals of the main task dataset, or 2) are sampled from the same data source, even with disparate dataset goals positively influence the performance of neural machine learning models on the main task.
The former condition further provides support for the findings in \cref{chap:liwc} where I identified that common dataset goals allow for better generalisation.
Rather than provide for better generalisation from one task to another, I find in this chapter that different datasets with similar goals can provide be used in a multi-task setting to improve the in-domain classification results for one another.
The latter observation can be understood through the aims and purposes of the MTL framework.
MTL operates with the express notion that distinct tasks may share inherent latent information that can be represented in a machine learning model through joint optimisation of the tasks.
Where language production on online platforms is created under restrictions put in place by the platforms and the cultures of communication that are fostered on the platforms.
Thus, through using auxiliary tasks for abuse where the data is sourced form the same platform(s), an MTL model can optimise representations of the particularities of communication on the platform(s) in question, thus gaining a representation that can yield improvements in the performances of the main task.

In addressing the second question, how data developed for tasks that are not directly related to abuse detection can impact the performance of abuse detection models, I find that such tasks can aid in the classification capabilities of abuse detection models, when the choice of tasks is informed by specific questions surrounding the primary task.
In particular, I find that the tasks of identifying whether an argument is made on an emotional or factual basis, detecting sarcasm, and detecting the moral sentiments all have a beneficial impact across the different main task settings.
While I find that all non-abusive auxiliary tasks that I experiment with are beneficial for some forms of abuse, I also find that not all such tasks are equally well suited for all forms of abuse.
For instance, the \textit{Argument Basis} auxiliary task does not perform well enough to be included in the experiments, when it is used as the only auxiliary task to the \textit{Hate Speech} main task.
Similarly, when the main task is the \textit{Offence} task, the \textit{Moral Sentiment} auxiliary task does not perform well enough on the validation data to warrant inclusion in the experiments.
% Thus, the selection of auxiliary tasks should be informed by the intuition of the designer of the machine learning experiment in addition to the empirical performance of a model.

For the third question pertaining to how the abusive and non-abusive auxiliary tasks interact when used in conjunction with each other, the results obtained in this chapter are clear.
I find that there are clear benefits to the main task from using a combination of abusive and non-abusive auxiliary tasks with several task configurations that include abusive and non-abusive auxiliary tasks posting highly competitive scores with other top performing models, if not outperforming all other task configurations outright.
By identifying abusive and non-abusive auxiliary tasks that perform well in the case where there is only one auxiliary task, it is possible for a model that uses them can benefit from the selected task to post performances that improve on the scores achieved in the single-auxiliary task setting.

\subsection{Future Work}
The findings in this chapter have implications in the way automated abuse detection has been performed to date.
Beyond the limited number of auxiliary tasks that have been investigated in this chapter, there is space for further experimentation on different kinds of auxiliary tasks.
For instance, two natural extensions of this work in terms of auxiliary task selection are apparent: 1) the investigation of how optimising for core NLP tasks can yield a potential benefit as these can aid MTL models in representing language construction in different data sources and 2) following on the work of \citet{Davidson:2019} and \citet{Dias:2021}, using datasets that specifically incorporate varieties of English spoken by groups that are often subject to high false positive errors can allow designers to encode in the model how different groups are affected and should be affected by machine learning models for content moderations.
Building on my findings of how similarity in data sources impact the main task, there is space for investigating how non-abusive tasks that are collected from the same data source can impact the modelling performance.

Beyond a consideration of different auxiliary tasks there are also several modelling questions that could be further explored.
First, from my experiments I find that the models tend to prefer the largest batch size that I experiment with, thus an investigation on the impacts of larger batch sizes is warranted.
The question of how to weight the different auxiliary tasks and the frequency with which batches are selected from each also warrants further investigation as all auxiliary tasks are unlikely to contribute equally, thus identifying dynamic methods for weighting each task or setting the frequency with which batches are chosen from each task may also provide an avenue for improved performances.
Finally, in my experimental models I do not use pre-trained embedding layers in my models, however the use of these have been shown to achieve high classification performances in single-task neural networks, thus the use of pre-trained embedding layers may allow for further improvements of the classification scores achieved.
