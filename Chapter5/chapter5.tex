% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi

\chapter[Tasks that Matter: Multi-task Learning for Abusive Language Detection]{Tasks that Matter: Multi-task Learning for Abusive Language Detection\footnotemark{}}\label{chap:mtl}
\footnotetext{This chapter contains elements of an ongoing collaboration with Joachim Bingel, Hero I/S. All contents of the chapter, are original work produced for this dissertation. The shared elements between the project and this chapter are the machine learning model designs.}

\begin{quote}
  ``So is hate speech detection kind of like sentiment analysis++'' -- ACL 2016 Conference Attendee\footnote{Check with AJ that I can attribute.}
\end{quote}

One of the frequently made assumptions is that hate speech detection, and in general abusive language detection, shares many similarities to other tasks that also take on the challenge of identifying and predicting subjective human experiences such as sentiment analysis, sarcasm detection, and emotion detection.
Indeed, each of these tasks share the characteristic that the identification of each of these on the basis of text is a task of linguistic pragmatics and that the interpretation of a given statement will vary on the basis of parties involved in the communicative act.
While they share this unifying characteristic, hate and humour, for instance occupy different but sometimes overlapping processes as highlighted by the NGO partners interviewed by \citet{Rottger:2021}.

Similarities between distinct related tasks pose several interesting questions.
First, is it best to create multiple annotations, either through re-annotating previously published data or creating an entirely dataset, such that each task is addressed in all of the data or should one try to develop modelling architectures that are overlapping?
Second, how much data from each task is necessary to annotate, in the case of creating multiple annotations for each document; or, if the task is approached in terms of developing modelling architectures, how much of the data from each task should be used in optimising the model, or alternatively, how should the data from each task be weighted to gain the largest modelling improvements?

In this chapter, I approach the question of overlapping data through a question of developing a modelling approach that aims to use potential overlaps between each task.
As each task may be related only in terms of the abstraction required to understand the meaning of a given text, creating mappings between different classes is a complex task that in some cases is not possible.
Moreover, data for each task may be collected from different sources, at different times, from different populations that use different vocabularies resulting in models that may learn spurious patterns in the data that are not trivial to identify and address.
Thus modelling can be approached in two distinct manners. Either all documents are collapsed into a single dataset without creating maps between the different classes or each task remains a distinct task and model architectures such as Multi-Task Learning (MTL) and Ensemble methods are explored.
Here I take the latter approach, developing a MTL model that considers each task simultaneously in tandem and distinctly from all other tasks (see \cref{sub:mtl} for more details on how MTL functions).
I select a MTL modelling approach over an ensemble approach as optimising an ensemble requires optimising $n$ entirely distinct models, one for each task, and a final model that considers the outputs of each model, MTL models on the other hand can be optimised such that a models is optimised to perform on its primary task, treating all auxiliary tasks as secondary.
Moreover, as I use a hard-parameter sharing design for my MTL models, an additional benefit is that all auxiliary tasks act as regularisers, even if they are not directly beneficial to the primary task.

Through the use of of MTL models, I find that non-abusive tasks as auxiliary can be beneficial to detecting all forms of abuse examined.
In line with the results in \cref{chap:liwc}, there is a difference in how helpful different abusive language datasets are for each other.
However, in spite of benefits from using MTL over some single-task baselines, some baseline models still out-perform some of the MTL models.
% TODO Add here about how dataset combinations contribute.

% WH - benefits from all aux tasks, abusive and non-abusive
% Wulczyn - benefits from all non-abusive tasks; benefits from WH, Davidson, but not Waseem
% Davidson - Benefits from all non-abusive and abusive tasks;

Thus, in this chapter I ask the following research questions to gain a deeper understanding on how different tasks that may share relations to abuse impact model performance:

\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}
        \item{\textit{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}
        \item{\textit{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}
    \end{enumerate}
\end{minipage}

\section{Previous work}

MTL has previously been applied for a number of tasks in NLP, including language specific tasks such as multi-word expression identification \cite{Bingel-Bjerva:2018}, machine translation \cite{Dong:2015}, and sequence labelling \cite{Rei:2017}.
Further, MTL has also been used in tasks that produce social outcomes such as predicting mental health conditions \cite{Benton:2017}, hate speech detection \cite{Waseem:2018,Rajamanickam:2020,Farha:2020,Djandji:2019}, and rumour verification \cite{Kochkina:2018}.

\subsection{Modelling}
For hate speech detection, and abusive language detection in general, MTL has been applied to English \citep{Waseem:2018,Rajamanickam:2020} and Arabic \citep{Farha:2020,Djandji:2019}.
Considering that I use datasets that are entirely in English, I only consider the previous work for hate speech detection using MTL for English language data.

\citet{Waseem:2018} show that the cultural gaps that exist between different datasets, as a result of their collection strategies and annotation procedures, could be addressed.
Using a hard parameter sharing strategy, they develop a MTL model that uses two different tasks for optimisation.
In their model, sampling of the batches is chosen at random, with one of the tasks set as the main task and a manual mapping between the distinct classes is performed.
The machine learning model chosen by \citet{Waseem:2018} for their MTL experiment is a back-propagated MLP with a tanh activation function and Adam as their optimisation function.
For the input representations \citet{Waseem:2018} experiment with a Bag-of-Words model that uses the $5,000$ most frequent terms and model that uses Byte-Pair encoded input data.
Similarly to \citet{Waseem:2018}, \citet{Rajamanickam:2020} show that using hard parameter sharing strategy for MTL with an auxiliary task can aid in the detection of hate speech.
Rather than using a different task coded for abuse as \citet{Waseem:2018} do, \citet{Rajamanickam:2020} instead ask whether jointly learning which emotions are invoked in a given task can aid in the detection of abuse.
Moreover, the architectures of the two different approaches diverge from one another.
\citet{Rajamanickam:2020} implement a double encoder model in which the primary and auxiliary share an encoder and each have a stacked Bi-directional Long-Short Memory Network that generate a second encoding.
The primary and auxiliary task models developed by \citet{Rajamanickam:2020} diverge at this point. The auxiliary task model directly passes the second encoding to a Bi-directional Long-Short Memory Network, the output of which is subject to an attention layer and finally passed through to a linear layer and subject to an activation function before producing the prediction of the model.
The primary task model sums the encodings obtained from the stacked Bi-directional Long-Short Term Memory networks for the primary and auxiliary task, passing this on to a Bi-directional Long Short Term Memory network.
The resulting representation is then passed through an attention layer and passed through an output layer generating the prediction.
A key difference between the hard parameter sharing models of \citet{Waseem:2018} and \citet{Rajamanickam:2020} is that the latter use a weighting parameter to distinguish between the primary and auxiliary task.
\citet{Waseem:2018} only distinguish between the primary and auxiliary tasks through the validation set.
The reason for this discrepancy is that \citet{Waseem:2018} seek to use multi-task learning to optimise a model that is capable of dealing with cross-cultural data, that is a model that is able to perform on both tasks.
\citet{Rajamanickam:2020} on the other hand seek to improve classification performance on the primary task, thus considering any performance gains on the auxiliary a side-benefit.
This discrepancy is the result of a natural prioritisation question, as the goal of \citet{Rajamanickam:2020} is to improve classification performance for abuse on a single dataset whereas \citet{Waseem:2018} seek to identify a classifier that can generalise beyond beyond the single dataset.

The work described in this chapter follows \citet{Rajamanickam:2020} in their focus on improving classification performances on the primary task.
For this reason, I choose auxiliary tasks (see \cref{sec:mtl_data} for an overview of the auxiliary datasets) that have been hypothesised as relevant to the question of detecting different forms of online abuse.

\subsection{Learning Tasks}\label{sec:mtl_tasks}

Multi-task learning, as the name of the framework implies, requires distinct tasks for learning, where each unique auxiliary task asks how learning representations from that task influences model performance on the primary task.
We saw in \cref{chap:liwc}, the optimised for each abusive dataset has different applications onto other datasets in the case of binary classification. Therefore, I choose to use three different tasks for abusive language as main tasks.
In contrast to the method in \cref{chap:liwc}, I do not binarise, or otherwise modify the classes in from those proposed by the authors of the datasets.
This provides for the more challenging tasks of predicting the type of abuse in addition to whether content is abusive or not.
A further consequence of not binarising the label sets for the main tasks is that the classes don't directly map onto other datasets.
This means that I preclude considerations of generalisability onto other datasets for abuse without further reduction of the predicted labels into a binarised label space.
Here I provide brief descriptions of the different datasets and the rationale for their inclusion, for more detail please refer to \cref{sec:datasets} and \cref{sub:liwc_datasets}.

\subsubsection{Main Task Datasets}
For the main tasks, I choose to use the \textit{Toxicity} dataset \citep{Wulczyn:2017}, the \textit{Offence} dataset \citep{Davidson:2017}, and the \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016}. I choose these three datasets in part due to their size and in part due to their examining of three different aspects of abuse.
Through this choice, I aim to identify which auxiliary tasks can improve performance for each type of abuse.
Each main task dataset is also used as an auxiliary task when it is not the used as a main task.

\paragraph{Hate Speech}
The \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} as proposed consists of $3,383$ comments labelled as sexist, $1,972$ labelled as racist and $11,559$ labelled as neither sexist or racist.
This dataset was proposed as a first step towards modelling racialised and gendered hate speech.
I use this dataset to show that the MTL framework can be used to distinguish between different targets of hate, as this dataset seeks to identify different forms of hate speech.
Beyond using this dataset to show the ability of MTL models to learn how to distinguish different forms of hate speech, this dataset also provides the largest source of hate speech, which otherwise is vanishingly small in other other main task datasets.

\paragraph{Offence}
The \textit{Offence} dataset \citep{Davidson:2017} was proposed to distinguish `offensive' content from `hateful' and content that is neither `hateful' or `offensive'.
In the class distribution proposed by \citet{Davidson:2017}, the `offensive' class occupies the vast majority of the dataset, with $19,190$ documents labelled into the class, followed by the negative class which consists of $4,163$ documents, and finally the `hateful' class which contains only $1,430$ documents.
As such, the class distribution for this dataset varies strongly from the \textit{Hate Speech} and the \textit{Toxicity} dataset, with the majority class being one of the two positive classes.
In using this dataset for the main task, I show that MTL models can provide a viable modelling approach in spite of a significantly different class distribution.

\paragraph{Toxicity}
The \textit{Toxicity} dataset \citep{Wulczyn:2017} provides a special case.
For one, it is the largest dataset consisting of $159,686$ labelled comments split into a training set of $95,692$ comments, a validation set of $32,128$ comments, and an evaluation set of $31,866$ comments.
In total, this dataset has more than $100,000$ more comments than either the \textit{Toxicity} or \textit{Hate Speech} datasets.
Second, the dataset proposes a binary classification of `toxic' and `not toxic'.
Thus, the results from the MTL models optimised for this dataset can be directly compared with the results obtained in \cref{chap:liwc}, unlike the models where the main task is \textit{Offence} or \textit{Hate Speech}.
Thus, I use this dataset to anchor the performances of the MTL models within the context of the preceding chapter and to show the impact of using a large scale dataset for abuse for MTL modelling.

\subsubsection{Auxiliary Task Datasets}

I choose the auxiliary task datasets for two different purposes: 1) to investigate the impact of using other datasets for abusive language as auxiliary tasks and 2) to examine how datasets that are labelled for other tasks can influence modelling for abuse.
To answer the first question, I use the three main task datasets in turn as auxiliary datasets when they are not serving as the main task. Moreover, to address the issue of the poor representation of content labelled within as hateful, I also use the \textit{Hate Expert} dataset.
Addressing the second motivation, I use a dataset labelled for sarcasm \citep{Oraby_sarcasm:2016}, a dataset labelled for the moral sentiment invoked by the text \citep{Hoover:2019}, and finally a dataset where documents are labelled for whether arguments are primarily based in emotion or in facts \citep{Oraby_factfeel:2015}.
With the exception of the \textit{Moral Sentiments} dataset, all auxiliary task datasets contain between $~5,000$ and $~16,000$ labelled documents, while the \textit{Moral Sentiments} consists of $~35,000$ labelled documents.
This spread of sizes of auxiliary task datasets allows for considering how auxiliary task dataset size impact the main task.

\paragraph{Sarcasm}
Previous work on hate speech detection \citep{Rottger:2021} has identified that sarcasm and irony can be contributing factors to misclassification from machine learning models as they take literally things that are communicated to be understood figuratively.
In efforts to better understand dialogue in online debate forums, \citet{Oraby_sarcasm:2016} develop a balanced dataset of $6520$ comments labelled for the occurrence of sarcasm.
Through this auxiliary task, MTL models learn representations of how sarcasm is constituted within \textit{Sarcasm} dataset, in addition to the other auxiliary tasks, and the main task.
Finally, through the use of this dataset, I explore how learning representations for sarcasm detection influences prediction of each operationalisation of detecting abuse.

\paragraph{Argument Basis}
Previous work on hate speech detection have suggested that many users who utter hate speech do so infrequently, suggesting that discriminatory speech may be produced in moments of carelessness and high emotionality \citep{Waseem:2016}. 
Moreover, as is apparent from the motivations used by \citet{Garcia:2019} for using StormFront as a data source, white supremacists may seek to mask their discrimination behind the use and distortions of fact.
Thus, whether hate is produced in the spur of the moment or is a part of a longer pattern, the basis upon which the argument is made, whether it is fact-based or based on emotion, may provide useful signals for learning to predict hate speech and abuse.
To model the hypothesis that high emotionality may influence the production of abuse, I include the \textit{Argument Basis} dataset \citep{Oraby_factfeel:2015}.
This dataset was developed using the same underlying data source as the \textit{Sarcasm} dataset, however rather than annotating the dataset for the occurrence of sarcasm, \citet{Oraby_factfeel:2015} annotate $5,848$ comments as being based in either fact or emotion.
The dataset is slightly imbalanced with $59\%$ of the dataset labelled as primarily fact-based and $41\%$ labelled as primarily based in feelings.
MTL models for abuse can take advantage of this dataset by learning a joint representation of the basis of an argument along with the main task in question.
Thus, this auxiliary task can provide insight into the question of whether learning such a joint representation is beneficial to detecting abuse and implicitly provide another signal into the feasibility of more deeply considering the emotional and mental state of the author when writing, e.g. through the use of LIWC in \cref{chap:liwc}.

\paragraph{Moral Sentiments}
The \textit{Moral Sentiments} dataset is annotated for the vices and virtues represented along five different `factors': \texttt{Loyalty/betrayal}, \texttt{care/harm},\\ \texttt{fairness/cheating}, \texttt{authority/subversion} and \texttt{purity/degradation} \citep{Hoover:2019}.
The authors of the dataset suggest that these five factors are likely to be represented in data that contains abuse, through their use of a subset of the \textit{Offence} dataset for annotation.
To explore this further, I use this dataset as an auxiliary task.
While the impacts of moral sentiments on the \textit{Offence} dataset are likely beneficial, using this auxiliary task on other datasets allows for examining whether representing moral sentiments has a positive impact on other datasets labelled for abuse.

\paragraph{Hate Expert}
Finally, as learned from \cref{chap:liwc}, some datasets for abusive language appear to be more closely related to others.
For this reason, I include the \textit{Hate Expert} dataset as an auxiliary task dataset to verify this finding and to provide a dataset to help address the poor representation of hate speech in the classes.
In \cref{chap:liwc} I binarise this dataset, here on the other hand I retain all $4$ classes in the dataset. 
In the expert annotated data, the four classes have a highly imbalanced distribution, the largest class being the negative class consuming $84\%$ of the data, while the second largest class `sexism' consumes $13\%$ of the data, the `racist' class consuming $1.4\%$ of the data and the final class, `both racist and sexist' contributing with $0.7\%$ of the data.

Focusing our attention on the smallest class for a moment, $0.7\%$ of $6,909$ documents means less than $50$ documents are labelled for the minority class, and given that I create stratified splits for the training ($80\%$), validation ($10\%$), and evaluation ($10\%$) sets, less than $40$ documents remain in the training set.
Thus there is not enough data for a machine learning model to learn patterns of abuse in the intersection between racist and sexist speech.
However, I choose to keep this data in the dataset to provide more instance of hate speech and to complicate, albeit only slightly, the question of what constitutes hate for the machine learning systems that use this dataset in the optimisation process.

\section{Modelling}\label{sec:mtl_modelling}
For the experiments conducted, I only use one form of tokens to allow for an examination of the impact of the auxiliary tasks rather than the impact of tokenisation.
I choose to represent all documents as their Byte-Pair encoded representations as these minimise the number of out-of-vocabulary tokens as this representation retains competitive performances in \cref{chap:liwc}.
To this end, I pre-process all documents using a $200$ dimensional Byte-Pair Embedding \citep{Heinzerling:2018}.
The pre-processing here follows the same method as in \cref{chap:liwc}, that is each document is lower-cased, all hyper-links are replaced with a `<URL>' token, all usernames are replaced with a `<USER>' token, and all hashtags are replaced with a `<HASHTAG>' token.
Then each document is passed through the Byte-Pair Embeddings to produce the Byte-Pair encoded representations, that is their sub-word units.

I develop three different types of baseline models: a linear single-task model where the model is optimised and evaluated on the same task, a non-linear single-task model, and a linear ensemble model where a model is optimised on the basis of outputs from the auxiliary task models.
In terms of experimental models, I follow \citet{Waseem:2018} in designing a multi-task Multi-Layered Perceptron implemented in PyTorch \citep{Paszke:2019}. I select an MLP over more complex neural networks architectures like Convolutional Neural Networks and Long-Short Term Memory networks due to the speed with which MLPs are optimised along with their general performance in \cref{chap:liwc}.

I perform parameter and hyper-parameter optimisation for the linear and non-linear models, respectively.
For the non-linear models I use the Weights and Biases library \citep{Wandb} to perform Bayesian Hyper-Parameter Optimisation.
For the linear models, I use grid-search as implemented in the Scikit-Learn library \citep{Pedregosa:2011}.

Once models have been optimised, they are each evaluated on the validation data and the evaluation data.
For non-linear models, the performance on the validation data guides the decision on which parameter configurations are chosen for analysis while for linear models, cross-validation is applied during the grid-search which aids in determining which parameter configuration performs best.

\subsection{Baseline Models}\label{sub:mtl_baselines}
% TODO Move all parameter tables to result section
I develop three baseline models: a linear single-task model, a neural network single-task model, and a linear ensemble model.
I choose to use a linear single-task model as a baseline as these often provide a strong baseline against neural network approaches for abuse detection \citep{Cite someone who says that here} while also being fast and efficient to optimise.
The non-linear neural network baseline is chosen as a counter-point to the linear baseline, using a MLP to more directly be able to consider the influence of the multi-task architecture for the experimental models.
Finally, I choose to an ensemble classifier that is optimised on the outputs of linear single-task models for each of the auxiliary task models as an ensemble, similarly to a multi-task model, can take advantage of learned representations for each auxiliary task for producing a prediction for the main task.

\paragraph{Single-Task Baselines}
Following prior work \citep{Waseem:2016,Davidson:2017}, I optimise all single-task models using a Support Vector Machine with a linear kernel (see \cref{sec:model_background} for more details on SVMs).
All linear single-task models are optimised on unigram counts of the Byte-Pair encoded tokens and are subject to parameter-optimisation of the regularisation type ($L1$ and $L2$) and the strength of regularisation (using values $0.1, 0.2, \ldots, 1.0$).

\paragraph{MLP Single-Task Baseline}
I develop a MLP as a non-linear counter-part to the linear single-task models to provide a baseline of the performance of a neural network approach that only relies on the Byte-Pair unigrams for the main task to optimise for the main task.
To ensure that the baseline model is also tuned for optimal performance, I perform a hyper-parameter sweep  over the batch size ($\{16, 32, 64\}$), the dropout value ($[0.0, 0.5]$), the dimensionality of the embedding layers ($\{64, 100, 200, 300\}$), the number of epochs for optimisation ($\{50, 100, 200\}$), the dimensionality of the hidden layers ($\{64, 100, 200, 300\}$), the learning rate ($[1^{-5}, 1.0]$), the Non-linearity to apply ($\{Tanh, ReLU\}$), and the optimiser function ($\{SGD, ASGD, Adam, AdamW\}$).
I conduct at least $50$ independent trials of distinct hyper parameter settings which identify the best hyper parameter configuration.

\paragraph{Ensemble Baseline}
The ensemble baselines require a different optimisation scheme that relies on a classifier that is optimised for each auxiliary task and an ensemble classifier that relies on the outputs of the auxiliary task classifiers, by virtue of the nature of ensemble classifiers.
For this reason, I first optimise a linear SVM for each auxiliary task and perform a grid search over the type of regulariser ({$L1$, $L2$}) and the strength of the regularisation (${0.1, 0.2,\ldots, 1.0}$) (see \cref{tab:aux_ensemble_params} for the parameter settings for each auxiliary task).
Once all auxiliary task classifiers have been optimised, I optimise a Logistic Regression model on the outputs of the auxiliary task classifiers on the main task training data, similarly subject to a grid-search over the same parameter values as the auxiliary task models
.
During this opitimisation procedure, the ensemble is provided with the training data for the main task, which is vectorised to the vocabulary of each auxiliary task and a prediction is obtained for each task.
For each document, predictions of all auxiliary task classifiers are vectorised and a classifier is optimised on the auxiliary task predictions.
While this method allows for every datasets to be passed through the model, by design this method limits the vocabulary to that which exists in the training datasets for the auxiliary tasks, rather than the main task.
This risk however is mitigated by the use of sub-words obtained by pre-processing all data through the Byte-Pair embeddings.

\subsection{Experimental Models}
For the experimental models, I follow \citet{Waseem:2018} in using a Multi-Layered Perceptron model.
The Multi-task MLP architecture that I design (see \cref{fig:mlp_mtl} for a depiction of the model architecture) consists of an input embedding layer which is unique to each task, a shared linear hidden layer, followed by another linear hidden layer that is specific to each task, a linear output layer for each task, and finally the \texttt{softmax} is computed on the model representation.
I also include a dropout layer and a non-linear activation function, where I treat the decision of activation function as an hyper-parameter optimising between the choice of \texttt{ReLU} and \texttt{Tanh} activation functions.

My architecture of the Multi-task MLP deviates from the architecture proposed by \citet{Waseem:2018} in two ways: the choice of input layer and the choice of activation function.
Where I use an embedding layer as the input layer for each task, \citet{Waseem:2018} use a onehot encoded input layer and they use a \texttt{Tanh} activation function for all of their experiments.
Following the experimental approach in \cref{chap:liwc}, I keep the embedding layer randomly initialised rather than using a pre-trained embedding layer.
The motivation for training the embedding layer, even with sparse data, is that pre-trained embeddings have been shown to harbour significant social biases against marginalised communities, a behaviour that is directly oppositional to the aims of abuse detection.

\begin{figure}
  \centering
  \includegraphics[scale=0.75]{mlp_mtl_embedding.jpg}
  \caption{Multi-task Learning MLP.}
  \label{fig:mlp_mtl}
\end{figure}

The optimisation procedure for the Multi-task MLP deviates significantly from the optimisation procedures associated with the baseline models.
For the Multi-task MLP, I optimise my models by giving all tasks an equal weight but distinguish between the main task and the auxiliary tasks by the probability with which a batch from task is chosen.
A task is chosen each time a batch is to be selected, where the primary task is chosen with a probability of $0.6$ when there are two or more auxiliary tasks and $0.7$ when there is only one auxiliary task.
As each task is chosen probabilistically, it is necessary for the probabilities to sum to $1.0$, thus the weight of each auxiliary task is $\frac{1.0-P(M)}{N}$, where $N$ is the total number of auxiliary tasks and $P(M)$ is the probability of the main task being chosen.
Given that I choose the task to be optimised probabilistically, I do not weight the loss as in \citet{Rajamanickam:2020}.
Once a task has been chosen, a batch is selected from the data associated with the task and is passed through the model and the loss on the batch is computed and back-propagated through the network, a process which is repeated for a number of epochs, where the exact number of epochs is a hyper-parameter that I tune.
For single-task models, it is common to iterate over the entire dataset, obtaining a batch count given the size of the dataset and the batch size.
MTL models however are optimised for a number of datasets, including auxiliary task datasets where obtaining a high performance on the auxiliary task may not be of concern, rather learning inductive biases from the data are.
For this reason, I limit the number of batches that are selected in each epoch, setting a global value of $300$ batches per epoch.
Through the use of the probabilities with batches are chosen from each task in conjunction with the number of epochs and the batches being shuffled between each epoch, I ensure that my models gain a representative perspective of each dataset and their labelled data.
These representations of the datasets afford the models the ability to jointly learn representations based on the auxiliary tasks and the primary task.

For my hyper-parameter exploration, I explore the hyper-parameters listed above, that is the number of epochs ($\{50, 100, 200\}$) and the activation function ($\{Tanh, ReLU\}$).
I also perform a hyper-parameter optimisation of the choice of optimisation algorithm ($\{Adam, AdamW,$\\$SGD,ASGD\}$); the dimensionality of the shared layer (${64, 128, 256}$); the learning rate ($[1^{-5}, 1.0]$); the dimensionality of the task-specific hidden layers (${64, 100, 200, 300}$); the dimensionality of the task-specific input layers ($\{64, 100, 200, 300\}$); the value of dropout $[0.0, 0.5]$; and lastly the batch size ($\{16, 32, 64\}$).
Note, that the batch size can have an influence over how much of each dataset is exposed to the model at training time as the number of batches selected per epoch does not scale with the variation in the batch size.

\subsection{Auxiliary Task Configurations}\label{sub:aux_task_selection}
In order to select the auxiliary tasks and their combinations that contribute most towards the performances of the primary, I add auxiliary tasks as they prove useful to the main task in terms of performance boosts.
To perform this selection, I design three different scenarios of auxiliary task configurations:

\begin{enumerate}
  \item Auxiliary tasks consist only of abusive language detection tasks,
  \item auxiliary tasks consist only of non-abusive language detection tasks, and
  \item auxiliary tasks are a combination of abusive language detection tasks and tasks that are not abusive language detection tasks.
\end{enumerate}

I initially experiment with only one auxiliary task and select those that either outperform all baseline models or obtain the highest performances, in the case where some baseline models outperform all experimental models with one auxiliary task.
I then construct experiments with all combinations of the selected auxiliary tasks.

\section{Results}
\zw{Maybe add some introductory words here?}

\subsection{Baseline Models}

In \cref{tab:linear_singletask_params,tab:mlp_singletask_params,tab:ensemble_params} I present the best identified hyper-parameters for each model type.
Focusing on the regulariser, all linear models prefer an $L2$ regulariser, likely because it redistributes the weights of equally important features rather than zeroing any of them out.
Moreover, as observed in \cref{tab:linear_singletask_params} all models prefer a low regularisation strength when a linear single-task classifier is optimised.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cc}
                                            & Regulariser & Regularisation Strength \\\hline
    \textit{Offence} Linear Single task     & L2          & 0.2                     \\
    \textit{Hate Speech} Linear Single task & L2          & 0.1                     \\
    \textit{Toxicity} Linear Single task    & L2          & 0.1
  \end{tabular}
  \caption{Best model parameters for linear single-task models.}
  \label{tab:linear_singletask_params}
\end{table}

For the ensemble classifier a different picture emerges (see \cref{tab:ensemble_params}.
As this model is optimised on a very sparse feature set that consists only of the predictions of the auxiliary task classifiers, it is no surprise that an $L2$ regulariser is preferred.
Moreover, there are indications of a correlation between the dataset size and the strength of the regularisation, with the smallest dataset requiring the greatest regularisation strength ($0.5$ for \textit{Hate Speech}) and the largest dataset requiring the lowest regularisation strength ($0.1$ for \textit{Toxicity}).

\begin{table}[]
  \centering
  \begin{tabular}{l|ccc}
    Dataset                  & Vocabulary Size & Training Documents & # Classes\\\hline
    \textit{Offence}         & 23263           & 19826              & 3\\
    \textit{Hate Speech}     & 19981           & 13525              & 3\\
    \textit{Toxicity}        & 95739           & 95692              & 2\\
    \textit{Hate Expert}     & 12005           & 5527               & 4\\
    \textit{Sarcasm}         & 21159           & 7508               & 2\\
    \textit{Argument Basis}  & 22275           & 8433               & 2\\
    \textit{Moral Sentiment} & 31779           & 27989              & 11
  \end{tabular}
  \caption{Vocabulary sizes for each of the datasets used.}
  \label{tab:aux_vocab_sizes}
\end{table}

This narrative however is complicated by the best parameters found in \cref{tab:ensemble_aux_params}.
Here, the smallest abusive language dataset requires the largest regularisation power while other, in line with the observation on \cref{tab:ensemble_params}.
However, classifiers optimised for the larger \textit{Offence} dataset require more regularisation strength than classifiers optimised the smaller \textit{Hate Speech}.
In tandem, these observations suggest that beyond the size of the dataset in terms of numbers, other factors may influence the strength of the regularisation.
One such potential factor may be the vocabulary size.
Observing the vocabulary sizes in \cref{tab:aux_vocab_sizes}, it appears that vocabulary sizes in conjunction with the dataset sizes may be causes for the regularisation strength for models optimised for the different datasets.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cc}
                                            & Regulariser & Regularisation Strength \\\hline
    \textit{Offence} Aux Classifier         & L2          & 0.2                     \\
    \textit{Hate Speech} Aux Classifier     & L2          & 0.1                     \\
    \textit{Toxicity} Aux Classifier        & L2          & 0.1                     \\
    \textit{Hate Expert} Aux Classifier     & L2          & 0.5                     \\
    \textit{Sarcasm} Aux Classifier         & L2          & 0.1                     \\
    \textit{Argument Basis} Aux Classifier  & L2          & 0.1                     \\
    \textit{Moral Sentiment} Aux Classifier & L2          & 0.1
  \end{tabular}
  \caption{Auxiliary task parameters for ensemble classifier.}
  \label{tab:ensemble_aux_params}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{l|cc}
                                             & Regulariser & Regularisation Strength \\\hline
    \textit{Offence} Ensemble Classifier     & L2          & 0.2                     \\
    \textit{Hate Speech} Ensemble Classifier & L2          & 0.5                     \\
    \textit{Toxicity} Ensemble Classifier    & L2          & 0.1
  \end{tabular}
  \caption{Parameters for the ensemble classifiers.}
  \label{tab:ensemble_params}
\end{table}

Turning to the hyper-parameters for the non-linear baseline in \cref{tab:mlp_singletask_params}, the number of similar and shared values across models optimised for each dataset decreases to share only one parameter, the batch size.
The models optimised for the larger datasets, the \textit{Offence} and \textit{Toxicity} dataset also share a preference for using ReLU as their non-linearity.
Moreover, the baseline models optimised for these two datasets also prefer a higher learning rate compared to the model optimised for the smaller \textit{Hate Speech} dataset.

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccccccc}
                                          & Batch Size & Dropout & Embedding Dim & Epochs & Hidden Dim & Learning Rate & Non-linearity & Optimiser \\\hline
    \textit{Offence} MLP Single Task      & 64         & 0.318   & 300           & 200    & 100        & 0.003586      & ReLU          & SGD       \\
    \textit{Hate Speech} MLP Single Task  & 64         & 0.1458  & 300           & 100    & 100        & 0.0007246     & Tanh          & AdamW     \\
    \textit{Toxicity MLP} Single Task     & 64         & 0.1978  & 200           & 50     & 200        & 0.006056      & ReLU          & Adam
  \end{tabular}%
  }
  \caption{Best hyper parameters for non-linear single task model for each main task dataset.}
  \label{tab:mlp_singletask_params}
\end{table}

\subsubsection{Baseline Model Performances}
\zw{Update baseline results}
Prior to an analysis of the baseline model performances on the evaluation set, I examine their performances on the validation set to gain an insight in the viability of the modelling approach and expected outcomes on the evaluation data.
Considering the results for all baseline models in \cref{tab:baseline_dev_wh,tab:baseline_dev_wulczyn,tab:baseline_dev_davidson} it is immediately clear that ensemble models provide a poor method for identifying each form of abuse.
Additionally, the linear SVM baseline models provide for good baselines to compare the experimental models with, as the SVM baselines tend to out-perform the non-linear baselines.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cccc}
               & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    Linear SVM & \textbf{0.8515} & \textbf{0.8239} & \textbf{0.7741} & \textbf{0.7962} \\
    Ensemble   & 0.8493          & 0.2123          & 0.2500          & 0.2296          \\
    MLP        & 0.8117          & 0.7117          & 0.7737          & 0.7378
  \end{tabular}
  \caption{Baseline validation scores on the \textit{Hate Speech} dataset.}
  \label{tab:baseline_dev_wh}
\end{table}

The exception to this pattern is provided by the MLP optimised for the \textit{Toxicity} dataset, where the MLP baseline outperforms the SVM baseline in terms of recall and F1-score.
For all datasets, the MLP classifiers show a drop in precision on the development set, suggesting that while they may be comparable in terms of recall, the MLP models tend to misclassify into the positive class at a greater rate than the negative classes.

Observing the results for the baseline models optimised for the \textit{Hate Speech} dataset in \cref{tab:baseline_dev_wh}, the largest drop in performance occurs for the positive classes when using the MLP.
The relatively smaller drop in accuracy is aligned with that the positive classes are minority classes, thus a performance drop in the positive classes has a small impact as the relative number of misclassification remains small.
For the ensemble, the negligible drop in accuracy, in comparison to precision and recall, suggests that although the performance on precision and recall are abysmal, the largest performance drop happens into the positive classes.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cccc}
               & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    Linear SVM & \textbf{0.8898} & \textbf{0.7224} & \textbf{0.7107} & \textbf{0.8661} \\
    Ensemble   & 0.7744          & 0.2581          & 0.3333          & 0.2910          \\
    MLP        & 0.8708          & 0.6581          & 0.7024          & 0.6773
  \end{tabular}
  \caption{Baseline validation scores on the \textit{Offence} dataset.}
  \label{tab:baseline_dev_davidson}
\end{table}

For the baseline models optimised for the \textit{Offence} dataset on the other hand, the accuracy score reveals a different performance drop.
In this dataset, the `offence' class is the majority class, thus the accuracy obtained provides insight into how well the models predict into that class.
The drop in precision is therefore likely to primarily occur in the other two classes in the dataset, one of which, the `hate' class, also being a positive class.
The model scores here tell a story of misclassifications primarily in the negative class and the positive `hate' class.

\begin{table}[h]
  \centering
  \begin{tabular}{l|cccc}
                & Accuracy        & Precision       & Recall          & F1-score        \\ \hline
    Linear SVM  & \textbf{0.9570} & \textbf{0.8967} & 0.8411          & 0.7151          \\
    Ensemble    & 0.9045          & 0.4522          & 0.5000          & 0.4749          \\
    MLP         & 0.9480          & 0.8145          & \textbf{0.8671} & \textbf{0.8382}
  \end{tabular}
  \caption{Baseline validation scores on the \textit{Toxicity} dataset.}
  \label{tab:baseline_dev_wulczyn}
\end{table}

Finally, the models optimised for the \textit{Toxicity} dataset are the only ones where the non-linear baseline outperforms the linear SVM.
This dataset is developed for binary classification on an imbalanced dataset, where the minority class is the positive class.
Thus, the negligible drops in accuracy provide information into the ability of the models to predict into negative class.
The MLP baseline optimised for the \textit{Toxicity} classifier provides a stronger performance on the recall, meaning that it has an improved ability in correctly identifying the data that does not belong in the positive class compared to the linear SVM.

On the basis of the model performances on the validation sets, we can expect that the ensemble models will uniformly under-perform on the evaluation data while the MLP models provide a competitive, but lower, performance than the linear SVM baselines.
The primary performance drop for the MLP models is likely to be in their precision, that is their ability to classify into the positive classes.\vspace{5mm}

Turning to the performances of the baseline models on the evaluation set, the model performances and the patterns remain mostly stable between the validation and the evaluation set across the datasets: the linear SVM out-perform all other models in most cases and the ensemble models mostly post poor classification performances.
In addition, the linear SVM models tend to perform best in terms of precision, with the MLP models obtaining a lower precision score.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll}
             & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    Linear   & \textbf{0.8440} & \textbf{0.8182} & 0.7671          & \textbf{0.7892} \\
    Ensemble & 0.6803          & 0.2268          & 0.3333          & 0.2699          \\
    MLP      & 0.8056          & 0.6686          & \textbf{0.7894} & 0.7132
  \end{tabular}
  \caption{Baseline model evaluation set performances on the \textit{Hate Speech} dataset.}
  \label{tab:baseline_test_wh}
\end{table}

Within the performances for each dataset there are some discrepancies between the performances on the validation and the evaluation sets.
Unlike for the validation set, the MLP models optimised for the \textit{Hate Speech} dataset obtain a higher recall score on the evaluation set (see \cref{tab:baseline_test_wh}) than the linear SVM.
This suggests that the MLP baseline is better suited for correctly identifying the negative class while the linear SVM is better suited for identifying the positive classes.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll}
             & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    Linear   & \textbf{0.8871} & \textbf{0.6997} & 0.6789          & \textbf{0.6850} \\
    Ensemble & 0.7713          & 0.4115          & 0.3696          & 0.3622          \\
    MLP      & 0.8790          & 0.5625          & \textbf{0.9163} & 0.5721
  \end{tabular}
  \caption{Baseline model evaluation set performances on the \textit{Offence} dataset.}
  \label{tab:baseline_test_davidson}
\end{table}

Further discrepancies are found for the models optimised for the \textit{Offence} dataset.
On the validation set, the linear SVM outperformed all other models across all metrics.
On the evaluation data however, the MLP baseline outperforms the linear SVM in terms of recall with a $0.2$ increase (see \cref{tab:baseline_test_davidson}).
This increase is obtained while there is a decrease in precision of $0.09$.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll}
                          & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    Linear                & \textbf{0.9582} & \textbf{0.9008} & 0.8450          & \textbf{0.8702} \\
    Ensemble\footnotemark & 0.9582          & 0.9008          & 0.8448          & 0.8701          \\
    MLP                   & 0.9397          & 0.6979          & \textbf{0.9359} & 0.7632
  \end{tabular}
  \caption{Baseline model evaluation set performances on the \textit{Toxicity} dataset.}
  \label{tab:baseline_test_wulczyn}
\end{table}
\footnotetext{The accuracy and precision for the ensemble classifiers have a lower performance, however rounding up to represent the performances by 4 decimal points creates the illusion of identical performance.}

The largest discrepancy between the validation set and evaluation set (see \cref{tab:baseline_test_wulczyn}) however is found in the ensemble baseline optimised for the \textit{Toxicity} dataset.
Here the ensemble classifier obtains a competitive classification performance across metrics to the linear SVM.
While the ensemble baseline breaks with the pattern observed on the validation set, the MLP baseline does not.
Similarly to its' performances on the validation data, the MLP is competitive with the linear SVM in terms of accuracy and posts poorer performance in terms of precision.
Meanwhile, the MLP baseline outperforms all other models in terms of recall however the poor performance in terms of precision results in a F1-score that is not competitive with the other baseline models.

% \zw{Show which models contribute most to ensemble models}
\subsection{Experimental Model Performances}
As the experimental models are MLP models that have been adapted for Multi-Task Learning, the experimental models are expected to \textit{at least} out-perform the MLP baselines as they share a similar architecture.
The best hyper-parameter settings for each of the experimental settings for each main task are shown in \cref{tab:mtl_params_wh,tab:mtl_params_davidson,tab:mtl_params_wulczyn}.\footnote{Note that the `Aux Task Weight' column only contains a single value as each auxiliary task is given the same weight.}
In these tables, I show the best hyper-parameter settings for the models with one auxiliary task and each of the subsequent dataset configurations selected based.
The dataset configurations are chosen on the basis of their F1-score performance on the validation data where only the main task and a single auxiliary task (see further detail on dataset combination selection in \cref{sub:aux_task_selection}).

Observing the best hyper-parameters identified for each dataset in \cref{tab:mtl_params_wh,tab:mtl_params_davidson,tab:mtl_params_wulczyn} three salient attributes are immediately clear.
The vast majority of the models prefer a ReLU for their non-linearity and some variant of Stochastic Gradient Descent as the optimisation function.
Moreover, a majority of models prefer a batch size of $64$, the largest option for batch size that I experiment with.

\begin{landscape}
  \begin{table}[]
    \centering
    \resizebox{0.7\paperheight}{!}{%
    \begin{tabular}{l|ccccccccccc}
      Aux                                               & Batch Size & Main Task Weight & Aux Task Weights & Dropout & Embedding Dim & Epochs & Hidden Dims     & Learning Rate & Non-linearity & Optimiser & Shared Dim \\ \hline
      Hate Expert                                       & 64         & 0.7              & 0.3              & 0.3705  & 200           & 50     & 200,200         & 0.9084        & ReLU          & ASGD      & 128        \\
      Toxicity                                          & 64         & 0.7              & 0.3              & 0.02884 & 100           & 100    & 64,64           & 0.3873        & Tanh          & ASGD      & 128        \\
      Offence                                           & 32         & 0.7              & 0.3              & 0.4568  & 64            & 100    & 200,200         & 0.2846        & ReLU          & ASGD      & 256        \\
      Moral Sentiment                                   & 64         & 0.7              & 0.3              & 0.1954  & 300           & 100    & 100,100         & 0.06402       & ReLU          & ASGD      & 256        \\
      Sarcasm                                           & 64         & 0.7              & 0.3              & 0.4534  & 300           & 100    & 300,300         & 0.3894        & ReLU          & ASGD      & 128        \\
      Argument Basis                                    & 64         & 0.7              & 0.3              & 0.1556  & 100           & 50     & 100,100         & 0.4948        & ReLU          & ASGD      & 256        \\ \hline
      Hate Expert | Offence                             & 64         & 0.6              & 0.2              & 0.3703  & 200           & 50     & 200,200,200     & 0.9429        & ReLU          & ASGD      & 128        \\
      Hate Expert | Moral Sentiment                     & 64         & 0.6              & 0.2              & 0.311   & 100           & 50     & 300,300,300     & 0.6185        & ReLU          & ASGD      & 128        \\
      Offence | Moral Sentiment                         & 16         & 0.6              & 0.2              & 0.1408  & 300           & 100    & 64,64,64        & 0.1237        & Tanh          & SGD       & 128        \\
      Sarcasm | Hate Expert                             & 64         & 0.6              & 0.2              & 0.3054  & 100           & 50     & 64,64,64        & 0.06252       & ReLU          & SGD       & 64         \\
      Sarcasm | Offence                                 & 16         & 0.6              & 0.2              & 0.4576  & 300           & 200    & 100,100,100     & 0.2276        & ReLU          & ASGD      & 256        \\
      Sarcasm | Moral Sentiment                         & 64         & 0.6              & 0.2              & 0.3586  & 300           & 50     & 100,100,100     & 0.3822        & ReLU          & ASGD      & 64         \\
      Hate Expert  | Offence | Moral Sentiment          & 16         & 0.6              & 0.133333333      & 0.414   & 200           & 100    & 300,300,300,300 & 0.8435        & ReLU          & ASGD      & 256        \\
      Sarcasm | Hate Expert | Moral Sentiment           & 32         & 0.6              & 0.133333333      & 0.152   & 64            & 100    & 200,200,200,200 & 0.3459        & ReLU          & ASGD      & 256        \\
      Sarcasm | Offence | Moral Sentiment               & 64         & 0.6              & 0.133333333      & 0.05853 & 300           & 100    & 64,64,64,64     & 0.04528       & ReLU          & ASGD      & 64         \\
      Sarcasm | Hate Expert | Offence | Moral Sentiment & 16         & 0.6              & 0.1              & 0.143   & 64            & 100    & 64,64,64,64,64  & 0.2368        & ReLU          & ASGD      & 256
    \end{tabular}%
    }
    \caption{Hyper-parameters for best performing MTL models optimised on the \textit{Hate Speech} dataset.}
    \label{tab:mtl_params_wh}
  \end{table}
  \vfill
  \begin{table}[]
    \centering
    \resizebox{0.7\paperheight}{!}{%
    \begin{tabular}{l|ccccccccccc}
      Aux                                               & Batch Size & Main Task Weight & Aux Task Weights & Dropout & Embedding Dim & Epochs & Hidden Dims     & Learning Rate & Non-linearity & Optimiser & Shared Dim \\ \hline
      Hate Expert                                       & 32         & 0.7              & 0.3              & 0.06554 & 100           & 200    & 300,300         & 0.2921        & ReLU          & ASGD      & 256        \\
      Hate Speech                                       & 64         & 0.7              & 0.3              & 0.1785  & 100           & 50     & 100,100         & 0.281         & ReLU          & SGD       & 128        \\
      Toxicity                                          & 64         & 0.7              & 0.3              & 0.4405  & 300           & 50     & 200,200         & 0.3469        & ReLU          & ASGD      & 256        \\
      Sarcasm                                           & 32         & 0.7              & 0.3              & 0.1476  & 300           & 50     & 200,200         & 0.9616        & ReLU          & ASGD      & 128        \\
      Argument Basis                                    & 16         & 0.7              & 0.3              & 0.2952  & 300           & 100    & 64,64           & 0.4306        & ReLU          & ASGD      & 64         \\
      Moral Sentiment                                   & 32         & 0.7              & 0.3              & 0.1953  & 200           & 100    & 300,300         & 0.1415        & ReLU          & SGD       & 64         \\ \hline
      Hate Speech | Toxicity                            & 64         & 0.6              & 0.2              & 0.1663  & 100           & 50     & 100,100,100     & 0.3764        & ReLU          & SGD       & 256        \\
      Sarcasm | Hate Speech                             & 32         & 0.6              & 0.2              & 0.1497  & 100           & 100    & 300,300,300     & 0.2229        & ReLU          & ASGD      & 256        \\
      Sarcasm | Toxicity                                & 32         & 0.6              & 0.2              & 0.166   & 64            & 100    & 300,300,300     & 0.511         & ReLU          & ASGD      & 64         \\
      Argument Basis | Hate Speech                      & 64         & 0.6              & 0.2              & 0.0265  & 100           & 200    & 200,200,200     & 0.4188        & ReLU          & ASGD      & 256        \\
      Argument Basis | Toxicity                         & 64         & 0.6              & 0.2              & 0.3497  & 300           & 100    & 200,200,200     & 0.3466        & ReLU          & ASGD      & 128        \\
      Argument Basis | Sarcasm                          & 64         & 0.6              & 0.2              & 0.4527  & 200           & 100    & 64,64,64        & 0.509         & ReLU          & ASGD      & 256        \\
      Sarcasm | Toxicity | Hate Speech                  & 64         & 0.6              & 0.133333333      & 0.4113  & 300           & 100    & 200,200,200,200 & 0.1113        & ReLU          & ASGD      & 256        \\
      Argument Basis | Hate Speech | Toxicity           & 16         & 0.6              & 0.133333333      & 0.2439  & 200           & 200    & 100,100,100,100 & 0.8852        & ReLU          & ASGD      & 64         \\
      Argument Basis | Hate Speech | Sarcasm            & 64         & 0.6              & 0.133333333      & 0.3725  & 200           & 200    & 200,200,200,200 & 0.3176        & ReLU          & ASGD      & 64         \\
      Argument Basis | Toxicity | Sarcasm               & 64         & 0.6              & 0.133333333      & 0.259   & 64            & 100    & 300,300,300,300 & 0.6679        & ReLU          & ASGD      & 128        \\
      Argument Basis | Hate Speech | Toxicity | Sarcasm & 64         & 0.6              & 0.1              & 0.0103  & 64            & 100    & 64,64,64,64,64  & 0.4785        & ReLU          & ASGD      & 64
    \end{tabular}%
    }
    \caption{Hyper-parameters for best performing MTL models optimised on the \textit{Offence} dataset.}
    \label{tab:mtl_params_davidson}
  \end{table}
\end{landscape}

\begin{landscape}
  \begin{table}[]
    \centering
    \resizebox{0.7\paperheight}{!}{%
    \begin{tabular}{l|ccccccccccc}
      Aux                                                                & Batch Size & Main Task Weight & Aux Task Weights & Dropout & Embedding Dim & Epochs & Hidden Dims             & Learning Rate & Non-linearity & Optimiser & Shared Dim \\ \hline
      Waseem                                                             & 64         & 0.7              & 0.3              & 0.1166  & 300           & 200    & 64,64                   & 0.4055        & ReLU          & ASGD      & 256        \\
      Hate Speech                                                        & 64         & 0.7              & 0.3              & 0.3346  & 100           & 50     & 300,300                 & 0.003051      & ReLU          & AdamW     & 256        \\
      Offence                                                            & 64         & 0.7              & 0.3              & 0.03044 & 300           & 200    & 200,200                 & 0.3432        & ReLU          & SGD       & 128        \\
      Moral Sentiment                                                    & 64         & 0.7              & 0.3              & 0.3558  & 300           & 200    & 64,64                   & 0.7853        & ReLU          & ASGD      & 256        \\
      Sarcasm                                                            & 64         & 0.7              & 0.3              & 0.3711  & 200           & 200    & 100,100                 & 0.9143        & ReLU          & ASGD      & 256        \\
      Argument Basis                                                     & 64         & 0.7              & 0.3              & 0.3995  & 300           & 50     & 200,200                 & 0.5104        & ReLU          & SGD       & 128        \\ \hline
      Hate Speech | Offence                                              & 64         & 0.6              & 0.2              & 0.03108 & 300           & 200    & 100,100,100             & 0.2465        & ReLU          & SGD       & 256        \\
      Moral Sentiment | Hate Speech                                      & 64         & 0.6              & 0.2              & 0.3406  & 300           & 200    & 200,200,200             & 0.2291        & ReLU          & SGD       & 64         \\
      Moral Sentiment | Offence                                          & 64         & 0.6              & 0.2              & 0.08389 & 200           & 200    & 64,64,64                & 0.8606        & ReLU          & ASGD      & 128        \\
      Sarcasm | Moral Sentiment                                          & 64         & 0.6              & 0.2              & 0.2051  & 200           & 200    & 300,300,300             & 0.8602        & ReLU          & ASGD      & 64         \\
      Sarcasm | Hate Speech                                              & 64         & 0.6              & 0.2              & 0.2315  & 300           & 200    & 300,300,300             & 0.2219        & ReLU          & SGD       & 128        \\
      Argument Basis | Moral Sentiment                                   & 64         & 0.6              & 0.2              & 0.01288 & 200           & 200    & 300,300,300             & 0.968         & ReLU          & ASGD      & 256        \\
      Argument Basis | Sarcasm                                           & 64         & 0.6              & 0.2              & 0.061   & 200           & 200    & 300,300,300             & 0.384         & ReLU          & SGD       & 256        \\
      Argument Basis | Hate Speech                                       & 64         & 0.6              & 0.2              & 0.09094 & 200           & 200    & 300,300,300             & 0.8456        & ReLU          & ASGD      & 128        \\
      Argument Basis | Offence                                           & 64         & 0.6              & 0.2              & 0.2733  & 300           & 100    & 100,100,100             & 0.3757        & ReLU          & SGD       & 256        \\
      Sarcasm | Offence                                                  & 64         & 0.6              & 0.2              & 0.02304 & 100           & 200    & 64,64,64                & 0.804         & ReLU          & ASGD      & 256        \\
      Moral Sentiment | Hate Speech | Offence                            & 64         & 0.6              & 0.133333333      & 0.3018  & 200           & 200    & 300,300,300,300         & 0.9543        & ReLU          & ASGD      & 256        \\
      Sarcasm | Moral Sentiment | Hate Speech                            & 64         & 0.6              & 0.133333333      & 0.2762  & 200           & 100    & 300,300,300,300         & 0.4007        & ReLU          & SGD       & 128        \\
      Sarcasm | Moral Sentiment | Offence                                & 64         & 0.6              & 0.133333333      & 0.2939  & 64            & 200    & 200,200,200,200         & 0.8591        & ReLU          & ASGD      & 64         \\
      Argument Basis | Moral Sentiment | Sarcasm                         & 64         & 0.6              & 0.133333333      & 0.03792 & 300           & 200    & 300,300,300,300         & 0.934         & ReLU          & ASGD      & 64         \\
      Argument Basis | Moral Sentiment | Hate Speech                     & 64         & 0.6              & 0.133333333      & 0.05325 & 300           & 200    & 300,300,300,300         & 0.4349        & ReLU          & SGD       & 64         \\
      Argument Basis | Moral Sentiment | Offence                         & 64         & 0.6              & 0.133333333      & 0.1082  & 300           & 200    & 100,100,100,100         & 0.3236        & ReLU          & SGD       & 128        \\
      Sarcasm | Moral Sentiment | Hate Speech | Offence                  & 32         & 0.6              & 0.1              & 0.1936  & 300           & 50     & 100,100,100,100,100     & 0.004907      & ReLU          & AdamW     & 64         \\
      Argument Basis | Moral Sentiment | Sarcasm | Hate Speech           & 64         & 0.6              & 0.1              & 0.06104 & 100           & 200    & 300,300,300,300,300     & 0.565         & ReLU          & SGD       & 256        \\
      Argument Basis | Moral Sentiment | Sarcasm | Offence               & 64         & 0.6              & 0.1              & 0.2149  & 300           & 200    & 100,100,100,100,100     & 0.6336        & ReLU          & SGD       & 128        \\
      Argument Basis | Moral Sentiment | Sarcasm | Offence | Hate Speech & 64         & 0.6              & 0.08             & 0.2348  & 100           & 100    & 200,200,200,200,200,200 & 0.546         & ReLU          & SGD       & 256
    \end{tabular}%
    }
    \caption{Hyper-parameters for best performing MTL models optimised on the \textit{Toxicity} dataset.}
    \label{tab:mtl_params_wulczyn}
  \end{table}
\end{landscape}

While most models prefer a version of stochastic gradient descent, two MTL models optimise for the \textit{Toxicity} dataset prefer the AdamW optimiser (see \cref{tab:mtl_params_wulczyn}).

% TODO You are here. Do dev set and test set evaluation analysis here

\begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                      & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    \textit{Hate Expert}                              & 0.8171          & 0.7463          & 0.7776          & \textit{0.7608} \\
    \textit{Offence}                                  & 0.8141          & \textbf{0.7518} & 0.7609          & \textit{0.7558} \\
    Toxicity                                          & 0.8082          & 0.7433          & 0.7562          & 0.7495          \\
    \textit{Moral Sentiment}                          & 0.8242          & 0.7462          & 0.7865          & \textit{0.7644} \\
    Argument Basis                                    & 0.8094          & 0.7374          & 0.7638          & 0.7494          \\
    \textit{Sarcasm}                                  & 0.8194          & 0.7259          & 0.7944          & \textit{0.7551} \\\hline
    Hate Expert | Moral Sentiment                     & 0.8212          & 0.7220          & 0.7996          & 0.7540          \\
    Offence | Moral Sentiment                         & 0.7957          & 0.7552          & 0.7408          & 0.7476          \\
    Hate Expert | Offence                             & 0.82            & 0.7521          & 0.7781          & 0.7623          \\
    Sarcasm | Hate Expert                             & 0.8171          & 0.7068          & 0.7994          & 0.7431          \\
    Sarcasm | Offence                                 & 0.8141          & 0.7284          & 0.7731          & 0.7449          \\
    Sarcasm | Moral Sentiment                         & \textbf{0.8289} & 0.7425          & 0.8009          & \textbf{0.7664} \\
    Hate Expert | Offence | Moral Sentiment           & 0.8165          & 0.7212          & 0.7913          & 0.7497          \\
    Sarcasm | Hate Expert | Moral Sentiment           & 0.8259          & 0.7279          & \textbf{0.8051} & 0.7566          \\
    Sarcasm | Offence | Moral Sentiment               & 0.8194          & 0.7204          & 0.7916          & 0.7497          \\
    Sarcasm | Hate Expert | Offence | Moral Sentiment & 0.8200          & 0.7389          & 0.7834          & 0.7561
  \end{tabular}%
  }
  \caption{Experimental model validation scores on the \textit{Hate Speech} dataset. Configurations written in \textit{italic} signify the auxiliary tasks chosen for further exploration and \textbf{bolded} scores indicate the highest performances.}
  \label{tab:mtl_dev_wh}
\end{table}

\begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                      & Accuracy        & Precision & Recall          & F1-score        \\ \hline
    \textit{Hate Speech}                              & 0.8857          & 0.7422    & 0.7076          & \textit{0.7232} \\
    \textit{Toxicity}                                 & 0.8962          & 0.7124    & 0.7562          & \textit{0.7286} \\
    Hate Expert                                       & 0.8882          & 0.7122    & 0.7042          & 0.7069          \\
    \textit{Sarcasm}                                  & 0.8845          & 0.7503    & 0.7149          & \textit{0.7312} \\
    \textit{Argument Basis}                           & 0.8987          & 0.7039    & 0.7489          & \textit{0.7164} \\
    Moral Sentiment                                   & 0.8853          & 0.7237    & 0.7020          & 0.7105          \\
    Hate Speech | Toxicity                            & 0.8914          & 0.7381    & 0.7251          & 0.7309          \\
    Sarcasm | Hate Speech                             & \textbf{0.9023} & 0.7005    & \textbf{0.7619} & 0.7167          \\
    Sarcasm | Toxicity                                & 0.8954          & 0.7367    & 0.7361          & \textbf{0.7349} \\
    Argument Basis | Hate Speech                      & 0.8862          & 0.7018    & 0.7136          & 0.7069          \\
    Argument Basis | Toxicity                         & 0.8958          & 0.7076    & 0.7449          & 0.7218          \\
    Argument Basis | Sarcasm                          & 0.8906          & 0.7126    & 0.7194          & 0.7098          \\
    Sarcasm | Toxicity | Hate Speech                  & 0.8979          & 0.7338    & 0.7449          & 0.7345          \\
    Argument Basis | Hate Speech | Toxicity           & 0.8765          & 0.7226    & 0.6992          & 0.7103          \\
    Argument Basis | Hate Speech | Sarcasm            & 0.8950          & 0.6952    & 0.7354          & 0.7069          \\
    Argument Basis | Toxicity | Sarcasm               & 0.8991          & 0.7122    & 0.7481          & 0.7211          \\
    Argument Basis | Hate Speech | Toxicity | Sarcasm & 0.8902          & 0.6977    & 0.7281          & 0.7079
  \end{tabular}%
  }
  \caption{Experimental model validation scores on the \textit{Offence} dataset. Configurations written in \textit{italic} signify the auxiliary tasks chosen for further exploration and \textbf{bolded} scores indicate the highest performances.}
  \label{tab:mtl_dev_davidson}
\end{table}

\begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{l|cccc}
                                                                       & Accuracy        & Precision       & Recall          & F1-score        \\\hline
    \textit{Hate Speech}                                               & 0.9513          & 0.8326          & 0.8726          & \textit{0.8511} \\
    \textit{Offence}                                                   & 0.9490          & \textbf{0.8523} & 0.8524          & \textit{0.8524} \\
    Hate Expert                                                        & 0.9499          & 0.8001          & 0.8881          & 0.8370          \\
    \textit{Moral Sentiment}                                           & 0.9528          & 0.8165          & 0.8918          & \textit{0.8491} \\
    \textit{Sarcasm}                                                   & 0.9511          & 0.8130          & 0.8846          & \textit{0.8441} \\
    \textit{Argument Basis}                                            & 0.9534          & 0.8195          & 0.8932          & \textit{0.8515} \\
    Moral Sentiment | Offence                                          & 0.9517          & 0.8158          & 0.8862          & 0.8465          \\
    Moral Sentiment | Hate Speech                                      & 0.9555          & 0.8167          & 0.9081          & 0.8551          \\
    Hate Speech | Offence                                              & 0.9504          & 0.8386          & 0.8652          & 0.8512          \\
    Sarcasm | Moral Sentiment                                          & 0.9533          & 0.8008          & 0.9093          & \textbf{0.8575} \\
    Sarcasm | Hate Speech                                              & \textbf{0.9558} & 0.8191          & 0.9079          & 0.8566          \\
    Sarcasm | Offence                                                  & 0.9522          & 0.7957          & 0.9068          & 0.8469          \\
    Argument Basis | Hate Speech                                       & 0.9521          & 0.8148          & 0.8889          & 0.8421          \\
    Argument Basis | Moral Sentiment                                   & 0.9538          & 0.8101          & 0.9038          & 0.8492          \\
    Argument Basis | Sarcasm                                           & 0.9535          & 0.8376          & 0.8808          & \textit{0.8575} \\
    Argument Basis | Offence                                           & 0.9542          & 0.8093          & 0.9066          & 0.8496          \\
    Moral Sentiment | Hate Speech | Offence                            & 0.9529          & 0.7956          & 0.9121          & 0.8419          \\
    Sarcasm | Moral Sentiment | Hate Speech                            & 0.9533          & 0.7938          & 0.917           & 0.8402          \\
    Sarcasm | Moral Sentiment | Offence                                & 0.9523          & 0.7861          & \textbf{0.9184} & 0.8368          \\
    Argument Basis | Moral Sentiment | Sarcasm                         & 0.9526          & 0.8214          & 0.8869          & 0.8503          \\
    Argument Basis | Moral Sentiment | Hate Speech                     & 0.9516          & 0.8469          & 0.8661          & 0.8562          \\
    Argument Basis | Moral Sentiment | Offence                         & 0.953           & 0.8248          & 0.8867          & 0.8523          \\
    Sarcasm | Moral Sentiment | Hate Speech | Offence                  & 0.9548          & 0.8117          & 0.9086          & 0.8519          \\
    Argument Basis | Moral Sentiment | Sarcasm | Hate Speech           & 0.9514          & 0.8315          & 0.8740          & 0.8511          \\
    Argument Basis | Moral Sentiment | Sarcasm | Offence               & 0.9517          & 0.8098          & 0.8905          & 0.8442          \\
    Argument Basis | Moral Sentiment | Sarcasm | Offence | Hate Speech & 0.9534          & 0.8039          & 0.9067          & 0.8459
  \end{tabular}%
  }
  \caption{Experimental model validation scores on the \textit{Toxicity} dataset. Configurations written in \textit{italic} signify the auxiliary tasks chosen for further exploration and \textbf{bolded} scores indicate the highest performances.}
  \label{tab:mtl_dev_wulczyn}
\end{table}

\zw{Identify which models will be experimental models once good results come out.}

\zw{Describe the models and experimental settings}

\zw{Add results and start by rough analysis of just numbers}
\zw{Add plots}
\zw{Look at predictions in detail, try to identify where they still fail}

\section{Conclusions}

Future work:
\zw{Finer values of regularisation strength for linear regularisers}
\zw{Explore weighting of each task}
\zw{Treat Weighting of tasks as an optimisable entity}
\zw{Use pre-trained embeddings}


