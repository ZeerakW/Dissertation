%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

% !TEX root = ../thesis.tex
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\chapter{Introduction}\label{chap:intro}
%zw{Rough contents here, needs updating to fit with narrative of the thesis and polishing of text}

%Structure:
%Introduction to the field in general
%Narrow down to the issues from the different chapters
%- Tie together how different chapters highlight different shades of the same problem
%- Add research questions to all chapters that don't have explicit research questions stated.
%- - Chapter arguments:
%- - - Chap 4 (Dirt): Abuse detection is broken because it operates with trying to classify uncertain boundaries
%- - - Chap 5 (LIWC): If the boundary is porous then maybe we're looking at the wrong problem, maybe the right problem is what people mean
%- - - Chap 6 (MTL): If we must look at dirt, then maybe we need to look more broadly to understand how people are trying to communicate
%- - - Chap 7 (Disembodied): But maybe this is all wrong if what we want to do is protect people but we're actually harming them.
%- - - Chap 8 (Conclusion): Well, maybe we need to rethink this all, but some components of rethinking it are in thinking about the things around the abuse, not the abuse itself.
%Add text about WOAH and WiNLP
%Provide structure of thesis

As people consume, interact with, and create \ZTdelete{media} online \ZTedit{media} at an ever growing rate, it becomes \ZTdelete{necessary to consider} \ZTedit{increasingly important to carefully} the content with which they interact, how this influences and impacts audiences and the cultures that are formed in online spaces.
\ZTedit{Online abuse is a small but prominent portion of online communication, that is expressed through various forms}
\ZTdelete{A small, in terms of absolute amount of content created, but prominent portion of online communications is online abuse in various forms}, including cyber-bullying, hate speech, harassment, aggressive speech, and offensive speech \citep{Vidgen:2019}.
\ZTedit{Extended exposure to such content can have adverse effects for users engagement in online spaces \citep{Leslie_jones}, lead to psychological harms to the targets of abuse \citep{Gelber_McNamara_2016}, and be a factor in increases in hate crimes \citep{Muller:2020}.}\ZTdelete{Such content can lead to users disengaging with online spaces \citep{Leslie_jones}, psychological harms on the targets of abuse and other people who are exposed to it \citep{Gelber_McNamara_2016}, and an increase in hate crimes \citep{Muller:2020}.}
As a result, social media platforms have long been subject to pressure \ZTdelete{from users and regulators} to moderate and remove such content \ZTedit{from users and regulators alike}.
\ZTdelete{This issue of} \ZTedit{Moderating, removing, and adjudicating content in online spaces} \ZTdelete{content moderation} has traditionally been a human effort \citep{Roberts:2019} \ZTedit{however as computational methods such as machine learning have matured, social media platforms such as Facebook have increasingly come to rely on such automated methods \citep{FacebookReporting}.} \ZTdelete{but has increasingly become reliant on computational methods \ZTedit{, as these have matured and regulatory pressure has increased \citep{FacebookReporting}.}}\ZTdelete{for detecting and adjudicating online abuse \citep{FacebookReporting}}.
These automated content moderation systems developed by commercial platforms operate across several different modalities, including images, text, and videos.
Along with this increase in automated content moderation systems developed by commercial entities, there has been an increased in attention to the challenge of developing automated systems for detecting text-based abuse in the Natural Language Processing (NLP) community.
Beyond the NLP community, there has been sustained academic attention to the challenges faced by content moderation systems, both human and automated.
This body of work has spanned across a wide variety of disciplines including, but not limited to, media studies \citep{Gillespie:2020,Gerrard:2020}, archival research \citep{Thylstrup:2019,Agostinho:2019}, and legal studies \citep{Cobbe:2020,Llanso:2020}.
While \ZTedit{commercial content moderation systems that are developed by social media platforms often span across modalities, I specifically focus on text-based content moderation systems, i.e. NLP systems, for online abuse in this dissertation.}\ZTdelete{the content that is moderated spans several modalities and is often multi-modal, in this dissertation I focus on text-based content moderation systems for online abuse, in specific NLP systems.}

\ZTedit{The academic inquiry into content moderations from the NLP community has identified several challenges with machinic content moderation systems, including poor generalisation of optimised models}
\ZTdelete{In the academic inquiry into content moderation systems conducted by the NLP community, several challenges have been identified in model performances, e.g. poor ability to generalise} onto new data and contexts \citep{Waseem:2016,Fortuna:2021}, \ZTedit{socially discriminatory model predictions against already-marginalised communities \citep{Davidson:2019,Dias:2021}, poor ability to understand in-group communicative norms \citep{Dias:2021}, annotation biases in the data creation process \citep{Waseem:2018,Davidson:2019}.
\ZTdelete{how machine learning models for abuse detection discriminate against marginalised communities \citep{Davidson:2019,Dias:2021}, misclassifications that occur as a product of in-group communicative norms \citep{Dias:2021}, human annotator bias in the data creation process \citep{Waseem:2018,Davidson:2019}.}
Beyond the challenges in model performance, several key concerns have been raised about the data creation process. For instance, \citet{Wiegand:2019} and \citet{Vidgen-Derczynski:2020} raise concerns about contemporary methods for collecting data, while \citet{Waseem:2017,Waseem:2018} point out that there is an incompatibility between widely used definitions, \citet{Vidgen-Derczynski:2020} further highlight how developing resources for English Language abuse detection has been over-emphasised by the NLP research community.
Finally, applying both to modelling and data creation, the processes in the field are unable to situate speech within the context and communicative intents of the speakers \citep{Bender:2021,Dinan:2020}.}
\ZTdelete{challenges in collecting data for abuse \citep{Wiegand:2019,Vidgen-Derczynski:2020}, how abuse is defined \citep{Waseem:2017}, the over-emphasis on English in research contributions \citep{Vidgen-Derczynski:2020}, and situating speech within the context and the communicative intents of speakers \citep{Bender:2021,Dinan:2020}.}
Although each of these challenges are of equal importance and each require urgent attention, I focus my efforts \ZTdelete{in this thesis on four different topics.}\ZTedit{first, on considering the socio-political consequences of content moderation technologies and aim to explain which modelling decisions may influence discriminatory outcomes.
Second, I seek to connect the consequences of contemporaneous methods for content moderation technologies and develop new techniques for modelling abuse that aim to address some of these concerns.
These twin objectives thus collectively seek to highlight the ways in which content moderation technologies are built to intrinsically optimise for white respectability politics through the political economies that they are created within (and reinforce), and the technical means of optimisation that prioritise the frequent and hegemonic over the diverse.
Moreover, by jointly considering and making explicit the connection between the technical and the social of such socio-technical systems, I am afforded the ability to examine ways in which inadequacies, stemming from optimisation technologies, can be addressed from within a framework of optimisation.
The over-arching research questions below then seek to first make explicit how the technical and social are connected, and rethink modes of computation to address methods that are factors in the socially discriminatory patterns of content moderation technologies.

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=I, label={\textbf{RQ \roman*}}]
      \item\textit{What technical and social factors are present in the socially discriminatory predictions of content moderation systems?}
      \item\textit{In which ways can computational methods be used to address limitations that are influential in discriminatory outputs from computational modelling?}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

I examine these research questions by subdividing my thesis into four sections:
}
First, I address questions around how abuse is defined and the consequences of such definitions on the content that is subject to such systems.
Second, I address model generalisability onto out-of-domain data labelled for abuse\ZTedit{ using low-dimensional data representations}.
Third, I examine the impacts of representing different contexts into machine learning models in terms of improvements on classification metrics.
And finally, fourth, I discuss how \ZTedit{marginalisation caused by machine learning systems is driven by modelling choices and the various ways in which designers of machine learning systems embed their own subjectivities into the models.} \ZTdelete{machine learning modelling choices \ZTdelete{drive marginalisation} and how the designers of machine learning systems embed their subjectivities into the model.}

I choose these specific foci \ZTedit{because my core interest is identifying how humans fit into the structure of content moderation systems, as they are currently being built.
Through my interventions, I hope to identify theoretical and practical means for content moderation technologies to come to more closely respect and represent the humanity of the people who are subject to them.}
\ZTdelete{to examine the over-arching questions of how the human fits into abusive language detection technologies and machine learning models for content moderation can come to more closely respect and represent their humanity.}

\ZTedit{
  To examine these questions, I begin by examining content moderation systems and logics theoretically through the field of discard studies.
  Specifically, I examine how people are impacted by the operationalisation of annotation guidelines and the modelling approaches chosen.
  Then, I consider a method for modelling abuse that seeks to address the concern of models over fitting to highly salient terms by performing large scale vocabulary reductions.
  Next, I investigate how joint optimisation of abusive, and seemingly, semantically similar non-abusive tasks impact model performances for abuse detection and analyse the impacts of different classification task configurations.
  Finally, in a return to theory, I view machine learning and NLP through the lens of Science and Technology Studies (STS) to expose the ways in which practitioners perform a series of embodiments and disembodiments to each stage in the machine learning development pipeline.
  For each section, I present research questions that are subsumed by the two over-arching questions presented above.
  These questions allow for specific points of entry through which we are afforded the ability examine content moderation technologies as a whole.
}
\ZTdelete{To gain insights into these questions, I start by examining how the operationalisation of annotation guidelines and modelling approaches impact users, through how different machine learning modelling architectures impact classification performances, and finalising with how the designers of such models and their beliefs and experiences are embedded within the automated content moderation systems that they develop.
To accomplish this larger goal, it is necessary to consider the question of content moderation from a variety of disciplines, for this reason, I address the questions posed in each chapter from the disciplinary basis that they require.}
\ZTedit{Contesting with the broadness and depth of content moderation technologies clearly requires that the research conducted has a base in a number of disciplines, from the computational methodologies to the anthropological and to the sociological, and in this case and reflexive methodologies.
For this reason, I address each section of my thesis from the disciplines which are best suited to answer the research questions at hand.
I note here that any attempt to divorce the social from the computational, or vice versa is bound to fail to grasp the complexities and complications that they each bring, rendering the insights wanting, if not incomplete.
Next, I provide a deeper description of each section of the paper, to serve as a guide to where my interests intersect with the reader's.
}
\ZTdelete{For the chapters where the questions are not directly linked to the development of computational resources, I ground myself in the disciplines most closely related, i.e. discard studies and science and technology studies, and consider these in conjunction with the computational modelling practices in NLP and machine learning.}

First, in \cref{chap:filter}, I consider the nature of the task of detecting abusive content from the perspective of discard studies.
Through an analysis of two content moderation systems, I examine how power differentials and respectability politics to determine the boundaries between ``the healthy'' and ``the toxic'' are embedded into content moderation infrastructures and expressed through them.
\ZTdelete{Here, I consider what the aims of computational processes are and how researchers have attempted to achieve these.
In this chapter, I critically examine how the notion of ``toxicity'' has been operationalised in computational boundary work and the implications such operationalisation has on the political economies of the content moderation infrastructures that co-constitute themselves with such operationalisation.}
\ZTedit{In these analyses, I consider the aims of computational processes for abuse detection and the methods with which researchers have attempted to achieve these.
Moreover, I critically examine how the notion of ``toxicity'' has been operationalised in computational boundary work and the implications that these operationalisations have on the political economies of the content moderation infrastructures.
Building on theories of classification and purification \citep{Douglas:1966} and content moderation cast a digital sanitisation practice \citep{Lepawsky:2019}, I argue that processes and political economies of content moderation are co-constitutive of one another, thereby entrenching content moderation infrastructures within pre-existing systems of marginalisation.
To address such issues, I suggest that content moderation should move beyond the question of content removal towards a productive ``re-ordering of [...] environment[s]'' \citep{Douglas:1966} to allow communities to constitute their own identities.
\ZTdelete{Building on work in cultural anthropology on classification and purification \citep{Douglas:1966}, and work in discard studies that suggest that content moderation is a practice in digital sanitisation \citep{Lepawsky:2019} , I argue that content moderation should move beyond the question of content removal towards a productive ``re-ordering of [...] environment[s]'' \citep{Douglas:1966} that can allow communities to constitute their own identities.}
\ZTdelete{Moreover}\ZTedit{Specifically}, I argue that current practices and operationalisations of ``toxicity'' for content moderation systems are rooted in patriarchal and white supremacist imaginaries of acceptability.
\ZTdelete{Through an analysis of two content moderation systems, I examine how power differentials and respectability politics to determine the boundaries between ``the healthy'' and ``the toxic'' are embedded into content moderation infrastructures and expressed through them.}
% \zw{Q for Adina: is the below sentence on conclusions better suited for the conclusion of the diss?}
% I conclude the chapter by calling for content moderation systems to reckon with their role in in the conflicts surrounding the boundary work that they engage in.
% Through such an engagement with the disparate impacts and political implications of deeming ``the healthy'' from ``the toxic'', recentre the development of content moderation infrastructures  with marginalised communities at their centre, rather than leave such communities to exist on the boundaries where they policed without protection.

\ZTdelete{Thus, the research questions asked that afford such analysis of content moderation infrastructures are:}
\ZTedit{This chapter then seeks to provide partial answers to \textit{RQ I} by asking:}

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \ZTedit{\item{\textit{How are notions of ``toxicity'' operationalised and modelled, and what are their socio-political implications for content moderation systems?}}}
        \ZTdelete{\item{\textit{How are notions of ``toxicity'' operationalised in content moderation infrastructures and what are the socio-political implications of such operationalisations?}}}
        \ZTdelete{\item{\textit{How do modelling choices impact how computational models for content moderation learn and represent hegemonic notions of respectability?}}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

Chapter \ref{chap:filter} is written on the basis of a collaborative research project with Nanna Bonde Thylstrup (Copenhagen Business School) and a paper has been accepted in a special issue of the journal First Monday.
In this chapter, I expand on the considerations of the research project in each section and particularly further develop \cref{sec:filter_models,sec:filter_conclusion}.

\ZTedit{Next,} in \cref{chap:liwc}, I turn to the technical development of new content moderation systems for abusive text.
Here, I address the issue of model generalisability, over-fitting and efficiency by representing documents through the Linguistic Inquiry and Word Count (LIWC) categories invoked by the contents of the documents.
LIWC is a software, and an associated dictionary, that maps a set of words to their ``psychology relevant'' categories, that can allow its users to computationally analyse \ZTedit{proxies to} the mental states of speakers.
Thus, by representing documents through the LIWC categories each word invokes, it is possible to gain \ZTedit{some} insight into the mental and emotional state of the speaker \citep{Pennebaker:2001}.
Consequently, models that are optimised on LIWC representations of documents are optimised on proxies for the mental and emotional states of the speaker.
\ZTedit{Moreover, the set of words that are included in the LIWC dictionary figure in much lower numbers than the raw token vocabularies or sub-words computed on the vocabulary.}
\ZTedit{Thus, using LIWC can afford a low-vocabulary modelling of the emotional context a speaker is in at the time of writing.}
\ZTdelete{The use of LIWC categories to represent documents thus allows for a closer modelling of the context a given speaker is in at the time of writing.}

\ZTedit{Reducing the vocabulary size throguh LIWC has a large implication for out-of-vocabulary tokens, the likelihood of over-fitting to particular tokens, and the time required for optimising the models.
Using LIWC, the vocabulary sizes are minimise by up to $99\%$ which also introduces a question of whether this particular vocabulary reduction method is viable for the task.}
\ZTdelete{Using LIWC to represent documents thus entails a reduction in the vocabulary sizes for the models, which has implications for out-of-vocabulary tokens as well as the likelihood of over-fitting to particular tokens, and on the time it takes to complete the optimisation processes for neural network models.}
\ZTdelete{Because using LIWC document representations drastically reduce the vocabulary sizes, I conduct experiments to examine the viability of using LIWC as data representation for neural networks.}
Moreover, the reduction in vocabulary sizes and the fact that LIWC categories act as proxies to the mental and emotional states of speakers, I experiment with the out-of-domain generalisability of models optimised on documents represented through their LIWC categories.
\ZTdelete{Finally, I analyse the temporal impacts on using LIWC-based documents representation for modelling.}

The research questions addressed in \cref{chap:liwc} are thus:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=3, label={\textbf{RQ \arabic*}}]
        \item{\textit{Can LIWC provide a meaningful substitute to using words or sub-word tokens as input tokens and how is model performance affected by such a substitution?}}
        \item{\textit{What are the implications of using LIWC as input on model development in terms of optimisation time?}}
        \item{\textit{What are the implications on generalisability of LIWC-based models?}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

In \cref{chap:mtl}, I proceed further into an inquiry of the impact of context on model performance for abusive language detection tasks.
I use Multi-Task Learning (MTL) to examine how jointly learning different representations for abuse detection tasks and related tasks can impact classification performance.
With MTL, I explore hypotheses of which tasks are related in such a way that machine learning models can take advantage of them.
The abuse detection auxiliary tasks that I explore are hate speech detection, offensive language detection, and toxicity detection.
Moreover, I explore sarcasm detection, predicting of the basis of an argument, and predicting moral sentiments.
Through the use of these auxiliary tasks, I optimise machine learning models that also learn representations of the auxiliary tasks.
Selecting auxiliary tasks can be a difficult endeavour, for this reason I explore how each of the auxiliary tasks impact the performance on the primary tasks.
Additionally, I conduct experiments identifying how different task combinations influence performances on the primary task.
Through this method, I identify which auxiliary task combinations contribute most positively to the classification of the primary task through learning representations for the different auxiliary tasks.

Thus, the research questions explored in this chapter are:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=6, label={\textbf{RQ \arabic*}}]
        \item{\textit{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}
        \item{\textit{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}
        \item{\textit{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

The work in \cref{chap:mtl} is the result of an ongoing collaboration with Joachim Bingel (Hero I/S).
All of the content produced in the chapter is new and developed specifically for the purposes of this dissertation.

In the final content chapter, \cref{chap:disembodied}, I turn a critical lens to the proposed objectivity of machine learning models and the influence of this imaginary on abuse detection.
Drawing on work on subjectivity from Science and Technology Studies, I examine how subjectivities are embodied in the machine learning pipeline.
That is, I examine how social and cultural meanings that are embedded in the human experience are also embedded in the derivatives of it, i.e. in the data that are created by humans and subsequently the machine learning pipelines that are created on the basis of these.
I argue that this illusion of objectivity \citep{Haraway:1988} disembodies machine learning pipelines from the people they are designed for and the people whose data they are developed on.
I further argue, that such imaginaries of objectivity provides a barrier to the goal of achieving equitable machine learning models, and that these imaginaries are a key reason that discriminatory models are produced as they embody white supremacist imaginaries of the world.
By extension, I argue that machine learning models for social predictions, e.g. abusive language detection, are currently rooted in such discriminatory imaginaries and thus, produce discriminatory outcomes.
I further argue that for machine learning models to be equitable and not disadvantage marginalised communities, it is necessary to centre the subjectivities of the people whose data make the basis of the machine learning models and the people that the models are developed for.
A step to achieving this goal is to analyse how the subjective embodiments of the designers are embedded in the machine learning process, such that decisions can be made which are aligned with the subjectivities of the people using the models.
The implication of these arguments is that ``bias'' is not a quantifiable entity that can be subject to optimisation, rather biases, or subjective embodiments, permeate the machine learning pipeline and as such the goal of ``debiased'' machine learning models and the work dedicated to this end, serve to obscure situatedness and subjective embodiments of machine learning models.
For these reasons, I examine how my own subjectivities are embodied through the modelling choices that I make throughout this dissertation.

The research questions that I explore in this chapter are thus:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=10, label={\textbf{RQ \arabic*}}]
        \item{\textit{How are the subjective embodiments embedded in the machine learning pipelines?}}
        \item{\textit{Which processes in the machine learning pipeline are influenced by the subjective embodiments of the designers and people they rely on and how are they expressed?}}
        \item{\textit{What are the implications of such subjective embodiments with regard to developing machine learning models?}}
        \item{\textit{What are the implications of subjective embodiments on the goal of developing fair and equitable machine learning models?}}
    \end{enumerate}
\vspace{5mm}
\end{minipage}
\end{center}

The work in \cref{chap:disembodied} extends on a collaboration with Smarika Lulz (Humboldt University), Joachim Bingel (Hero I/S), and Isabelle Augenstein (University of Copenhagen). Subsections of the chapter are currently in review at the journal Computational Linguistics.
The work presented here expands on the collaborative work across all sections and the sections examining the machine learning models developed in this dissertation are entirely new.

Through the work in these chapters, I examine different aspects of the machine learning pipeline for abusive language detection to gain an understanding on a) what the implications of task definitions and modelling approaches are, b) how do distinct data representations influence model generalisability and efficiency in optimisation, c) how related tasks can influence in-domain performance, and finally d) how subjective embodiments are expressed throughout the machine learning pipeline and what the implications are for developing equitable machine learning models.
By examining the machine learning pipeline for abuse detection from conceptualisation, through model development, to mapping limitations of current practices, I provide detailed insights into the different facets of developing machine learning models for abusive language detection and identify specific avenues for future research while also highlighting the specific limitations of current practices.

\ZTedit{
Finally, before I release the reader to the core content of my thesis, I provide disclaimers for the contents in this thesis.
First, following \citet{Kulynych:2020} I refer to \textit{training} machine learning models as \citet{optimising}.
I make this decision because machine learning models are a specific kind of optimisation technology rather than a sentient entity which can be trained.
Moreover, this distinction between \textit{training} and \textit{optimisation} allows us to view optimised models more clearly as statistical artefacts rather than anthropomorphised machines that ``learn''.
Casting aside the veneer of sentient capabilities provides space to critically examine the decisions and artefacts that influence machine learning models to produce the desired and undesired effects alike.

Second, I do not make use of pre-trained embedding layers or fine-tuned language models in my work.
Although both techniques have been shown to improve performances of machinic abuse detection \citep{Fortuna:2018}, they are not compatible with some experiments in \cref{chap:liwc} and thus influence comparability between the experiments conducted in the chapter.
For the chapters in which their use would not limit direct comparability, these methods carry a range of social biases and norms that are beyond the scope of this thesis to examine.
Thus I avoid using them as a way to ensure that the claims I make in this thesis are a result of the models and modelling decisions, rather than spurious correlations.

\ZTcomment{Incomplete. Add references.}
Third, as I argue in \cref{chap:disembodied}, there is a need for an active consieration of the lived subjectivitites practitioners' and researchers' in computational methods, I use first person singular pronouns throughout this thesis.
In doing so, I follow a tradition in feminist \citep{...} and Black feminist \citep{...} scholarships and critiques of the veneer of objectivity in science.

Fourth, I seek to avoid providing examples of abusive texts where possible.
Although my thesis is centered around content moderation and therefore requires some examples, I believe that we, as researchers, should limit the amount of abusive content in our work, as frivellous inclusion of such texts can be limiting to the potential readership and institutionalise and archive the very harms that we seek to mitigate.

Lastly, beyond being a culmination of the research I have conducted, this thesis is also strongly influenced by the conversations I have had with activists and social scientific researchers, and the organisational work that I have done towards diversity and inclusion in the NLP field and organising the Workshop on Online Abuse and Harms, which I founded in 2017.
Through my organisation I have come to have discussions with a variety of scholars and organisations that are addressing aspects of online abuse, including Seyi Akiwowo, Mikki Kendall, Maya Indira Ganesh, Alvin Grissom II, Kat Lo, Su Lin Blodgett, Joris van Hoboken, Seeta Pe{\~n}a Gangadharan, and many others.
While I do not claim to speak for any of them or any particular community in this work, there is little doubt that the shape of my would be very different, and much poorer, without the discussions and conversations that we have shared.
}
%********************************** % Report Structure  *************************************
\section{Dissertation Structure}\label{sec:structure}

In this chapter, I have introduced the different research questions that I will be examining throughout this dissertation. Here I provide a brief overview the structure of the dissertation.

In \cref{chap:socialscience}, I introduce some of the foundational concepts and theories that I rely on that have been developed in primarily non-computational fields.\\
In \cref{chap:nlp}, I turn to lay the computational foundation for the work conducted, introducing the various modelling aspects that I rely on.\\
In \cref{chap:filter}, I examine how notions of ``toxic'' and ``healthy'' are operationalised and the implications these have on machine learning models and their outcomes.\\
In \cref{chap:liwc}, I focus on questions of model generalisability and the impacts of data representations.\\
In \cref{chap:mtl}, I examine how abuse detection tasks and other tasks related to abusive language detection can be used in a Multi-Task Learning setting to improve in-domain classification performances.\\
In \cref{chap:disembodied}, I consider how machine learning models rely on the illusion of objectivity to disembody themselves, and the developers, from the subjective contexts they are embedded in.
Finally, in \cref{chap:conclusion}, I conclude on the insights from the different chapters and suggest avenues of future research that take these insights into account.


%In this chapter, we have introduced the tasks we'll undertake in the thesis along with the research questions and brief outlines of how we will address each research question. For the sake of the confirmation panel chapters \ref{chap:nlp}, \ref{chap:done}, \ref{chap:experiments}, and \ref{chap:ddp} deal with the requirements of the report, while \ref{chap:socialscience} deals with more background knowledge that further motivates and provides deeper understanding of the social scientific concepts and how they will be used.\footnote{In Chapter \ref{chap:socialscience} we also present our ethical considerations.}\vspace{5mm}

%\noindent Chapter \ref{chap:socialscience} presents the knowledge from Critical Race Theory, Gender Studies, ethics for social media research, and Psychology upon which we base our methodology and how we will use these concepts.\vspace{3mm}

%\noindent Chapter \ref{chap:nlp} presents the literature review on current approaches to abusive language detection and counter speech generation in NLP, and seeks to explain which methods we will use and how they will be used.\vspace{3mm}

%\noindent Chapter \ref{chap:done} presents the work that has been conducted thus far.\vspace{3mm}

%\noindent Chapter \ref{chap:experiments} details the tasks and approaches that will be carried out during this PhD project, as well as a tentative execution timetable.\vspace{3mm}

%\noindent Chapter \ref{chap:ddp} outlines the activities completed regarding the Doctoral Development Program (DDP), as well as other activities that have been carried out.\vspace{3mm}

%\noindent Appendix \ref{appendix:published} provides the full papers that have been published thus far.\vspace{3mm}

%\noindent Appendix \ref{appendix:ethics} provides the successful and pending applications for ethical approval. Successful applications also include the notification of acceptance.\vspace{3mm}

%\noindent Appendix \ref{appendix:draft} contains draft papers and papers which are currently under review.

% \section{Previous work}
% As social media platforms have been subject to a growing amount of governmental attention for the abusive content on their platforms \citep{HomeOffice:2016,EUCommission:2016}, they must find scalable methods for dealing with online abuse which can deal with the large volume of new documents added daily \citep{Kuchera:2014,Bates:2013,Sood:profanity:2012}.
% By considering how these cultures are formed, which methods for regulation and self-regulation are effective, and the linguistics markers we may gain insight into the distinct forms of culture and speech which is deemed acceptable by a given platform.
% With this knowledge it is possible to develop methods to encourage healthy conversations and mitigate negative societal constructs \citep{Bolukbasi:2016,Hannon:2016}.\vspace{5mm}

% Recently, there has been an increase in the amount of research on abusive language and hate speech in the Natural Language Processing (NLP) community.
% In spite of this increase in research attention, many questions remain unanswered, the methods for detection are not generalizable beyond the individual data sets, and there has been little consideration of how models learn social biases that discriminate against different groups.
% In addition, while this increase of interest in disparate and abusive treatment of people is quite new in NLP, it has been the subject of long and sustained interest in many other fields such as gender studies \citep{McIntosh:1988,Butler:1990,Beauvoir:1953}, law \citep{Han:2015,Crenshaw:1989}, media studies \citep{Shah:2015,Shah-FirstMonday:2015}, sociology \citep{Zaleski:2016}, and critical race theory \citep{Crenshaw:1989,Lawrence:1992} to name a few.
% In spite of a rich body of work on abuse and discrimination, work in NLP often neglects this history relying solely the community guidelines or terms and conditions documents put forth by social media companies \citep{Nobata:2016,Ross:2016}.
% This lack of consideration of work previous work has the consequence of creating blind spots in annotation guidelines and inconsistent judgments on acceptable language.\vspace{5mm}
%
% As abusive language may be expressed along several axis, we focus on abusive language that is directed and either explicit in its potential to be abusive or seeks to use implicit signals to notify abuse \citep{Waseem:2017}. We limit ourselves to these forms of abuse (please see Table \ref{tab:examples} for examples) to provide a reasonable scope of the project, and because currently the majority of data sets released deal with directed abuse. Further, due as the relationship between and identities of speaker and listener are important (e.g. black person using the n-word colloquially with another black person is not necessarily problematic\citep{Rahman:2012}, whereas a non-black person using the n-word directed at a black person is most likely problematic, as they speak to different histories), we will consider directed abuse as it has a clearly identifiable speaker and listener.\vspace{5mm}
%
% \begin{table}[ht]
%   \centering
%   \scriptsize
%   \begin{tabular}{p{0.30\textwidth}|p{0.30\textwidth}|p{0.30\textwidth}}
%     {\bf Input} & {\bf System} & {\bf Output}\\\hline
%   ``Go kill yourself'' & Abusive Language Detection Model & Abusive\\\hline
%     ``(((@User))) and what is your job? Writing cuck articles and slurping Google balls? \#Dumbgoogles'' & Procedural System & $0.80-0.85$ likelihood to be abusive\\\hline
%     ``@User shut yo b**n*r ass up sp*c and hop your f****t ass back across the border little n***a'' & Counter Speech Generation System & ``Why do you think they should leave the country? This is their country too.''\\
%   \end{tabular}
%   \caption{Examples of abusive comments and the outputs of our systems.}
%   \label{tab:examples}
% \end{table}

% Beyond a consideration of speaker and listener, it is necessary to consider of predictive impact. That is whether a given model's performance treats all investigated demographics equally, given a definition of equality.\footnote{Several competing definitions of equality exist which influence the evaluation of whether a model is discriminatory or not.} Specifically for issues in which the labels are highly contextual, such as hate speech or offensive language, social biases on acceptable language may influence annotations and thus models which are learned on the data.

% In this Ph.D. we will seek to marry the findings and knowledge obtained in fields outside of NLP on the nature of abuse and notions of equal and fair treatment.

% As machine learning methods rely on the data sets and labels upon which they are trained \citep{Mitchell:2018,Waseem:2016}, we will aim to identify and address issues of bias in datasets and propose methods to address such issues. We will address the issue of bias by considering a number of datasets and variables which may give cause for discrimination.

% Online abuse is heavily situational and influenced by external forces such as social events, disasters, violence, elections, and political movements \citep{Marsh:2018,Burret:2017,Dearden:2019}. For this reason, we will consider a longitudinal investigation of online abuse as it occurs in targeting British members of parliament.


%Finally, we will aim to improve the generalizability of machine learning models for hate speech detection.

%\section{Research Questions}\label{sec:rq}

%\subsection{Abuse in Context: The legal and socio-technical landscape of abuse}

%As more countries and legal territories are considering or have implemented regulation about artificial intelligence

%As artificial intelligence becomes increasingly ubiquitous in society, there is is a growing need for a consideration of regulation of such technologies. This need has prompted the European Union to develop the ``Ethics Guidelines for Trustworthy Artificial Intelligence'' \citep{EU:TrustworthyAI}. In the case of content moderation, the European Union is also considering developing regulation of online dissemination of terrorist content, a proposal which may also have implications of abuse and hate speech in particular. Further, given the issues of disparate impact and harms of content moderation \citep{Gomes:2019}, it becomes necessary to consider the role of content moderation and how automated measures may be influenced by concerns of communities that they may be unfairly penalized by such technology.

%\begin{quote}
%  {\bf RQ1}: What are the implications of the legal realities, socio-technological and cultural contexts within which technology for abusive language detection exists for machine learning tools and what is the consequence for building tools for content moderation of abuse?
%\end{quote}


%\subsection{Hidden Variables: Minimising Disparate Impacts of Machine Learning Models Learned on Biased Annotations}\label{sub:critical}

%As more data sets for abusive language detection are released it is important to critically assess the hidden variables that exist in the data sets which impact classifiers: i.e. which type of abusive language they can aid in identifying \citep{Jha:2017,Waseem:2017} as well as which cultural phenomena they speak to.

%For instance \cite{Waseem:2018} and \cite{Sap:2019} find that the data set published by \cite{Davidson:2017} has a great deal of African-American Vernacular English (AAVE) labeled as either hate speech or offensive language. By training a classifier on this data set without ensuring that there are safeguards to prevent demographic discrimination, the trained model is likely have a preference for detecting that people who use AAVE are being either offensive or are using hate speech, in large parts disregarding whether the contents are truly offensive or abusive. Another issue that comes to light with the fact that AAVE is more likely to be tagged as offensive or hate speech is that the computational models built for removal, moderation, or even generation of counter speech, will explicitly make a judgment about the appropriateness of AAVE as a dialect. An implied role in abusive language detection is the responsibility of determining which forms of speech are socially acceptable; for this reason it is vital that seek to investigate which demographics, if any, are being singled out by the data sets we employ for training our models.

%As there are no published datasets for abuse that also contain information about the speaker, we will also consider datasets that do not relate to abuse, such as the Wang \citeyear{Wang:2017} fake news dataset and the PAN Author Profiling dataset \citep{PAN:2015} to demonstrate rebiasing, while we develop a dataset with annotation for dialectical use.

%\begin{quote}
%  {\bf RQ2}: How can we identify correlations betweeen classes and protected variables (e.g. race, gender, and political affiliation) in machine learning methods and can we minimise the bias in classifiers for NLP systems?
%\end{quote}

%\subsection{Out of Time: Temporal patterns in the development of abuse.}

%Many cases of abuse is situational and influenced by events external to the message being sent. Considering offline contexts, there is evidence suggesting that external influences such as terrorist attacks and elections can serve as catalysts for abuse and hate crimes \citep{Marsh:2018,Burret:2017,Dearden:2019}. In an online setting we see a similar pattern for the use of online hate speech, where a rise in hate speech follows external events \citep{Olteanu:2018}. Previous work on abusive language has primarily considered singular instances of abuse \citep{Davidson:2017,Waseem:2016}, constructivity of threads \citep{Napoles:2017,Kolhatkar:2017}, and the language and volume of abusive tweets \citep{Gorrell:2018}.

%While prior work has addressed event detection \citep{Chen:2018,Orr:2018} and, more topically, predicting riots \citep{Alsaedi:2017}, to the best of our knowledge no prior work has examined the temporal developments of abuse. Here we consider the dataset of abuse against members of British Parliament \citep{Gorrell:2018} and ask the questions of how abuse develops over time and predicting the moral sentiment \citep{Hoover:2019} of responses to tweets. The purpose of this study is to allow for early detection of abuse occurring in addition to identifying features of tweets which elicit a disproportionately abusive response.

%\begin{quote}
%{\bf RQ3}: What are the temporal developments of abusive language production towards British MPs and can these patterns be used for early detection of abusive responses.
%\end{quote}

% \zw{Tweets that illicit emotional responses and generate abuse | could also be co-reference resolution to identify whether the target is actually the original speaker}

%\subsection{Abusive Language Detection Using Machine Learning}\label{sub:detection}

%One large issue with the current state of datasets available for abusive language, is the relatively small scale \citep{Waseem:2016,Davidson:2017}. The small size of the datasets allow for easy overfitting to confounding tokens (e.g. ``islam'' \citep{Waseem-Hovy:2016}). Further due to the small sizes of the datasets the issue of cultural-specificity arises, in which the datasets represent only one specific culture, which acts as a further inhibitor to building models which are generalizable across data sets.
%Another challenge facing the field of abusive language detection is that the vast majority of abusive language data sets display large class imbalances with the majority class being documents that are not abusive, e.g. in \cite{Waseem-Hovy:2016}, where $31\%$ of the data is labeled as abusive spread over two classes ($20\%$ labelled as sexist and $11\%$ labelled as racist). As such classifiers are given the task of identifying abusive language while being given access to very little data and highly variable data in its notions of acceptability and demographic targeting. For a task such as abusive language and hate speech detection, where subjects such as racial and gender minorities are often discussed in both constructive and abusive ways, this means that classifiers are prone to over-fit to the majority class thus resulting in poor usability of the models and indeed, poor generalizability beyond the specific data set.\vspace{5mm}

%Most data sets that have been for abusive language detection do not allow for generalization beyond the given data set due to being very small e.g. in \cite{Waseem:2016} there are less than $100$ documents labeled in the minority class; the data is collected for a limited number of targets \citep{Waseem-Hovy:2016}; or some communities being overrepresented in the data sets \citep{Davidson:2017}. In addition, many annotated data sets for abusive language are collected on Twitter \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Jha:2017} which naturally influences a given model's performance on data obtained from other sources which may have not have length limitations on documents (e.g. comments or posts). Poor generalisability also influences moving from one data set to another \citep{Waseem:2016} which greatly reduces the usability of the models that are trained on a given data set. To overcome this issue of poor generalisability stemming from a limited number of targets, we will be collecting additional data which will be used to explore the utility of neural network models along with linear models. Further, we will seek to abstract away from the words in the documents to avoid over-fitting to specific terms. We will to utilize unannotated data from communities that are known to be abusive in order to address poor generalizability stemming from a limited number of target groups in the annotated data as well as the issue of documents that exceed Twitter's document length restrictions. Thus, we will seek to address the following research question:\vspace{5mm}
%\begin{quote}
%{\bf RQ4}: How can embeddings, weak supervision, linguistic annotation, and meta data help improve platform and topic independent detection of abusive language and hate speech?
%\end{quote}


%********************************** % Report Structure  *************************************
%\section{Report Structure}\label{sec:structure}

%In this chapter, we have introduced the tasks we'll undertake in the thesis along with the research questions and brief outlines of how we will address each research question. For the sake of the confirmation panel chapters \ref{chap:nlp}, \ref{chap:done}, \ref{chap:experiments}, and \ref{chap:ddp} deal with the requirements of the report, while \ref{chap:socialscience} deals with more background knowledge that further motivates and provides deeper understanding of the social scientific concepts and how they will be used.\footnote{In Chapter \ref{chap:socialscience} we also present our ethical considerations.}\vspace{5mm}

%\noindent Chapter \ref{chap:socialscience} presents the knowledge from Critical Race Theory, Gender Studies, ethics for social media research, and Psychology upon which we base our methodology and how we will use these concepts.\vspace{3mm}

%\noindent Chapter \ref{chap:nlp} presents the literature review on current approaches to abusive language detection and counter speech generation in NLP, and seeks to explain which methods we will use and how they will be used.\vspace{3mm}

%\noindent Chapter \ref{chap:done} presents the work that has been conducted thus far.\vspace{3mm}

%\noindent Chapter \ref{chap:experiments} details the tasks and approaches that will be carried out during this PhD project, as well as a tentative execution timetable.\vspace{3mm}

%\noindent Chapter \ref{chap:ddp} outlines the activities completed regarding the Doctoral Development Program (DDP), as well as other activities that have been carried out.\vspace{3mm}

%\noindent Appendix \ref{appendix:published} provides the full papers that have been published thus far.\vspace{3mm}

%\noindent Appendix \ref{appendix:ethics} provides the successful and pending applications for ethical approval. Successful applications also include the notification of acceptance.\vspace{3mm}

%\noindent Appendix \ref{appendix:draft} contains draft papers and papers which are currently under review.
