%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

% !TEX root = ../thesis.tex
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\chapter{Introduction}\label{chap:intro} \zw{Rought contents here, needs updating to fit with narrative of the thesis and polishing of text}

%Structure:
%Introduction to the field in general
%Narrow down to the issues from the different chapters
%- Tie together how different chapters highlight different shades of the same problem
%- Add research questions to all chapters that don't have explicit research questions stated.
%- - Chapter arguments:
%- - - Chap 4 (Dirt): Abuse detection is broken because it operates with trying to classify uncertain boundaries
%- - - Chap 5 (LIWC): If the boundary is porous then maybe we're looking at the wrong problem, maybe the right problem is what people mean
%- - - Chap 6 (MTL): If we must look at dirt, then maybe we need to look more broadly to understand how people are trying to communicate
%- - - Chap 7 (Disembodied): But maybe this is all wrong if what we want to do is protect people but we're actually harming them.
%- - - Chap 8 (Conclusion): Well, maybe we need to rethink this all, but some components of rethinking it are in thinking about the things around the abuse, not the abuse itself.
%Add text about WOAH and WiNLP
%Provide structure of thesis


As people consume, interact with, and create media online at an ever growing rate, it becomes necessary to consider the content with which they interact, how this influences and impacts audiences, and the cultures that are formed in online spaces.
A small, in terms of absolute numbers of content created, but prominent portion of online communications is online abuse in various forms, including cyber-bullying, hate speech, harassment, aggressive speech, and offensive speech \citep{Turing:2019}.
Such content can lead to users disengaging with online spaces \citep{Users leaving}, psychological harms on the targets of abuse and other people who are exposed to it \citep{Paper on negative effects of witnessing abuse}, and an increase in hate crimes \citep{Muller:2020}.
As a result, social media platforms have long been subject to pressure from users and regulators to moderate and remove such content.
This issue of content moderation has traditionally been a human effort \citep{Roberts:2019} but has increasingly become reliant on computational methods for detecting and adjudicating online abuse \citep{Facebookreports}.
These automated content moderation systems developed by commercial platforms span several different modalities, including images, text, and videos.
Along with this increase in automated content moderation systems developed by commercial entities, there has been an increased in attention to the challenge of developing automated systems for detecting text-based abuse in the Natural Language Processing (NLP) community.
Beyond the NLP community, there has been sustained academic attention to the challenges faced by content moderation systems, both human and automated.
This body of work has spanned across a wide variety of disciplines including, but not limited to, media studies \citep{Gillespie:2020,Gerrard:2020}, archival research \citep{Thylstrup:2019,Agostinho:2019}, and legal studies \citep{Cobbe:2020,Llanso:2020}.
While the content that is moderated spans several modalities and is often multi-modal, in this dissertation I focus on text-based content moderation systems, that is NLP systems for the detection of online abuse.

In the academic inquiry into content moderation systems conducted by the NLP community, several challenges have been identified including the lack of model ability to generalise onto new data and contexts \citep{Waseem:2016,Fortuna:2020}, how machine learning models for abuse detection discriminate against marginalised communities \citep{Davidson:2019,Dias:2020}, misclassifications that occur as a product of in-group communicative norms \citep{Dias:2020}, human annotator bias in the data creation process \citep{Waseem:2018,Davidson:2019}, challenges in collecting data \citep{Wiegand:2019,Vidgen:directions:2020}, how abuse is defined \citep{Waseem:2017}, the over-emphasis on English in research contributions \citep{Vidgen:directions:2020}, and situating speech within the context and the communicative intents of speakers \citep{Bender:2021}.
Although each of these challenges are of equal importance and each requiring urgent attention, I focus my efforts in this thesis on four different topics.

First, I address questions around how abuse is defined and the consequences of such definitions on the content that is subject to such systems.
Second, I address model generalisability.
Third, I examine the impacts of representing different contexts into machine learning models in terms of improvements on classification metrics.
And finally, fourth, I discuss how machine learning modelling choices drive marginalisation and how the designers of machine learning systems embed their subjectivities into the model.
I choose these specific foci to examine the larger question of how the automated content moderation pipeline affects different stakeholders, starting with how the operationalisation of annotation guidelines and modelling approaches impact users, through how different machine learning modelling architectures impact classification performances, and finalising with how the designers of such models and their beliefs and experiences are embedded within the automated content moderation systems that they develop.
To accomplish this larger goal, it is necessary to consider the question of content moderation from a variety of disciplines, for this reason I address the questions posed in each chapter from the disciplinary basis that they require.
For the chapters where the questions are not directly linked to the development of computational resources, I ground myself in the disciplines most closely related, i.e. discard studies and science and technology studies and consider these in conjunction with the computational modelling practices in NLP and machine learning.

In \cref{chap:filter}, I consider the nature of the task of detecting abusive content from the perspective of discard studies.
Here, I consider what the aims of computational processes are and how they are attempted to be achieved.
In this chapter, I critically examine how the notion of ``toxicity'' has been operationalised in computational boundary work and the implications such operationalisation has on the political economies of the content moderation infrastructures that co-constitute themselves with such operationalisation.
Building on work in cultural anthropology on classification and purification, and work in discard studies that suggest that content moderation is a practice in digital sanitisation, I argue that content moderation should move beyond the question of content removal towards a productive ``re-ordering of [...] environment[s]'' that can allow communities to constitute their own identities.
Moreover, I argue that current practices and operationalisations of ``toxicity'' for content moderation systems are rooted in patriarchal and white supremacist imaginaries of acceptability.
Through an analysis of two content moderation systems, I examine how power differentials and respectability politics to determine the boundaries between ``the healthy'' and ``the toxic'' are embedded into content moderation infrastructures and expressed through them.
% \zw{Q for Adina: is the below sentence on conclusions better suited for the conclusion of the diss?}
% I conclude the chapter by calling for content moderation systems to reckon with their role in in the conflicts surrounding the boundary work that they engage in.
% Through such an engagement with the disparate impacts and political implications of deeming ``the healthy'' from ``the toxic'', recentre the development of content moderation infrastructures  with marginalised communities at their centre, rather than leave such communities to exist on the boundaries where they policed without protection.

Thus, the research questions asked that afford such analysis of content moderation infrastructures are:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=1, label={\textbf{RQ \arabic*}}]
        \item{\textit{How are notions of ``toxicity'' operationalised in content moderation infrastructures and what are the socio-political implications of such operationalisations?}}
        \item{\textit{How do the modelling choices impact how computational models for content moderation learn and represent hegemonic notions of respectability?}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

The chapter is written on the basis of a collaborative research project with Nanna Bonde Thylstrup (Copenhagen Business School) and a paper is currently under review in a special issue of the journal First Monday.
In this chapter, I expand on the considerations of the research project in every section and particularly further develop \cref{sec:filter_models,sec:filter_conclusion}.\vspace{5mm}

In \cref{chap:liwc} I turn to the development of new content moderation systems for abusive text.
Here, I address the issue of model generalisability and efficiency by representing documents through the Linguistic Inquiry and Word Count (LIWC) categories invoked by the texts in the documents.
LIWC is a software, and an associated dictionary, that maps a set of words to their ``psychology relevant'' categories, that can allow users to analyse the mental states of speakers computationally.
Thus, by representing documents through the LIWC categories each word invokes it is possible to gain insight into the mental and emotional state of the speaker.
Consequently, models that are optimised on LIWC representations of documents are optimised on proxies for the mental and emotional states of the speaker.
The use of LIWC categories to represent documents thus allows for a closer modelling of the context a given speaker is in at the time of writing.
Additionally, the set of words that are included in the LIWC dictionary figure in much lower numbers than the raw token vocabularies or sub-words computed on the vocabulary.
Using LIWC to represent documents thus entails a reduction in the vocabulary sizes for the models, which has implications for out-of-vocabulary tokens as well as the likelihood of over-fitting to particular tokens, and on the time it takes to complete the optimisation processes for neural network models.
Because using LIWC document representations drastically reduce the vocabulary sizes, I conduct experiments to examine the viability of using LIWC as data representation for neural networks.
Moreover, the reduction in vocabulary sizes and the fact that LIWC categories act as proxies to the mental and emotional states of speakers, I experiment with the out-of-domain generalisability of models optimised on documents represented through their LIWC categories.
Finally, I analyse the temporal impacts on using LIWC-based documents representation for modelling.

The research questions addressed in \cref{chap:liwc} are thus:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=3, label={\textbf{RQ \arabic*}}]
        \item{\textit{Can LIWC provide a meaningful substitute to using words or sub-word tokens as input tokens and how is model performance affected by such a substitution?}}
        \item{\textit{What are the implications of using LIWC as input on model development in terms of training time?}}
        \item{\textit{What are the implications on generalisability of LIWC-based models?}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

In \cref{chap:mtl} I proceed further into an inquiry of the impact of context on model performance for abusive language detection tasks.
In this chapter, I use Multi-Task Learning (MTL) to examine how jointly learning different representations for abuse detection tasks and related tasks can impact classification performance.
Using MTL, I explore hypotheses of which related tasks are related in such a way that machine learning models can take advantage of them.
The abuse detection auxiliary tasks that I explore are hate speech detection, offensive language detection, and toxicity detection.
Moreover, I explore sarcasm detection, the prediction of the basis of an argument, and predicting the moral sentiments.
Through the use of these auxiliary tasks, I optimise machine learning models that also learn representations of the auxiliary tasks.
Selecting auxiliary tasks can be a difficult endeavour, for this reason I explore how each of the auxiliary tasks impact the performance on the primary tasks.
Additionally, I conduct experiments identifying how different task combinations influence performances on the primary task.
Through this method, I identify which auxiliary task combinations contribute most positively to the classification of the primary task through learning representations for the different auxiliary tasks.

Thus, the research questions explored in this chapter are:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=6, label={\textbf{RQ \arabic*}}]
        \item{\textit{What is the impact of using auxiliary tasks that are non-abusive in nature for detecting specific forms of abuse?}}
        \item{\textit{Does using different forms of abusive language detection tasks as auxiliary tasks have a positive influence on model performance for specific forms of abuse?}}
        \item{\textit{How does combining abusive and non-abusive auxiliary tasks influence model performance on specific forms of abuse?}}
    \end{enumerate}
    \vspace{5mm}
\end{minipage}
\end{center}

The work in this chapter is the result of an ongoing collaboration with Joachim Bingel (Hero I/S).
All of the content produced in the chapter is new and developed specifically for the purposes of this dissertation.\vspace{5mm}

In the final content chapter, \cref{chap:disembodied}, I turn a critical lens to the proposed objectivity of machine learning models.
Drawing on work on subjectivity from Science and Technology Studies, I examine how subjectivities are embodied in the machine learning pipeline.
That is, I examine how social and cultural meaning that are embedded in the human experience are also embedded in the derivatives of it, i.e. in the data that are created by humans and subsequently the machine learning pipelines that are created on the basis of these.
I argue that this illusion of objectivity disembodies machine learning pipelines from the people they are designed for and the people whose data they are developed on.
I further argue, that such imaginaries of objectivity provides a barrier to the goal of achieving equitable machine learning models and that these imaginaries are a key reason that discriminatory models are produced as they embody white supremacist imaginaries of the world.
By extension, I argue that machine learning models for social predictions, e.g. abusive language detection are currently rooted in such discriminatory imaginaries and thus, produce discriminatory outcomes.
I further argue that for machine learning models to be equitable and not disadvantage marginalised communities, it is necessary to centre the subjectivities of the people whose data make the basis of the machine learning models and the people that the models are developed for.
A step to achieving this goal is to analyse how the subjective embodiments of the designers are embedded in the machine learning process, such that decisions can be made which are aligned with the subjectivities of the people using the models.
The implication of these arguments is that ``bias'' is not a quantifiable entity that can be subject to optimisation, rather biases, or subjective embodiments, permeate the machine learning pipeline and as such the goal of ``debiased'' machine learning models and the work dedicated to this end, serve to obscure situatedness and subjective embodiments of machine learning models.
For these reasons, I examine how my own subjectivities are embodied through the modelling choices that I make throughout this dissertation.

The research questions that I explore in this chapter are thus:

\begin{center}
\begin{minipage}{0.9\textwidth}
\vspace{5mm}
    \begin{enumerate}[start=10, label={\textbf{RQ \arabic*}}]
        \item{\textit{How are the subjective embodiments embedded in the machine learning pipelines?}}
        \item{\textit{Which processes in the machine learning pipeline are influenced by the subjective embodiments of the designers and people they rely on and how are they expressed?}}
        \item{\textit{What are the implications of such subjective embodiments with regard to developing machine learning models?}}
        \item{\textit{What are the implications of subjective embodiments on the goal of developing fair and equitable machine learning models?}}
    \end{enumerate}
\vspace{5mm}
\end{minipage}
\end{center}


The work in this chapter extends on a collaboration with Smarika Lulz (Humboldt University), Joachim Bingel (Hero I/S), and Isabelle Augenstein (University of Copenhagen). Subsections of the chapter are currently in review at the journal Computational Linguistics.
The work presented here expands on the collaborative work across all sections and the sections examining the machine learning models developed in this dissertation are entirely new.
\vspace{5mm}

Through the work in these chapters, I examine different aspects of the machine learning pipeline for abusive language detection to gain an understanding on a) what the implications of task definitions and modelling approaches are, b) how distinct data representations of generalisability, c) how related tasks can influence in-domain performance, and finally d) how subjective embodiments are expressed throughout the machine learning pipeline and what the implications are for developing equitable machine learning models.
By examining the machine learning pipeline for abuse detection from conceptualisation, through model development, to mapping limitations of current practices, I provide detailed insights into the different facets of developing machine learning models for abusive language detection and identify specific avenues for future research while also highlighting the specific limitations of current practices.

%********************************** % Report Structure  *************************************
\section{Dissertation Structure}\label{sec:structure}

In this chapter, I have introduced the different research questions that I will be examining throughout this dissertation. Here I provide a brief overview the structure of the dissertation.

in \cref{chap:socialscience}, I introduce some of the foundational concepts and theories that I rely on that have been developed in primarily non-computational fields.\\
In \cref{chap:nlp}, I turn to lay the computational foundation for the work conducted, introducing the various modelling aspects that I rely on.|\
In \cref{chap:filter}, I examine how notions of ``toxic'' and ``healthy'' are operationalised and the implications these have on machine learning models and their outcomes.\\
In \cref{chap:liwc}, I focus on questions of model generalisability and the impacts of data representations.\\
In \cref{chap:mtl}, I examine how abuse detection tasks and other tasks related to abusive language detection can be used in a Multi-Task Learning setting to improve in-domain classification performances.\\
In \cref{chap:disembodied}, I consider how machine learning models rely on the illusion of objectivity to disembody themselves, and the developers, from the subjective contexts they are embedded in.
Finally, in \cref{chap:conclusion}, I conclude on the insights from the different chapters and suggest avenues of future research that take these insights into account.


%In this chapter, we have introduced the tasks we'll undertake in the thesis along with the research questions and brief outlines of how we will address each research question. For the sake of the confirmation panel chapters \ref{chap:nlp}, \ref{chap:done}, \ref{chap:experiments}, and \ref{chap:ddp} deal with the requirements of the report, while \ref{chap:socialscience} deals with more background knowledge that further motivates and provides deeper understanding of the social scientific concepts and how they will be used.\footnote{In Chapter \ref{chap:socialscience} we also present our ethical considerations.}\vspace{5mm}

%\noindent Chapter \ref{chap:socialscience} presents the knowledge from Critical Race Theory, Gender Studies, ethics for social media research, and Psychology upon which we base our methodology and how we will use these concepts.\vspace{3mm}

%\noindent Chapter \ref{chap:nlp} presents the literature review on current approaches to abusive language detection and counter speech generation in NLP, and seeks to explain which methods we will use and how they will be used.\vspace{3mm}

%\noindent Chapter \ref{chap:done} presents the work that has been conducted thus far.\vspace{3mm}

%\noindent Chapter \ref{chap:experiments} details the tasks and approaches that will be carried out during this PhD project, as well as a tentative execution timetable.\vspace{3mm}

%\noindent Chapter \ref{chap:ddp} outlines the activities completed regarding the Doctoral Development Program (DDP), as well as other activities that have been carried out.\vspace{3mm}

%\noindent Appendix \ref{appendix:published} provides the full papers that have been published thus far.\vspace{3mm}

%\noindent Appendix \ref{appendix:ethics} provides the successful and pending applications for ethical approval. Successful applications also include the notification of acceptance.\vspace{3mm}

%\noindent Appendix \ref{appendix:draft} contains draft papers and papers which are currently under review.

% \section{Previous work}
% As social media platforms have been subject to a growing amount of governmental attention for the abusive content on their platforms \citep{HomeOffice:2016,EUCommission:2016}, they must find scalable methods for dealing with online abuse which can deal with the large volume of new documents added daily \citep{Kuchera:2014,Bates:2013,Sood:profanity:2012}.
% By considering how these cultures are formed, which methods for regulation and self-regulation are effective, and the linguistics markers we may gain insight into the distinct forms of culture and speech which is deemed acceptable by a given platform.
% With this knowledge it is possible to develop methods to encourage healthy conversations and mitigate negative societal constructs \citep{Bolukbasi:2016,Hannon:2016}.\vspace{5mm}

% Recently, there has been an increase in the amount of research on abusive language and hate speech in the Natural Language Processing (NLP) community.
% In spite of this increase in research attention, many questions remain unanswered, the methods for detection are not generalizable beyond the individual data sets, and there has been little consideration of how models learn social biases that discriminate against different groups.
% In addition, while this increase of interest in disparate and abusive treatment of people is quite new in NLP, it has been the subject of long and sustained interest in many other fields such as gender studies \citep{McIntosh:1988,Butler:1990,Beauvoir:1953}, law \citep{Han:2015,Crenshaw:1989}, media studies \citep{Shah:2015,Shah-FirstMonday:2015}, sociology \citep{Zaleski:2016}, and critical race theory \citep{Crenshaw:1989,Lawrence:1992} to name a few.
% In spite of a rich body of work on abuse and discrimination, work in NLP often neglects this history relying solely the community guidelines or terms and conditions documents put forth by social media companies \citep{Nobata:2016,Ross:2016}.
% This lack of consideration of work previous work has the consequence of creating blind spots in annotation guidelines and inconsistent judgments on acceptable language.\vspace{5mm}
%
% As abusive language may be expressed along several axis, we focus on abusive language that is directed and either explicit in its potential to be abusive or seeks to use implicit signals to notify abuse \citep{Waseem:2017}. We limit ourselves to these forms of abuse (please see Table \ref{tab:examples} for examples) to provide a reasonable scope of the project, and because currently the majority of data sets released deal with directed abuse. Further, due as the relationship between and identities of speaker and listener are important (e.g. black person using the n-word colloquially with another black person is not necessarily problematic\citep{Rahman:2012}, whereas a non-black person using the n-word directed at a black person is most likely problematic, as they speak to different histories), we will consider directed abuse as it has a clearly identifiable speaker and listener.\vspace{5mm}
%
% \begin{table}[ht]
%   \centering
%   \scriptsize
%   \begin{tabular}{p{0.30\textwidth}|p{0.30\textwidth}|p{0.30\textwidth}}
%     {\bf Input} & {\bf System} & {\bf Output}\\\hline
%   ``Go kill yourself'' & Abusive Language Detection Model & Abusive\\\hline
%     ``(((@User))) and what is your job? Writing cuck articles and slurping Google balls? \#Dumbgoogles'' & Procedural System & $0.80-0.85$ likelihood to be abusive\\\hline
%     ``@User shut yo b**n*r ass up sp*c and hop your f****t ass back across the border little n***a'' & Counter Speech Generation System & ``Why do you think they should leave the country? This is their country too.''\\
%   \end{tabular}
%   \caption{Examples of abusive comments and the outputs of our systems.}
%   \label{tab:examples}
% \end{table}

% Beyond a consideration of speaker and listener, it is necessary to consider of predictive impact. That is whether a given model's performance treats all investigated demographics equally, given a definition of equality.\footnote{Several competing definitions of equality exist which influence the evaluation of whether a model is discriminatory or not.} Specifically for issues in which the labels are highly contextual, such as hate speech or offensive language, social biases on acceptable language may influence annotations and thus models which are learned on the data.

% In this Ph.D. we will seek to marry the findings and knowledge obtained in fields outside of NLP on the nature of abuse and notions of equal and fair treatment.

% As machine learning methods rely on the data sets and labels upon which they are trained \citep{Mitchell:2018,Waseem:2016}, we will aim to identify and address issues of bias in datasets and propose methods to address such issues. We will address the issue of bias by considering a number of datasets and variables which may give cause for discrimination.

% Online abuse is heavily situational and influenced by external forces such as social events, disasters, violence, elections, and political movements \citep{Marsh:2018,Burret:2017,Dearden:2019}. For this reason, we will consider a longitudinal investigation of online abuse as it occurs in targeting British members of parliament.


%Finally, we will aim to improve the generalizability of machine learning models for hate speech detection.

%\section{Research Questions}\label{sec:rq}

%\subsection{Abuse in Context: The legal and socio-technical landscape of abuse}

%As more countries and legal territories are considering or have implemented regulation about artificial intelligence

%As artificial intelligence becomes increasingly ubiquitous in society, there is is a growing need for a consideration of regulation of such technologies. This need has prompted the European Union to develop the ``Ethics Guidelines for Trustworthy Artificial Intelligence'' \citep{EU:TrustworthyAI}. In the case of content moderation, the European Union is also considering developing regulation of online dissemination of terrorist content, a proposal which may also have implications of abuse and hate speech in particular. Further, given the issues of disparate impact and harms of content moderation \citep{Gomes:2019}, it becomes necessary to consider the role of content moderation and how automated measures may be influenced by concerns of communities that they may be unfairly penalized by such technology.

%\begin{quote}
%  {\bf RQ1}: What are the implications of the legal realities, socio-technological and cultural contexts within which technology for abusive language detection exists for machine learning tools and what is the consequence for building tools for content moderation of abuse?
%\end{quote}


%\subsection{Hidden Variables: Minimising Disparate Impacts of Machine Learning Models Learned on Biased Annotations}\label{sub:critical}

%As more data sets for abusive language detection are released it is important to critically assess the hidden variables that exist in the data sets which impact classifiers: i.e. which type of abusive language they can aid in identifying \citep{Jha:2017,Waseem:2017} as well as which cultural phenomena they speak to.

%For instance \cite{Waseem:2018} and \cite{Sap:2019} find that the data set published by \cite{Davidson:2017} has a great deal of African-American Vernacular English (AAVE) labeled as either hate speech or offensive language. By training a classifier on this data set without ensuring that there are safeguards to prevent demographic discrimination, the trained model is likely have a preference for detecting that people who use AAVE are being either offensive or are using hate speech, in large parts disregarding whether the contents are truly offensive or abusive. Another issue that comes to light with the fact that AAVE is more likely to be tagged as offensive or hate speech is that the computational models built for removal, moderation, or even generation of counter speech, will explicitly make a judgment about the appropriateness of AAVE as a dialect. An implied role in abusive language detection is the responsibility of determining which forms of speech are socially acceptable; for this reason it is vital that seek to investigate which demographics, if any, are being singled out by the data sets we employ for training our models.

%As there are no published datasets for abuse that also contain information about the speaker, we will also consider datasets that do not relate to abuse, such as the Wang \citeyear{Wang:2017} fake news dataset and the PAN Author Profiling dataset \citep{PAN:2015} to demonstrate rebiasing, while we develop a dataset with annotation for dialectical use.

%\begin{quote}
%  {\bf RQ2}: How can we identify correlations betweeen classes and protected variables (e.g. race, gender, and political affiliation) in machine learning methods and can we minimise the bias in classifiers for NLP systems?
%\end{quote}

%\subsection{Out of Time: Temporal patterns in the development of abuse.}

%Many cases of abuse is situational and influenced by events external to the message being sent. Considering offline contexts, there is evidence suggesting that external influences such as terrorist attacks and elections can serve as catalysts for abuse and hate crimes \citep{Marsh:2018,Burret:2017,Dearden:2019}. In an online setting we see a similar pattern for the use of online hate speech, where a rise in hate speech follows external events \citep{Olteanu:2018}. Previous work on abusive language has primarily considered singular instances of abuse \citep{Davidson:2017,Waseem:2016}, constructivity of threads \citep{Napoles:2017,Kolhatkar:2017}, and the language and volume of abusive tweets \citep{Gorrell:2018}.

%While prior work has addressed event detection \citep{Chen:2018,Orr:2018} and, more topically, predicting riots \citep{Alsaedi:2017}, to the best of our knowledge no prior work has examined the temporal developments of abuse. Here we consider the dataset of abuse against members of British Parliament \citep{Gorrell:2018} and ask the questions of how abuse develops over time and predicting the moral sentiment \citep{Hoover:2019} of responses to tweets. The purpose of this study is to allow for early detection of abuse occurring in addition to identifying features of tweets which elicit a disproportionately abusive response.

%\begin{quote}
%{\bf RQ3}: What are the temporal developments of abusive language production towards British MPs and can these patterns be used for early detection of abusive responses.
%\end{quote}

% \zw{Tweets that illicit emotional responses and generate abuse | could also be co-reference resolution to identify whether the target is actually the original speaker}

%\subsection{Abusive Language Detection Using Machine Learning}\label{sub:detection}

%One large issue with the current state of datasets available for abusive language, is the relatively small scale \citep{Waseem:2016,Davidson:2017}. The small size of the datasets allow for easy overfitting to confounding tokens (e.g. ``islam'' \citep{Waseem-Hovy:2016}). Further due to the small sizes of the datasets the issue of cultural-specificity arises, in which the datasets represent only one specific culture, which acts as a further inhibitor to building models which are generalizable across data sets.
%Another challenge facing the field of abusive language detection is that the vast majority of abusive language data sets display large class imbalances with the majority class being documents that are not abusive, e.g. in \cite{Waseem-Hovy:2016}, where $31\%$ of the data is labeled as abusive spread over two classes ($20\%$ labelled as sexist and $11\%$ labelled as racist). As such classifiers are given the task of identifying abusive language while being given access to very little data and highly variable data in its notions of acceptability and demographic targeting. For a task such as abusive language and hate speech detection, where subjects such as racial and gender minorities are often discussed in both constructive and abusive ways, this means that classifiers are prone to over-fit to the majority class thus resulting in poor usability of the models and indeed, poor generalizability beyond the specific data set.\vspace{5mm}

%Most data sets that have been for abusive language detection do not allow for generalization beyond the given data set due to being very small e.g. in \cite{Waseem:2016} there are less than $100$ documents labeled in the minority class; the data is collected for a limited number of targets \citep{Waseem-Hovy:2016}; or some communities being overrepresented in the data sets \citep{Davidson:2017}. In addition, many annotated data sets for abusive language are collected on Twitter \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Jha:2017} which naturally influences a given model's performance on data obtained from other sources which may have not have length limitations on documents (e.g. comments or posts). Poor generalisability also influences moving from one data set to another \citep{Waseem:2016} which greatly reduces the usability of the models that are trained on a given data set. To overcome this issue of poor generalisability stemming from a limited number of targets, we will be collecting additional data which will be used to explore the utility of neural network models along with linear models. Further, we will seek to abstract away from the words in the documents to avoid over-fitting to specific terms. We will to utilize unannotated data from communities that are known to be abusive in order to address poor generalizability stemming from a limited number of target groups in the annotated data as well as the issue of documents that exceed Twitter's document length restrictions. Thus, we will seek to address the following research question:\vspace{5mm}
%\begin{quote}
%{\bf RQ4}: How can embeddings, weak supervision, linguistic annotation, and meta data help improve platform and topic independent detection of abusive language and hate speech?
%\end{quote}


%********************************** % Report Structure  *************************************
%\section{Report Structure}\label{sec:structure}

%In this chapter, we have introduced the tasks we'll undertake in the thesis along with the research questions and brief outlines of how we will address each research question. For the sake of the confirmation panel chapters \ref{chap:nlp}, \ref{chap:done}, \ref{chap:experiments}, and \ref{chap:ddp} deal with the requirements of the report, while \ref{chap:socialscience} deals with more background knowledge that further motivates and provides deeper understanding of the social scientific concepts and how they will be used.\footnote{In Chapter \ref{chap:socialscience} we also present our ethical considerations.}\vspace{5mm}

%\noindent Chapter \ref{chap:socialscience} presents the knowledge from Critical Race Theory, Gender Studies, ethics for social media research, and Psychology upon which we base our methodology and how we will use these concepts.\vspace{3mm}

%\noindent Chapter \ref{chap:nlp} presents the literature review on current approaches to abusive language detection and counter speech generation in NLP, and seeks to explain which methods we will use and how they will be used.\vspace{3mm}

%\noindent Chapter \ref{chap:done} presents the work that has been conducted thus far.\vspace{3mm}

%\noindent Chapter \ref{chap:experiments} details the tasks and approaches that will be carried out during this PhD project, as well as a tentative execution timetable.\vspace{3mm}

%\noindent Chapter \ref{chap:ddp} outlines the activities completed regarding the Doctoral Development Program (DDP), as well as other activities that have been carried out.\vspace{3mm}

%\noindent Appendix \ref{appendix:published} provides the full papers that have been published thus far.\vspace{3mm}

%\noindent Appendix \ref{appendix:ethics} provides the successful and pending applications for ethical approval. Successful applications also include the notification of acceptance.\vspace{3mm}

%\noindent Appendix \ref{appendix:draft} contains draft papers and papers which are currently under review.
