\ifpdf
    \graphicspath{{Chapter7/Figs/Raster/}{Chapter7/Figs/PDF/}{Chapter7/Figs/}}
\else
    \graphicspath{{Chapter7/Figs/Vector/}{Chapter7/Figs/}}
\fi


\chapter[State of the Art White Supremacy: On Disembodiment in the Machine Learning Pipeline]{State of the Art White Supremacy: On Disembodiment in the Machine Learning Pipeline\footnotemark{}}\label{chap:disembodied}
\chaptermark{State of the Art White Supremacy}
\footnotetext{This chapter contains elements from a collaboration with Smarika Lulz, Joachim Bingel, and Isabelle Augenstein. The associated paper is currently under review in the journal Computational Linguistics. The title is taken from a conversation between Abeba Birhane, Chris Dancy and myself, where Chris offered the term State-of-the-Art White Supremacy.}

\begin{citequote}{\citet[p.110-111]{Lorde:1984}}
What does it mean when the tools of a racist patriarchy are used to examine the fruits of that same patriarchy?  It means that only the most narrow parameters of change are possible and allowable.
\end{citequote}

In each of the previous chapters we have identified different areas of concern for the use of models and data.
From the constraints of document transformation in \autoref{chap:liwc} to the influence of multiple data sources and prediction tasks in \autoref{chap:mtl}.
\ZTedit{
  In each of these chapters, I sought to find means to make computational methods more closely come to represent the subjectivities and contexts of speakers from within frames of existing computational methods.
  As the chapters collectively point to, there is an inherent limitation to what is achievable within computational pipelines in which the entire process, from dataset creation to modelling, is not developed while respecting human subjectivities.
  Specifically, the need for finding ways to approximate contexts and subjectivities highlights how the current machine learning pipeline does not specify how subjectivities are embedded in these technologies.
  Understanding precisely where such shortcomings arise in the machine learning pipeline requires a deeper consideration of human embodiments within it.
  Moreover, it requires a deeper consideration of how machine learning, as an academic practice, presents itself and disembodies itself from the subjective human experiences that machine learning purports to be developed for.
  For this reason, I turn to considering how human embodiment and disembodiment happens throughout the machine learning pipeline.
  Here, I return to \textit{RQ I} by asking how subjective experiences are embodied in the machine learning pipeline (\textit{RQ 4}) and what the implications of this are (\textit{RQ 5}).
}
In this chapter, I then theorise over the core sources of these issues: the context within which models and exist, the models and the data.
To describe these issues, I invoke the metaphor of the body in three different ways: first, pertaining to the physical material \textit{human body} that we each possess; second, to signify a collection of observations and data points \textit{created by humans}; and third, to refer subjective embodiments, that is how \textit{social and cultural meaning} is embedded in the human experience and derivatives of it, i.e. data created by humans.
\ZTdelete{Through a consideration of both data generation processes and the modelling stages of the machine learning pipeline, I apply my theory to the models that I have described in this dissertation in addition to providing a critique of these technologies.}
\ZTedit{I then apply my theory to the computational models in \cref{chap:liwc,chap:mtl} and apply a critique of these techologies through a consideration of the data generation process and the modelling stages of the machine learning pipeline.}
Finally, I discuss the implications of current practices in machine learning and argue that for machine learning for social prediction tasks to achieve their goals, we must radically reconsider current approaches to better align with the stated aims.

\section{Disembodied Machine Learning}
Machine learning is a practice that is concerned with making decisions based on machine-discernible patterns in observed data.
Often, the data \ZTedit{used to optimise machine learning methods are `extracted' from the context within which they are created, i.e. by ``scraping'' online platforms for user-generated content.}
\ZTdelete{upon which machine learning methods are optimised, and later applied to, are `extracted' from the context within which they are created.}
Through this process of \ZTdelete{separation of}\ZTedit{separating} context and datum, a notion of `objectivity' is imposed upon the data and the subsequent operations, \ZTedit{i.e. optimising machine learning methods} on the data and their results \ZTedit{further entrench this notion}.
\ZTedit{Datasets, or bodies of data, are thus created through a repeated separation of datum from context.}
\ZTdelete{Through the repeated separation of datum from context bodies of data, or datasets, are created.}
These amalgamated bodies of data exist only by virtue of their strict separation from the material bodies \ZTedit{from which the datum are derived.
These disembodied and amalgamated bodies are then used to optimise machine learning models.}
\ZTdelete{that they derive from are then used to optimise machine learning methods.}
Machine learning methods come in two different forms: Supervised learning methods which seek to \ZTdelete{learn to} distinguish \ZTdelete{between}\ZTedit{distinct limbs which are pre-drawn, e.g. classes, from the data}
\ZTdelete{bodies of data}, and unsupervised models which seek to identify discernible limbs of data within a single body of data \ZTedit{without direct guidance from designers}.
For both supervised and unsupervised models \ZTdelete{both} the underlying data and the models applied to them have strong influences as to what bodies are discovered and what may be discovered within \ZTdelete{them}\ZTedit{the data}.
As \citet{Benjamin:2019} writes technology operates within social structure `codes [that] operate within powerful systems of meaning that render some things visible, others invisible, and creates a vast array of distortions and dangers'.\vspace{5mm}

\ZTedit{With the advent of machine learning, a new technology came to be hailed as objective and unimpeded by subjective human biases, and by extension social marginalisation \citep{oneil:2017}.}
\ZTdelete{In the advent of machine learning, these models were hailed as objective, unimpeded by subjective human biases, and by extension social marginalisation \citep{oneil:2017}.}
However, an increasing amount of research suggests that social biases are common in machine learning models \citep{Shah:2020,Buolamwini:2018,Agarwal:2018}.
\ZTedit{Moreover, research has found} that biases in the underlying data may be exacerbated by the machine learning models \citep{Zhao:2017,Jia:2020}.
As a result of this growing awareness \ZTedit{of the emergence of social biases in machine learning models, there has been} a number of research directions seek to identify \citep{Shah:2020,Bender-Friedman:2018,Mitchell:2019,Buolamwini:2018}, reduce or remove social biases \citep{Zhao:2017,Agarwal:2018,Romanov:2019,Jia:2020} from machine learning models to prevent further marginalisation.
\ZTedit{However, s}uch work assumes that social biases operate within a positivist logic \ZTdelete{in which the removal social bias is cast as an optimisation problem}\ZTedit{which casts the removal of social biases as an optimisation problem.
That is, this work assumes that bias is a finite and quantifiable entity that can be}
\ZTdelete{i.e. that there is a finite and quantifiable source of bias that can be} disentangled, isolated, and mathematically reduced out of the body of data or mathematical model from which the designer \ZTdelete{of both models and data are removed}\ZTedit{is disembodied}.

Here, I provide a challenge to such a positivist logic. 
Drawing on work from feminist Science and Technology Studies and examples from Natural Language Processing, I argue that bias and subjectivity in machine learning pipelines are inescapable and can therefore not be simply be reduced or removed.
\ZTdelete{for which reason}\ZTedit{Therefore,} I hold that an ongoing recognition and reflection on our own positions and the \ZTdelete{imaginary}\ZTedit{fiction} of objectivity found in subjective realities, reflect political choices throughout the machine learning pipeline.
Through a \ZTdelete{contextualisation}\ZTedit{conceptualisation} of bias in these terms, I aim to shift the surrounding discourse away from bias an its elimination to subjective positionality and its implications on the machine learning pipeline from data generation to optimised model.

\subsection{Embodiments in the Machine Learning Pipeline}\label{sec:ml_embodiments}
Through \ZTedit{Donna} Haraway's \citeyearpar{Haraway:1988} critique of objectivity (see \cref{chap:socialscience}) it is possible to \ZTdelete{understand}\ZTedit{rethink how } subjectivity\ZTdelete{, or biases} \ZTedit{is embedded} in machine learning.
\ZTedit{Rethinking subjectivities in machine learning affords a recognition of machine learning's}\ZTdelete{in a way that recognises its} potential to create social marginalisation without casting the problem in a positivist, optimisational logic.
\ZTedit{
  That is, without casting the issue as an issue of ``debiasing''---a problem that purports to be an optimisable problem.
  In fact, reframing the issue of socially biased machine learning systems away from such positivist fantasies allows us to view machine learning systems as embedding, and embedded in systems of oppression.
  By viewing machine learning systems as co-constitutive of the social systems within which they are embedded, it becomes clear that mathematical approaches to ``debias'' machine learning technologies seek to recast and reduce the issue of discriminatory social systems into issues of statistics and mathematics.
  Moreover, as the machine learning pipeline relies on data created within discriminatory social systems, i.e. by humans who constitute and are subject to such systems, the fantasy of ``debiasing'' only serves to obscure how machine learning systems are co-constitutive of such discriminatory social systems.
}
I argue that the disembodied, or ``objective'' positions exist within the machine learning pipeline at multiple junctions:
\begin{enumerate}
  \item{In the data which is often removed from context and potentially adjudicated by externalised others,}
  \item{in the model optimised on the disembodied data stemming from embodied data subjects, and}
  \item{in the person designing the experiment and pipeline.}
\end{enumerate}

\ZTedit{
  In constructing datasets for machine learning, a series of decisions about the data are made at different levels of granularity---from selecting a source of data to specific means of operationalising it.
  These decisions come to represent how contemporary machine learning methods disembody the speaker from their speech.
  At a higher level, designers of machine learning infrastructures make decisions that impact every aspect of the pipeline.
  In their decisions, designers specify what counts and what does not count as relevant information and how such information should be represented by machine learning models.
  Finally, once data has been gathered models are optimised on disembodied data from embodied subjects.
  In this way, the model becomes embodied through an amalgamation of limbs that have been disembodied from embodied data subjects.
  % This amalgamated-embodied model may then be deployed and exert power over those who have not been involved with any stage of the development process, and those who have alike.
  % An important exception here is that designers of machine learning pipelines, if and when they are subject to the machine learning pipeline, they have the resources to change and modify the pipeline such that they do not experience any negative consequences from it.
}

\ZTedit{
When constructing datasets for machine learning, including datasets for content moderation, it is necessary to make decisions on that delineate individual pieces of datum as relevant or irrelevant to the task across several layers of granularity.
First, one must consider how to obtain a large sample of content which may contain the phenomena under study.
For instance, in developing a resource for online abuse a decision must be made to which online communities, topics, or types of discourse may provide a large enough sample for study.
The data which is collected is often produced by a large number of people on online platforms.
Often, this process does not include collecting all posts produced by the individuals in the sample.
Instead only posts that pertain to the phenomena under study are collected.
In this way, a first step is made towards disembodying the sampled data from the individuals who have created it.
In NLP, the primary focus of interest is in the text for which reason data about the user such as the name they provide (username and provided name), their location, and other meta data are often discarded.
Thus, a second step is made towards disembodying the creator of the content, the speaker, from the speech that they produce.
Often, the discursive structure, such as a posts and replies are flattened, which further disembodies the speech act from the context within which it is produced.
Thus, data is disembodied from the social and political contexts within which they are created.
}

\ZTedit{Often an initial data sample is large to ensure breadth in the sample and that as much evidence towards the phenomena under study is collected.
A second level of granularity in the data sample is then performed by selecting a smaller sample to study, within the larger sample.
Here designers may seek to qualify and disqualify certain samples in their collected data, as some sections may not be pertinent or may only infrequently contain the phenomena under study, as it is conceptualised by the designers.}

\ZTedit{In the case of supervised machine learning, the data is passed through a third level of granularity.
Here,
}
\ZTdelete{
First, as data collection processes require decisions to be made to delineate individual pieces of datum that are relevant to the task from those that are deemed irrelevant, it is often necessary to also create a separation between the person who created the datum and the datum in and of itself.
Thus, a datum is removed from the context of its creator.
Further, datum is often disassociated with the time and the social and political contexts within which it is created.
In the case of supervised machine learning,} the datum is \ZTdelete{then} provided to a number of annotators, who \ZTdelete{frequently are neither}\ZTedit{are rarely} the creators of the datum.
\ZTedit{Moreover, it is the exception that the annotators are situated within the contexts of the creators of the datum.}\ZTdelete{necessarily situated with contexts of the creator or the datum.}
These annotators then determine which limb of data i.e. the class, within the larger body of data, the datum belongs to given a set of criteria for such judgement.
\ZTedit{
These criteria are provided by the designers of the pipeline.
}

\ZTedit{
Turning to the optimisation technologies.
In their operation on data, machine learning models encompass different ways of embodying and disembodying data.
Specifically, in the optimisation process machine learning models operate on disembodied data and further disembody them from the speakers through mathematical processes with the goal of settling on a distinct embodiment derived from the data.
The disembodiment that the optimisation process performs happens through a manipulation of the data representations to draw discernible boundaries between the limbs of data, i.e. the classes.
The underlying assumption that machine learning models make is that the data provided is all that there is necessary to know to draw \textit{meaningful} decision boundaries.
It is then up to the designer to discern whether the decision boundaries drawn are truly meaningful or they represent spurious correlations.
Making this decision however has proven to be large a challenge as recent research on the challenges of benchmarking, that is evaluating the performance of machine learning models has shown \citep{Kiela_2021,Bowman_2021}.
}
\ZTdelete{Second, as the designer of the model themselves are frequently removed from the contexts within which the body of data are created, they determine how the data are to be represented through a choice of features, setting limits on vocabularies (see \autoref{chap:intro}), transforming the input (as seen in \autoref{chap:liwc}), and the choice of models with their requirements of representing data.
Third, as machine learning models step through disembodied data and embody it within itself, the model itself manipulates the representations of data to identify discernible boundaries between the limbs of data.
In this process, the models further disembody the data from the data itself, operating within an assumption that datum consist of the sum of its parts.
}

\ZTedit{
Finally, a great deal of attention has been given to how the lack of inclusion of designers across axes of identity can contribute to the producing socially biased systems \citep{West:2019,Holstein:2019}.
However, the ways in which designers embed themselves in the machine learning pipeline can be unclear.
I argue here that through choices that designers make in the process of developing these technologies, e..g how to represent data, how features are selected and limits are set on vocabulary, designers come to embed their own subjectivities into the machine learning pipeline.
In spite of being deeply embedded in the machine learning pipeline and technologies, designers are rarely subject to the machine learning systems or a part of the data that they rely on.
}

In each of these \ZTdelete{situations}\ZTedit{aspects} lay \ZTedit{a large number of} value judgements on \ZTedit{the perspectives of data that are deemed relevant}\ZTdelete{which perspectives of the data are relevant and which are irrelevant}.
I observe here a peculiarity of \ZTedit{the} machine learning \ZTedit{pipeline}.
\ZTdelete{as}\ZTedit{When} data \ZTdelete{which} is disembodied from its creator \ZTdelete{then}\ZTedit{the data} becomes \ZTdelete{the}\ZTedit{an archive, a} body of knowledge upon which the machine learning model draws on.
\ZTedit{In drawing upon the archive, machine learning models} implicitly transform all positions that exist outside of the model's internal body\ZTedit{, i.e. the archive} \ZTdelete{then }becomes disembodied from the model.
This transformation from disembodied to embodied then can serve as an explanation for calls for `more' and `more diverse' data \citep{Holstein:2019}.
It is worth noting here that the model-embodiment is tacitly acknowledged in the research fields of domain adaptation \citep{Daume:2007} and transfer learning.
Through these fields' acknowledgement that to the knowledge held in machine learning models are limited to the domains that are present in the datasets the models are optimised on and that even small perturbations in the input to the model may drastically degrade its performance~\citep{Szegedy:2014,Daume:2007}.
These acknowledgements of embodiment exist in a self-contradictory tension with the position of objectivity within which these transfer-learning and domain adaptation methods operate within.

\ZTedit{
\subsection{Embodiment in Data}\label{sec:data_embodiments}
As \citet{Gitelman:2013} argues, datasets do not exist naturally but must be produced.
Considering this production of data through \citet{Haraway:1988}, datasets can be understood as a form of knowledge that is produced through disembodying embodied experiences.
Subjectivity can thus stem from a number of sources including the source of the data \citep{Gitelman-Jackson:2013}, the data sampling method \citep{Shah:2020}, and the selection of annotators \citep{Waseem:2016,Derczynski:2016}.}

\ZTedit{Grounding my discussion in Natural Language Processing, I show how subjectivity manifests itself in machine learning models through a number of meaning-making processes, modelling choices, and data idiosyncrasies.
I seek here to highlight the subjective and embodied nature of of data and classifications and that by taking a position of objectivity, we cannot do justice to the needs and wants of individuals or communities.}

\ZTedit{
\subsubsection{Natural Language Processing Tasks}
}\ZTedit{
A range of, if not all, Natural Language Processing tasks are highly sensitive to the subjective values encoded in data.
While such issues have frequently been studied in the context of high-level tasks, such as machine translation and abusive language detection, less attention has been given to core Natural Language Processing tasks.
Notably, the primary object of study of biases in core Natural Language Processing has been the Part of Speech tagging task \citep{Blodgett:2016,Jorgensen:2016} for which reason I also investigate the task.
Generally, I argue that by removing the adjudication of content, be it \ZTedit{for} abusive language, translations; text simplification; part of speech tagging; or any of the many other tasks natural language processing as a field has and is addressing, from the user experiencing the phenomenon, I delegitimise the very tools that are built through a cloak of presumed objectivity which is neither truly neutral nor objective.}

\ZTedit{
\paragraph{High-level tasks}
High-level tasks that require semantic and pragmatic understanding, e.g. machine translation, dialogue systems, metaphor detection, sarcasm detection, and abusive language detection are all highly sensitive to subjective values encoded in the data.
In machine translation, research has identified a range of issues including stylistic \citep{Hovy:2020} and gender biases \citep{Vanmassenhove:2018}.
Particularly issues that pertain to the reinforcement of sexist stereotypes have been the object of academic \citep{Zhao:2017} and public \citep{Locklear:2018} scrutiny. 
A classic example of stereotypical translation are the translations stereotyped occupations from a language that does not contain grammatical gender to a language that does, e.g. the translations of \textit{doctor} from English (unmarked for gender) to the German Arzt (marked for masculine) and \textit{nurse} from English (unmarked for gender) to Krankenschwester (marked for feminine). 
Here we see that the `objective', yet stereotypified translations are embodied in a patriarchal context which delegates high prestige to men and low prestige to women.
While the translations may be correct in individual cases, they are not always correct.
Moreover, assigning a single gold label to a given translation in itself provides an issue as an input text may have several distinct and correct translations.
However, most optimisation processes and evaluation algorithms assume that there exists a single correct translation, and that is the one the model is provided for optimisation and evaluation.
I note that word embeddings similarly harbour stereotypical associations \citep{Bolukbasi:2016}.}

%similarly to the example of translations of \textit{doctor} and \textit{nurse} in machine translation, word embeddings have been shown to harbour similar stereotyped associations, e.g. that the feminine equivalent of masculine-coded \textit{programmer} is to \textit{homemaker} \citet{Bolukbasi:2016}.

\ZTedit{The issue of highly subjective `truths' and gold labels for data extends to several other tasks such as text simplification and abusive language detection.
In text simplification, numerous datasets make the claim that some words, sentences, or texts are difficult to read while others are easy.
These labels are typically provided by human annotators who may agree on some labels and this agreement may aid in the ability of models optimised on such data to generalise to other datasets.
However, the process of externalising the labelling process disembodies the data and subsequent models from the embodiments of the diverse set of users of simplification technology and how text difficulty manifests for those users \citep{Bingel:2018}.}

\ZTedit{Further, as is apparent in abusive language detection, if the positionality of the adjudicators deviate within the group of adjudicators less consistent annotations can be derived \citep{Waseem:2016}, which harm both the model and the supposed users of it.
Many other causes and effects of disembodiment have been considered in the task of identifying abusive language.
For instance, \citet{Waseem:2018} argue that datasets for abusive language embody a white perspective on respectability, finding that almost all uses of the \textit{n-word} are tagged in the positive class in the dataset released by \citet{Davidson:2017} regardless whether its use is within the African-American community.
The labelling of the \textit{n-word} does not necessarily embody a white perspective on respectability as the word does have frequent pejorative uses \citep{Croom:2013}, however disregarding the usage of the word within the black diaspora, as datasets and tools frequently do \citep{Davidson:2019}, does constitute a white supremacist idea of control of marginalised bodies and languages, for which there is a rich history \citep{Craft:2020}.
Indeed \citet{Waseem:2018} find that a large subset of the documents that contain the \textit{n-word} in \citet{Davidson:2017} that are labelled as hate speech and offensive language are likely to be in-group uses.
This issue however is not limited to the dataset presented by \citet{Davidson:2017}, in fact, all datasets examined by \citet{Davidson:2019} result in consistent and disproportionate error rates for African American English speakers.
Systems built on these datasets, or as I argue here, datasets that are constructed within a social order where the white cisgender male body is constructed as the `neutral' or `objective', will replicate such biases.
Thus, the race towards state-of-the-art machine learning models for content moderation is also a race towards state-of-the-art white supremacy.}

\ZTedit{
\paragraph{Core Natural Language Processing tasks}
Beyond these issues existing in high-level tasks which may require a certain level of cognitive abstraction, they also exist in lower level, core Natural Language Processing tasks such as Part of Speech tagging and dependency parsing.
While I restrict the examples here to part of speech tagging, I contend that precisely the same arguments apply to dependency parsing.}

\ZTedit{Considering part of speech tagging, I find multiple junctions at which theory and data influence the process of developing tag-sets.
First, the tagset is developed based on a subjective linguistic theory that licenses some tags while rejects other.
This linguistic theory is typically informed by observations on specific types language in the data it is developed to describe.
Second, in the choice of sources of data.
If the observed language production is a forum dedicated to computer games, the linguistic theory used to develop the part of speech tagset, the linguistic theories that form the basis of the tagset are likely to focus on informal, and perhaps adolescent communication patterns.
If on the other hand, the source of data primarily consists of newswire documents, the linguistic theory is likely to specifically address language production in formal settings.
Third, in the development of a dataset of part of speech tags, I see similar issues of adjudication as for the high level tasks.\footnote{Although this may be mitigated by using optimised linguists to label the dataset.}
Thus, the development of part of speech tag-sets and datasets it is applied on is a practice in developing descriptors and data which are mired in the context of the language production they seek to describe.}

\ZTedit{An example of one such tagset is the Penn Treebank tagset \citep{Marcus:1993}, the \textit{de-facto} standard for describing English word classes in Natural Language Processing.
The Penn Treebank tagset was developed on primarily financial newswire text and published works in the \ZTdelete{U.S.}\ZTedit{United States of America} in 1961 \citep{Francis:1982}.
The tagset was primarily motivated by economic factors, such as there being several word classes that were ambiguous or word classes which occurred with such low frequency that they might only describe a single word.
The Penn Treebank tagset was thus developed with formal Standard American English in mind and is thus better suited to describe language which conforms to the English the underlying theory the tagset describes than other varieties, sociolects, or slang \citep{Blodgett:2016,Jorgensen:2016}.
This issue becomes even more drastically apparent when a tagset developed for English is forced upon some other language, which it is far removed from being able to describe.}

\subsection{Embodiment in Modelling}\label{sec:model_embodiments}
While datasets are an important source of how a model may be embodied, machine learning methods themselves encode which embodiments are highlighted and which are subjugated.
I primarily focus on supervised machine learning in the exploration of how models exacerbate disembodied positions, as unsupervised methods are more directly volatile to subjective choices of the researcher, e.g. how the data is represented and which parameters the model is subject to.

As I seek to distinguish distinct model behaviours, I offer that models act on a spectrum from \textit{localized} to \textit{global} behaviour.
In this conceptualisation, localized behaviour refers to when a model seeks to ground the datum within the context it is derived from, often using knowledge external to the training data, e.g. context-aware models \citep{Garcia:2019,Devlin:2019}.
Conversely, global modal behaviour instead operates only within the realm of the training data it is optimised on, i.e. models that compound multiple senses of a word with little or no regard to their local contexts.
Although language production is situated within a wider socio-political context of society, I limit my use of `context' to the entirety of the sentence provided to the model.

By virtue of the subjective nature of embodying a datum within its context, there is large variation in how locally acting models may be developed.
One tactic to situate datum within its context is through the use of transfer learning which allows for knowledge produced outside of the training data to alter what the model embodies.
For instance, should a dataset embody the language production within multiple sociolects, through the use of pre-trained language models \citep{Devlin:2019} or mixed-member language models \citep{Blodgett:2016} deeper information about the sociolects in question can be provided by using the context of the sentence to identify how to situate the representation of a document.\footnote{As different language and dialectal varieties may not be equally distributed in training data for contextual models \citep{Dunn:2020}, similar issues of which bodies are given the privilege space plague such models \citep{Tan-Celis:2019}.}
The Multi-task learning paradigm also offers an avenue for embodying data in their contexts through their ability to encode information about the creator of the datum \citep{Benton:2017,Garcia:2019}.
Transfer learning can similarly be applied to direct the model to embody the context a datum is derived from.
For instance, \citet{Romanov:2019} encode demographic information of the datum's creator into the model in efforts to deter models from learning stereotyped representations of marginalised speakers and communities.

Globally acting models on the other hand do not afford such embodiment.
Through their reduction of a features in a model to a single sense, they are inherently unable to take into account the embodiment of the author, even if they are provided signals for how to embody a document at optimisation and inference time due to the fact that such models remake meaning according to the distribution of features.
Any step taken towards embodying datum in its original context move globally acting models along the spectrum towards being locally acting models.
An example of such a step are word embeddings.
Through their representation of words by the words that co-occur with the word's neighbouring words, thus assuming a similarity between the word and other words.
While they provide a slight shift towards locally acting models, the frequency-based nature of how closely associated a word is, they fail to take a meaningful step away from being globally acting models, as all instances of a token occurring in the dataset will be reduced to a singular representation that does not take the surrounding context, i.e. the sentence, into account.
It is important to note here that while word embeddings, and in fact contextual word embeddings provide a step towards localising models, the techniques of developing such embeddings rely on processes of disembodying a large set of data from their creators and constructing an amalgamated body of data that can collective embodiments.
This amalgamated data carries with it many small influences of the specific subjective embodiments of each data creator.

\subsection{The Embodied Designer}
Though often referred to as a `researcher' or `developer', I draw on Herbert \citet{Simon:1969} to construct my understanding of a \textit{designer}.
I direct attention not to the profession of the individual or team in the machine learning pipeline but instead to the their action.

\begin{citequote}{\citep[p. 111]{Simon:1969}}
  Everyone designs who devises courses of action aimed at changing existing situations into preferred ones.
\end{citequote}

Following \citet{Simon:1969}, the \textit{designer} can then be understood to be anyone in the machine learning pipeline.
While this includes annotators in addition to developers and researchers, I direct focus to the last two, i.e. the designers, as as they direct annotators and can supersede the choices made by the annotators.

\ZTedit{
  The designer is embedded in the machine learning pipeline by virtue of the choices that they make throughout the development process, from the initial conceptualisation of the task to the final optimised system.
  All decisions that are and described in \cref{sec:data_embodiments} and \cref{sec:model_embodiments} are either directly or indirectly made by the designer, in efforts to shape the final optimised model such that it fits the subjective positions of the designer.
  Direct decisions such as the choice of model, how to pre-process the data and transform it are direct decisions made by the designer.
  Indirect decisions refer to instances where the designer relinquishes control over some part of the process, for instance in annotation.
  Annotations are indirect decisions as the annotation guidelines are developed by the designer.
  The decision on how the guidelines are to be operationalised however is a matter that is predominately controlled by the annotators, as they internalise and operationalise the annotation guidelines according to their own lived experiences and subjectivities.
  Moreover, should subsets of the annotations not agree with the positions that the designer holds, then the designer can choose several ways in which to disregard the data labelled by the annotators.
  In this way, although designers relinquish some control through the annotation process, they maintain, and often exert power over the result of the annotation process.
  Through control of these decision making processes, the designers exert power and embody their own subjectivities into the machine learning pipeline.
}

\ZTedit{An oft proposed solution to the issues of socially biased machine learning systems is to diversify the teams of designers who are developing the technologies \citep{West:2019,Holstein:2019}.}
\ZTdelete{Often a lack of diversity in machine learning development teams is cited as a source of socially biased technologies along with corresponding calls for an increase in designers embodying diverse experiences \citep{West:2019}.}
\ZTedit{This line of work has a similar argument to mine, i.e.}
\ZTdelete{Similar to my argument, such calls argue} that the subjective designers project an embodiment of self into the technologies that they develop through the data and modelling choices that they make.
\ZTedit{Drawing on \citet{Haraway:1988}, this then suggests that the God trick that machine learning methods employ is a reflection of the ways in which the subjectivities of the designers are embedded in the systems.
Rather than calling for diversifying the identities of the group of designers behind a tool, I argue that it is only through the recognition of ones own subjective embodiments that the issue of socially biased machine learning can be addressed.
That is, it is only by recognising ones own subjectivities and actively making choices to represent the subjectivities of those that the technologies will be applied to, that one can hope to develop machine learning technologies that do not produce socially biased outcomes when applied to the target user group.}
\ZTdelete{This argument, in line with \citet{Haraway:1988} suggests that it is only through the recognition and promoting different embodiments that certain perspectives, understandings, and uses can be achieved.}

\section{Embodiment and Disembodiment in the Abusive Language Detection Pipeline}
In the above sections, I describe generally how subjective embodiments are manipulated and inserted throughout the machine learning pipeline for the general case.
In this section, I turn my attention to abuse detection technologies in an examination of the subjective embodiments for this particular application of machine learning.

As with many machine learning pipelines, abusive language detection pipelines can have different starting points depending on whether any data is being annotated, or previously annotated data is used.
For the latter case, the considerations of feature and model selection are particularly relevant to the development, however designers of models should be aware of the influences of subjective embodiment in the annotation process and as the effects of the annotation process remain in the dataset.
One such effect is of the designer of the dataset is that the subjective embodiments of the designers (and annotators) permeate through every step of the pipeline, as I have argued in the previous sections.
For this reason, I address how the subjective embodiments of the designer influence each step of the pipeline in the subsections below.

\subsection{Annotation Guidelines}
Perhaps the most clear case of subjective embodiments being inserted into pipeline is apparent in the annotation guidelines.
For the abusive language detection there is no consensus on how to operationalise abuse \citep{Waseem:2017}.
This lack of consensus leads distinct groups of designers \ZTdelete{to create their}\ZTedit{creating their} own guidelines on the basis of distinct sources and understandings of abuse.
The choice of which background source is used depends strongly on the researchers.
For instance, \citet{Waseem-Hovy:2016} rely on critical race theory and gender studies to inform their annotation guidelines.
Conversely, \citet{Davidson:2017} rely on social media platforms' community guidelines to define abuse, and \citet{Fiser:2017} rely on Slovenian legal definitions of hate speech to inform their annotation guidelines.
These distinct motivations in part are informed by the cultures within which the researchers exist.
For instance, the designers behind \citet{Davidson:2017} are situated in the \ZTedit{United States of American and their annotation guidelines are thus contextualised by the highly permissive freedom of speech protections enshrined by the second amendment of the constitution of the United States of America.}\ZTdelete{context of the freedom of speech protections in the United States of America.}
The aim of their work, distinguishing hate speech from otherwise offensive content, can then be understood to be motivated by the issue of incorrectly labelling non-hateful entries as hateful, \ZTedit{which could be read as contrastive to the freedom of speech protections in the United States of America.}
\ZTdelete{in contrast to the strong freedom of speech protections that exist in the United States of America.}
On the other hand, \citet{Waseem-Hovy:2016} seek to address the issue of discriminatory speech, motivated by the harassment of women on social media.
Their understanding of hate speech is then motivated by ensuring protection of marginalised communities, in part due to their belonging to a marginalised community.
Thus, while annotation guidelines are strongly argued and motivated, the local embodiments and contexts of the authors influence the guidelines that they create.

\subsection{Sampling data}
Beyond distinctions in the annotation guidelines, the sampling of data similarly is influenced by the subjective embodiments of the designers, resulting in distinct datasets examining different geographic cultures \citep{Waseem:2018}.
\ZTdelete{For instance, the}\ZTedit{These} distinct motivations \ZTdelete{also} influence the \ZTdelete{topics}\ZTedit{questions} that are under investigation \ZTedit{in the research of different groups}\citep{Waseem:2018}.
\ZTedit{For instance,} \citet{Fiser:2017} detail a framework based in the Slovenian legal context, where the authors of the study reside, directing the hate studied to be directly addressing hate occurring in Slovenia.
Similarly, \citet{Davidson:2017} seek to examine in hate in the United States of America, further they also limit their data sampling to tweets posted from inside the United States of America.
Finally, \citet{Waseem-Hovy:2016} specifically seek to address abuse towards women and other minorities, notably religious minorities \ZTedit{and therefore do not limit the selection of data to any particular geography}.
In this way, datasets reflect more than investigations into different aspects of abuse.
\ZTedit{The dataset also reflect the}\ZTdelete{they also result in the} specific interests and values of the designers as they choose sampling strategies that align with such interests.
\ZTdelete{Additionally, it's}\ZTedit{It is} worth noting here that the source of funding for the construction of the pipeline may also hold influence.
For instance, grants from government agencies may specify that abuse must be considered within a national context or geographic territory.

\subsection{Annotators}
Another source of subjective embodiments \ZTdelete{being}\ZTedit{that are} encoded into \ZTedit{the} data is \ZTdelete{through} the annotation process\citep{Waseem:2016}.
As \citet{Waseem:2016} show, distinct groups of people will internalise and operationalise annotation guidelines \ZTdelete{depending on the values they hold.}\ZTedit{according to their pre-existing values.}
As such, groups of human workers will annotate data according to their values, resulting in annotations that \ZTdelete{highlight}\ZTedit{embed} how different people and groups \ZTdelete{of people} operationalise the annotation guidelines.
This has strong implications for abusive language detection datasets\ZTdelete{through the implication that}\ZTedit{ as what these allow for modelling is then annotators' views on acceptability}.
Unless annotators are carefully selected and educated, the annotations derived from \ZTdelete{each annotator, or} groups of annotators \ZTdelete{with similar backgrounds and values }may be internally incompatible with the \ZTdelete{remainder}\ZTedit{with other groups} of annotators \ZTedit{who are working on constructing the same dataset}.

Annotators \ZTdelete{too} are \ZTedit{also} subject to the embodiments and goals of the designers.
Such influence is \ZTdelete{afforded the designers} is wielded \ZTedit{by the designers} through directly influencing aspects of annotations, such as the annotator selection \citep{Waseem:2016}, training \ZTedit{of annotators}\citep{Vidgen:2020}, \ZTedit{the} guidance that is provided \citep{Palmer:2020}, \ZTedit{the} selection of annotations to use \citep{Hovy:2013}; and through indirect selection \ZTedit{criteria}, such as payment-level for annotation \citep{Sabou:2014}.

As \citet{Hovy:2013} show, the reliability of annotations is important to the successes of any subsequent task, however the question of what constitutes a reliable annotation is one that reflects the designer's positions on `correct' labelling for \ZTedit{a given} task.
In terms of abusive language detection, `high quality' annotations thus reflect how the designers envision the task \ZTedit{of abuse detection} and the embodiments that the \ZTedit{designers} operate within.
Consider for instance a pool of annotators with diverse and divergent political positions tasked with annotating hate speech.
If the designers' understanding of what constitutes hate speech does not align with a sub-group of annotators, those annotations can then be disregarded as they do not conform \ZTedit{to the subjective position of the designers}.
However, considering the positionality of the \ZTedit{divergent} sub-group, their annotations may be entirely consistent with how they operationalise hate speech \ZTedit{and their own subjectivities}.
For instance, should a group of people who politically self-identify to be on the far-right form a sub-group of annotators, then their operationalisation of hate speech is likely to diverge in \ZTedit{key} areas from the remaining annotating population, while being consistent with their own operationalisation of hate speech.
In such a case, the designers are likely to disregard their annotation to ensure that the resulting data aligns with their own aims and \ZTdelete{hopes}\ZTedit{subjectivities}.
As \citet{Waseem:2016} argues, annotators form their operationalisations of hate speech on the basis of the annotation guidelines and will embed their own subjective positioning into the resulting data.

This issue exists not only for subsets of the annotation pool, entire pools of annotators may also consistently label within the designers' expectations, yet in \ZTdelete{contrast}\ZTedit{conflict} to the annotation framework.
In one such instance, \citet{Davidson:2017} find that ``[h]uman coders appear to consider racists or homophobic terms to be hateful but consider words that are sexist and derogatory towards women to be only offensive''.
Such divergences in labels towards groups is inconsistent with the annotation guidelines \ZTedit{provided by \citet{Davidson:2017}}. 
However, the authors \ZTdelete{note}\ZTedit{highlight} this as a strength of their annotation framework, \tdelete{through the affordance of distinguishing}\ZTedit{arguing that their annotation process allowed for distinguishing} between hateful and offensive content, \ZTedit{even if such distinction runs counter to the guidelines provided}.
Where there are distinct sub-groups within the data, selecting which group to consider has bearing on the internal consistency of the dataset and subsequently on any patterns a model might learn \citep{Waseem:2016}.
This leads \citep{Waseem:2016} to conclude that the selection of annotators should follow processes that allow for identifying, if not recruiting, annotators that share backgrounds and align on socio-political issues.
Such discrepancies can \ZTedit{also} be addressed through annotator training, as \citet{Vidgen:2020} show.
In instituting annotator training and addressing discrepancies between annotators, the designers directly train the annotators to reconstruct the embodied positions on hate speech that \ZTedit{the designers}\ZTdelete{they} hold.
\ZTedit{Thus, the designers}\ZTdelete{ and} train the annotators to disregard their own \ZTdelete{notions}\ZTedit{operationalisations} of \ZTdelete{what defines} hate speech \ZTedit{in favour of the designers' operationalisation}.

Finally, \citet{Sabou:2014} argue \ZTedit{that} designing the task and setting the payment level can indirectly influence which annotators put themselves forward to work on the task.
To attract `good' annotators, it necessary to set payment for each annotation, using incentives such as high payment per document labelled.
In this way, annotators are incentivised to learn the subjective \ZTdelete{positionings}\ZTedit{positions} of the designers.
For a great deal of work in abusive language detection, the task and data are further disembodied in the annotation selection process as the annotators are unlikely to appear in the dataset. 
\ZTedit{Thus, by adding an additional layer of disembodiment through the adjudication process that operates on already disembodied data, the annotation process further disembodies the data, and subsequently the model, from the context within which the data are derived from.}
\ZTdelete{or have the knowledge that they may be in the dataset, thus by adding an additional layer of disembodiment and embodiment, through adding an adjudication layer on disembodied data then further disembodies the model from the contexts they are derived from.}
One study however diverges from this notion of universal understandings of what abuse constitutes \citep{Arora:2020}.
By asking the very journalists who are a target of abuse \ZTedit{to perform annotation work}, they ensure that the labels that are associated with each data point is embedded within the subjective positioning of each journalist.

% TODO: Add writing about how the designers influence (through annotator selection, training, providing guidance, payment, etc.)

\subsection{Feature selection}
Considering what information the machine learning models consider as pertinent, i.e. the bodies of data that are provided to the model at optimisation time, I similarly find ample space for subjective positioning\ZTdelete{ to be embedded}.
I construct here the notion of feature selection to mean the construction of features based on theoretical insights, hypotheses about the phenomena and \ZTdelete{in addition to} the sub-selection of complete vocabularies.
Considered through the lens of abusive language detection, harmful patterns of marginalisation are apparent \ZTedit{as designers realise themselves in the features that they construct}.

\subsubsection{Manually constructed features}
A large body of work surrounding hate speech detection has investigated the question of which manual features are useful to the task of automated detection \citep{Waseem:2016,Chiril:2019,Fortuna:2018,Stankovic:2020}.
Similarly, in \autoref{chap:liwc} I explore whether rationalising over content using LIWC can have beneficial influences for machine learning approaches for abusive language detection.
Clearly, there is an interest in providing scaffolding for computational models to identify and address hate speech detection.

Through the use of higher level cognition, designers embed preconceived notions of what information computational models should deem relevant, for instance in \autoref{chap:liwc}, I consider whether higher level cognitive information about the function of language can influence modelling \ZTedit{and performance}.
The assumption is that while words may provide ample space for over-fitting models to specific instances and patterns that are do not generalise beyond the data provided, \ZTedit{other sources of information, i.e. the LIWC dictionary, may be less prone to over-fitting in such a way}.
By limiting the feature space to a much smaller discrete space of possible inputs, I argue that it is possible to achieve performance gains on out-of-domain data, relative to the input.
Another frequently used modelling assumption is that computational models can benefit from considering words in some context, generally obtained using n-gram representations of the text \citep{Waseem-Hovy:2016,Davidson:2017,Chiril:2019}.
This modelling choice represents an assumption that \ZTedit{that the context within which words appear carries significance beyond the word on its own.}\ZTdelete{not only do the individual words matter, but the context within which they appear and are embodied carries significance.}
This stands in contrast to lexicon-based methods \citep{Hurtlex:2019} that assume that the occurrence of some terms, disembodied from the local sentential context, should direct the model towards predicting either abuse or not abuse.

\subsubsection{Feature selection in neural architectures}
Many neural machine learning models are used with an assumption of simply providing the input data as it occurs, subject to replacing usernames with a user token, hashtags with a hashtag token, and so forth.
This modelling choice on the designers part relies on two strong assumptions. 
First, that neural models\ZTdelete{, by way of loss functions,} can \ZTedit{loss functions to} update the model's internal representation of the data \ZTedit{in order} to identify patterns that correlate in the input \ZTdelete{to the model} with the output labels, without \ZTedit{the} need for human cognition \ZTedit{or oversight} over the data \ZTedit{or optimisation process}.
The second assumption is that all information provided by users will, to some degree, be relevant to modelling abuse without any abstractions.

Considering the second assumption first, it stands in contrast with the use of externally computed word embeddings that such models frequently rely on \citep{Kshirsagar:2018,Isaksen:2020}.
To use pre-trained embeddings, it is necessary to align the vocabulary of the training data with the vocabulary that the previously optimised models hold.
\ZTedit{In this alignment, there is a selection of which tokens can even be considered as potentially relevant.}\ZTdelete{thus there is some selection of which tokens can be considered for relevance.}
Particularly considering hate speech on social media, where users may \ZTedit{intentionally} obscure words \ZTdelete{and misspellings are common}\ZTedit{or unintentionally misspell them} \citep{Rottger:2021}, the very words that are omitted from the model's knowledge and embodiments may in fact be the tokens that distinguish the \ZTedit{abusive} content from non-abusive.

Consider for instance the tweet posted by the American rapper Azealia Banks, an African American woman, directed towards fellow musician Zain Malik, a South Asian man, (see \autoref{fig:azealia_banks}).
While the tweet uses profane language, the text is written in African American English, making the use of the \textit{n-word} ambiguous.
Similarly, as Azealia Banks is a woman, the use of the \textit{b-word} similarly holds \ZTdelete{ambiguous meaning}\ZTedit{ambiguity}, thus on the basis of those terms \ZTedit{alone, the tweet}\ZTdelete{it} cannot unambiguously be identified as hate speech.
\ZTdelete{It}\ZTedit{However, the tweet is} clearly \ZTdelete{is} abusive and offensive, in the call for the \ZTdelete{target}\ZTedit{Zain Malik} to perform fellatio on Banks.
Only through the use of \textit{curry scented} does the tweet move unambiguously beyond \ZTedit{``merely''} being offensive to being hateful.
As `curry' and `scented' are tokens likely to exist in pre-trained word embeddings and language models, we \ZTdelete{should}\ZTedit{might} expect a model to correctly identify this tweet as abusive.
\ZTedit{However, as ``curry'' and ``scented'' are unlikely to frequently appear in context of abusive texts, the driver for a correct classification of hate speech is likely going to be the use of the \textit{n-word} and the \textit{b-word}---tokens that in this case cannot be relied on to determine abuse.}
\ZTdelete{However}\ZTedit{Moreover}, should there be attempts at obfuscating those tokens, e.g. by replacing all occurrences of the letter `e' with the number `$3$' resulting in `sc3nt3d', it is reasonably to expect that a language model and word embeddings would not have previously encountered this token. 
\ZTedit{The token would then be transformed into an unknown token, and the hateful rhetoric would then be lost, forcing a model to rely on the ambiguous tokens to make a decision.}\ZTdelete{and mapping into the embedding vocabulary would result in the token being represented by a token designated for tokens unseen at the optimisation time of the embeddings.}
On this basis, a model may incorrectly label it as simply offensive rather than hate speech.\footnote{Given the social biases against African American English in computational models, the tweet is likely to be identified as hateful in spite of the obscuring as a result of computational models disproportionately labelling African American English as hate speech \citep{Davidson:2019}.}
% In this way, the aligning the training data to the representations known in the embedding further disembodies the model from the human speaker, resulting in the model being shifted further towards the globalised end of our spectrum.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{Azealia_banks.jpeg}
  \caption{Azealia Banks tweeting to Zain Malik.}
  \label{fig:azealia_banks}
\end{figure}

Returning to the first assumption: that neural models can simply rely on input data to identify salient and relevant patterns in the data without human cognition over the relevance of inputs \ZTedit{or human oversight over the process}.
The strength of neural network models, and in fact machine learning in general, lies in the ability for models to identify patterns in data that correlate to output labels.
\ZTdelete{Thus}\ZTedit{In this way} models construct and manipulate their embodiment of the disembodied data that they are provided.
\ZTedit{Moreover, in}\ZTdelete{In} the construction and manipulation of the model's embodiment of the data lies an implication that the designers' subjectivities \ZTedit{ought to be} reflected in the model.
Specifically, through \ZTedit{the designers' decision of what data to include, models assume} \ZTdelete{deciding }that no higher order representations of the data are \ZTdelete{to be made}\ZTedit{necessary in order to adequately represent the phenomena that is being modelled.}
\ZTedit{Thus,} the designers implicitly \ZTdelete{argue,}\ZTedit{construct} and embed the normative values \ZTedit{of}\ZTdelete{that} what is relevant to abusive language detection.
\ZTedit{That is, rather than a base in theoretical or qualitative insights, model weights and probabilistic correlations are emphasised as the appropriate basis for classifications of abuse, so long as they reflect the designers' subjectivities.}
\ZTdelete{of the concept of abuse, is not human reasoning as to the concept of abuse nor the theoretical and qualitative insights that have been gathered made.}
\ZTdelete{Instead, such}\ZTedit{Such} a practice \ZTedit{then} theorises that there exist some degree of distributed language understanding that renders \ZTdelete{such}\ZTedit{human} cognition irrelevant.
\ZTdelete{Such an}\ZTedit{This} (implicit) assumption made by the designers contradicts recent studies that argue that language understanding models do not \ZTdelete{obtain the}\ZTedit{optimise to the point of having an} ability to understand language, instead they learn to parrot it \citep{Bender-Koller:2020}.

\ZTedit{Disregarding for a moment whether such models truly understand language or simply parrot it, models that only use the surface forms of tokens lay on the globalised end of the model spectrum.
The use of already-optimised language models and word embeddings in a modelling architecture shift the models slightly towards a more localised end of the spectrum, as these allow for some social context to be derived from the way in text is written.
The use of these pre-optimised technologies thus come to shift the model embodiment away from the embodiments of the users and towards the embodiments of the designers' specific subjectivities.
This shift happens as the choice of which language model and which word embeddings to use is a decision made by the designers.
The decision is made on the basis of which specific pre-optimised technology best aligns with the designers' subjective position on what constitutes abuse and how it is best modelled, i.e. which underlying dataset for these technologies best aligns with the designers' perception of the distinctive features of abuse.}
\ZTdelete{Disregarding for a moment whether such models truly learn to understand or simply parrot language, models that only use surface forms of tokens with the use of pre-trained language models or word embeddings similarly exist on on the globalised end of the model spectrum, albeit further towards the localised end of the spectrum than models that do not use representations optimised on external bodies of data, as these models shift away from the embodiments of the users their data is derived from and further towards the designers' specific positionalities.}

\subsection{Model selection}
As a number of models are optimised to identify which model best embodies the data, the designers must make normative decisions to identify what constitutes `best'.
In this way, designers \ZTdelete{reassert}\ZTedit{make a final assertion, embedding} their embodiments onto the decision on which model is selected for further use.
However, the choice of designating what constitutes `best' is often times a decision that is made prior to any model optimisation.
For abusive language detection, best refers to performance for some metrics.
For instance, \citet{Gorrell:2018} set out to have a model that has a high precision at the cost of recall.
They make this choice to ensure high confidence in their model's predictions of the positive class as their use case is comments made to politicians, where the ability to criticise without sanction is of particular importance.
\citet{Wulczyn:2016} and \citet{Kshirsagar:2018} on the other hand select their models using the Area Under the receiver operating characteristic Curve (AUC) and F1-score, respectively.
Both of these metrics for identifying what constitutes `the best performing' model give preference to models that balance classification error, such that models are attuned to false positives as well as false negatives.

Through these choices of metrics we can discern some aims of the modelling process.
Where \citet{Gorrell:2018} aim to situate their model within the context of abuse towards British Members of Parliament as it occurs on Twitter, they forego claims of universal applicability.
The best performing model, within their understanding is a model which, within the context, produces as few false positives as possible, explicitly accepting that the number of false negatives may be high.
Considering then the purpose of their modelling process, \ZTdelete{it is}\ZTedit{i.e.} to allow for embodied downstream analysis of how abuse targets a very specific group\ZTedit{, their choice affords an ability to speak to the things that are highly likely to be abuse within their construction of the phenomena.
On the other hand, their choice does not afford them the ability to speak to what is not abuse nor what their model misclassifies as not being abusive.}
\citet{Wulczyn:2016} and \citet{Kshirsagar:2018} on the other hand develop their models with the aim of obtaining a high degree of generalisation onto the test data in addition to data outside of the sample the model is trained to perform on.
Within this goal lies an assumption that there exist a `universal' and  `objective' understanding of what constitutes abuse, which is invariable to the specific embodiments of different users. 
That is, \citet{Wulczyn:2016} and \citet{Kshirsagar:2018} assume the existence of a global understanding of what \ZTedit{constitutes} abuse, in addition to an imagined average user that is disembodied from all facets of human life.

\section{Dissertation Models}
Here I consider the two model types that I have developed in \autoref{chap:liwc} and \autoref{chap:mtl}.
I document the considerations and assumptions each model type reveals throughout the machine learning pipeline.
Rather than go through the entire pipeline, I begin my analysis at the entry points, that is the choices of datasets and the modelling choices, as I only make use of previously published datasets.

\subsection{Vocabulary reduction}\label{sub:vocab_redux}
In \autoref{chap:liwc}, I optimised the machine learning models using the datasets published by \citet{Davidson:2017}, \citet{Wulczyn:2016}, \citet{Waseem:2016}, \citet{Waseem-Hovy:2016}, and \citet{Garcia:2018}.
The decision to use these datasets as optimisation data stems from these datasets originating from three distinct sources: Twitter; StormFront, a white nationalist internet forum; and Wikipedia editor discussions.
To be able to measure the generalisability of the models optimised on \citet{Davidson:2017}, \citet{Waseem:2016}, and \citet{Waseem-Hovy:2016}, I reduce the multi-class classification tasks to binary classification tasks.
Through this reduction in classes, I enforce a normative choice that the detection of abuse has greater value than the identification of the specific type of abuse.
\ZTdelete{Similarly, for all other datasets used for evaluating generalisability (i.e. \citet{Waseem:2016,Waseem-Hovy:2016,Garcia:2018}) are similarly subject to a reduction of output classes, here to align the labels learned by the model with the output labels in these external evaluation sets.}
My own experiences of hate speech and racialised abuse are at the heart of such a prioritisation, that is having been subject to such abuse I am more concerned with the ability to detect abuse than identifying which specific type of abuse it is.
Further, the modelling choice of how to represent data are also subject to my subjectivities .
While on one hand the reduction of the input space to a much smaller input space means that the size of the subsequent models, and by extension the complexity of the models.\footnote{I appreciate that even with a reduction of the model size and complexity, neural networks are still too complex to be readily understood without the aid of additional tools.}
On the other hand, through such a reduction in the vocabulary, a large majority of words will no longer be represented by the text in the models.
Here, my own beliefs that abuse detection models that rely too strongly on the occurrence of specific tokens ultimately provide an issue towards the goal of achieving models that can protect marginalised people from abuse, is at the centre of my decision.
On the other end of the vocabulary size spectrum, I use byte-pair encoded documents.
Due to the nature of generating sub-words, this increases the size of the vocabulary, in comparison to simply using the existing word.
I use sub-words and byte-pair encoding to minimise issues of out-of-vocabulary items on the basis of intentional obfuscation of words, e.g. through inserting spaces or punctuation in the middle of words \citep{Rottger:2021}.
Moreover, it is motivated by my own lived experiences belonging to a target demographics of abuse and observations I have made of other groups, where people using intentional obfuscation of words, often targets to circumvent simple content filtering techniques, i.e. `moslems' instead of `Muslims'.
While the modality I work with is text, such obfuscations also occur in the spoken word through intentional mispronunciation.

In the use of linear models as baseline models, the underlying assumption \ZTedit{that I make is} is that simple correlations of word occurrences with labels\ZTdelete{, as are found with linear models,} do not capture the complex interactions between words in text required to make qualified judgements on abuse.
This assumption too is influenced by my own positionality as a brown Muslim who grew up in a predominately white country where brown people, and in particular Muslims, are vilified for their existence.
As I am often wont to recount, growing up and watching the news Danish police bulletins for wanted persons frequently used the description `Muslim looking' to describe brown men.
Such experiences have made it clear to me that social norms surrounding the use of tokens cannot be readily understood from the words \ZTedit{using simple correlations}\ZTdelete{, or multi word expressions} without greater contextualisation.
For this reason, I use LSTMs as they can capture long interactions between words and are less directly reliant on the occurrence of individual patterns.
In addition, a number of past studies having shown the efficacy of CNNs for abuse detection \citep{Park:2017, Mitchell:2019,Kolhatkar:2020,Rizwan:2020,Safaya:2020,Gamback:2017}.
I \ZTedit{therefore use CNNs} \ZTdelete{use such models,} due to the influence of past work on the topic of hate speech and abuse detection.

The models described in \autoref{chap:liwc} that only use words or byte-pair encoded words as input rely entirely on the training data to learn patterns in data.
\ZTedit{It is thus}\ZTdelete{its} apparent that those models fall towards the very extreme of the globalised end of the \ZTedit{model} spectrum\ZTdelete{that I propose}.
On the other hand, while still towards the globalised end of the spectrum, the models that rely on LIWC as the input come are shifted more to the localised end of the spectrum, as the LIWC dictionary \ZTdelete{that is used to map from words to LIWC categories} is informed by considering data external to the data the models rely on.

%{\color{green!80!black}
%\zw{If there is time to add pre-trained embeddings + LIWC embedding layer}
%All models described in \autoref{chap:liwc} fall towards the globalised end of the spectrum, however some further to the extreme ends than others. All linear models that use words or byte-pair encoded data as input represent the extreme globalised-end of the spectrum, as these models only rely on the training data. The linear models that use LIWC categories, and those that use words/BPE in addition to LIWC categories shift slightly towards the localised end of the spectrum by virtue of the LIWC dictionary being developed using external data to inform the creation of the categories.
%As all neural network models use pre-trained word or BPE embeddings, they can be placed further towards the localised end of the spectrum. Similarly to the use of LIWC for linear models, the use of LIWC as additional input shifts the models further towards the localised end of the spectrum.
%}

The motivation for using Byte-Pair encoding is reflected in a) my own personal assumptions about the importance of textual representations and b) computational considerations.
Briefly, to minimise the number of out-of-vocabulary items, I use BPE to deconstruct the training data into smaller word-parts, that allow for deconstructing words to minimise the influence of intentional obfuscations (e.g. `mooslim women`) of tokens, as I have \ZTdelete{frequently noted}\ZTedit{observed} and been subject to abuse that seeks to obscure \ZTedit{itself}.
\ZTedit{Furthermore, the choice to BPE is likely to} better \ZTdelete{handling of}\ZTedit{handle} unknown tokens for the computational models.

\ZTedit{Although some of the} models \ZTedit{are positioned further towards the localised end of the spectrum than others, all the models used in \cref{chap:liwc} are on the globalised end of the model spectrum.
Their globalised position derives from the fact that none of the data representations take local subjective positionalities of individuals in the data into account.
Instead, all of the models rely on some abstraction away from the self through processes of disembodiment.}
\ZTdelete{find themselves further towards the localised end of the spectrum than others, all models that I use in \autoref{chap:liwc} are positioned in the globalised end of the spectrum as none of the data representations take into account the local subjective positionality of the individuals in the data, instead they all rely on some abstraction away from the subjective self through processes of disembodiment.}

\subsection{Multi-task learning}\label{sub:mtl_inchap}
%\zw{If we use current methods}
In \autoref{chap:mtl} I turn to the question of which \ZTedit{constructs would be helpful for machine learning models to embody in aims of improving performance for a given abusive language detection task.}\ZTdelete{embodiments}
\ZTedit{Specifically, I examine whether jointly learning representations of sarcasm \citep{Oraby_sarcasm:2016}, whether an argument is based in fact or feelings \citep{Oraby_factfeel:2015}, the moral sentiments elicited \citep{Hoover:2019}, and related notions of hate speech and offensive language \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Wulczyn:2016} improves classification performance.}
\ZTdelete{, in terms of social constructs such as sarcasm \citep{Oraby_sarcasm:2016}, the basis of an argument (i.e. whether it is based in feelings or facts) \citep{Oraby_factfeel:2015}, moral foundation \citep{Hoover:2019}, and conceptualisations of hate speech and offensive language \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Wulczyn:2016} would be helpful for a classification model for abuse detection in improving performance on the associated test set.}

For this task, I reuse the BPE representations of documents from \autoref{chap:liwc}.
Therefore some of the embodiments of the models, with regard to text representation remain \ZTedit{the same as described in \cref{sub:vocab_redux}}.
Here I focus on \ZTedit{the factors from \cref{chap:mtl} that are distinct from \cref{chap:liwc}.}\ZTdelete{those that change for the models described in \autoref{chap:mtl}.}
\ZTdelete{Due to the reuse of BPE representations, the analysis of those modelling choices remain the same \autoref{sub:vocab_redux}.}
\ZTdelete{However, as}\ZTedit{As} I include more datasets into consideration, I also implicitly invite the question `why those datasets'?
To answer this question, it's necessary to revisit the aims of each dataset.

One frequently identified issue with computational modelling of abuse is the issue of sarcasm \citep{Rottger:2021} and I use \ZTedit{the dataset labelled for sarcasm that was proposed by }\citet{Oraby_sarcasm:2016}.
In this choice lay two assumptions: first, that computational models for abuse detection can benefit from better understanding what constitutes \ZTdelete{abuse}\ZTedit{sarcasm}.
Second, that there does exist some line between \ZTdelete{in which} sarcasm \ZTedit{and abuse} that \ZTdelete{mimics}\ZTedit{could appear as} abuse \ZTdelete{is not also abuse}\ZTedit{but is sarcasm or is both sarcastic and abusive}.
Both assumptions are the result of years of researching online abuse, and in particular exposing myself to the abuse that occurs in online spaces.
While I may have become desensitised to abuse through the disproportionate amounts I am exposed to through my research, I frequently see that online abuse, and responses to it are expressed through humour, in particular sarcasm.\footnote{By responses I mean general reactions and responses to abuse beyond the direct responses to a perpetrator of abuse.}

The second dataset, asks the question of whether an argument is made in the basis of feeling or on the basis of facts \citep{Oraby_factfeel:2015}.
As a majority of people who perpetrate online abuse do so infrequently \citep{Waseem-Hovy:2016}, an underlying cause for being abusive may well be due to being impassioned, and thus being able to determine whether an argument is made with a basis in feelings or fact may be possible to help improve performance for abuse detection.

I also use a dataset annotated for moral foundation \citep{Hoover:2019}.
In this dataset, each document is labelled for which moral foundations it invokes in the annotators.
Moral foundations and online abuse can be thought of as concepts that are orthogonal to each other. 
\ZTedit{Moral foundations, as annotated in the dataset, provide for a higher level cognition about the content that is read, in which abusive content is likely to elicit the moral sentiments that comprise the moral foundations framework \citep{Hoover:2019}.
I therefore believe that machine learning models jointly embodying abuse and moral foundations can aid with improving performance of machine learning classifiers for abuse detection.
My own experiences of racialised abuse and observations of abuse in online spaces combined with the apparent desire of abusers to inflict harm upon their target are central to my inclusion of this task.

}
\ZTdelete{as moral foundations provide for a higher level of cognition about content that is read, I believe that using such a dataset can aid in optimising a multi-task classifier for the primary task.
Here, my own experiences of the harms that occur from being subject to racialised abuse and the apparent desire to inflict such harm from the perpetrators lead me to include this as an auxiliary task.}
Finally, I use a number of datasets for online abuse \citep{Garcia:2018,Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Wulczyn:2016}.
For this task, I do not reduce the question of detecting to a binary task, instead I use the auxiliary task datasets as a means to provide the model with more conceptualisations of abuse.
However, my subjective positioning does not change from what is detailed in \autoref{sub:vocab_redux}.

Similarly for the choices in developing the data and textual representations, my own subjective embodiments and experiences are a factor in the modelling decisions (see \cref{sub:vocab_redux}).
This is particularly true for multi-task learning, where I specifically set the weights for how much each task is to contribute to the main task through the frequency of selection.
Such a weighting relies on my own consideration of how important each task is to the overall task of identifying abuse \ZTedit{and to which degree auxiliary tasks should be afforded space to influence the model representations for abuse}.
The specific architecture of the model is influenced by its usefulness in prior work \citep{Bingel:2018} in addition to seeking an to answer the question of how more complex models would influence the performance on the task.

Using the multi-task learning framework has strong implications for where on the localisation spectrum the model is positioned\ZTdelete{can exist within}.
For instance, the use multiple different datasets to influence a single model precludes the extreme ends of the modelling spectrum.
\ZTedit{Jointly optimising multiple tasks in a shared internal model layer explicitly shifts models away from the extreme of the globalised spectrum by providing using datasets that are external to the main task dataset and by contextualising the main task with the representations obtained from the auxiliary tasks.
Similarly, the localised extreme of the model spectrum is also precluded using this method, as the auxiliary tasks do not afford embedding the subjectivities and embodiments of individuals.
On a demographic level, however, multi-task learning does hold potential for shifting towards the localised extrema, if and only if all auxiliary tasks also come from the demographic that the main task is concerned with.}
\ZTdelete{However, such an optimisation regime also precludes the extremely localised end of the spectrum for the same reason.}
\ZTedit{Moreover, as}\ZTdelete{As} each auxiliary task will work, if as nothing else, as a regulariser for the main task, \ZTedit{multi-task learning will shift the model away away from the extremes of the spectrum.}
\ZTedit{In my use of multi-task learning for abusive language detection with the auxiliary tasks that I have chosen, the models that I have developed are shifted away from the globalised extrema towards a more localised position on the model spectrum.}
\ZTdelete{As I use multi-task learning to contextualise the task of abusive language detection within the frame of tasks that have been indicated in prior literature as relevant to the task of abusive language detection, the model is both drawn to a more localised position.}
However, as I do not optimise my models on any tasks that seek to make predictions on users, and the distinct datasets do not originate from the same demographic, the model does not cross entirely into the space of localised models.
Instead the models that I produce, by virtue of learning considerations on tone, argument basis, sarcasm, and moral sentiment learn some representations of the faculties that I believe of importance to the task.
These auxiliary tasks then provide an avenue for the models to be more closely situated within the how each individual person can be represented as a function of how they express themselves.
Thus, while further towards being a localised end of the spectrum than the LIWC models, the models fall short of significantly situating the modelling of individuals within the context and lived experiences of that individual.

%{\color{green!80!black}
%\zw{If chapter with Joachim and James is used}
%In \autoref{chap:mtl} I turn to the question of which embodiments, in terms of cultural contexts of the production of abuse matters. Here, I consider the question of whether hate speech that is produced in one cultural context can map to hate speech produced in another cultural context. From this follows then that rather than building a single model or dataset that can be the arbitrator of what constitutes abuse, one can build a model that allows for translations between different cultural contexts. To this end, I explore whether, on the basis of training data alone, a multi-task learning model can learn to bridge two different cultural contexts. Here, I use two different kinds of input with two different expectations and rationales.
% \zw{Text representations: BPE, BoW}
%First, I use a Bag-of-Words (BoW) as input to my multi-task learning model and second, I use BPE encoded data. The use of BPE follows along the logic and reasoning outlined in \autoref{sub:vocab_redux}. The use of BoW as input on the other hand is motivated by the contrast: the wish to represent the data directly. Here the focus is not on obfuscation but on considering the cross-cultural means of expression. For instance the use of the \textit{n-word} with the colloquial `-ga' ending is situated differently in some of the datasets than the same word using the '-er' ending. While both words stem from the same root, they have vastly different meanings and communicate distinct messages \citep{Rahman:2012}. As such, in spite of their shared stem, it can be desirable to retain the two words as distinct tokens as a BoW approach allows for.

% \zw{Dataset choices: Davidson, Waseem \& Hovy, Waseem}
%The motivating factor behind the choice of datasets used is the knowledge that what constitutes hate speech in one cultural context may not constitute it in another. It's therefore necessary to use datasets that represent different cultural contexts, that can then be examined. Rather than using \citet{Wulczyn:2017} in addition to \citet{Davidson:2017}, I use \citet{Waseem:2016,Waseem-Hovy:2016} for two different reasons: The combination of \citet{Waseem:2016,Waseem-Hovy:2016} result in a dataset of comparable size, albeit slightly smaller, than \citet{Davidson:2017}. The second reason for choosing \citet{Waseem:2016,Waseem-Hovy:2016} over \citet{Wulczyn:2017} is that like \citet{Davidson:2017}, \citet{Waseem:2016,Waseem-Hovy:2016} is collected from Twitter, thus allowing one cultural aspect, i.e. the affordances of the platform, fixed while the content and geographic position of the users writing content vary.


% \zw{Globalised-localised model spectrum}
%Using the multi-task learning framework has strong implications for where on the localisation spectrum the models can exist within. For instance, the use multiple different datasets to influence a single model precludes the extreme ends of the modelling spectrum. However, such a training regime also precludes the extremely localised end of the spectrum for the same reason. As each auxiliary task will work, if as nothing else, as a regulariser for the main task.
%As I use multi-task learning to try to learn models that can model abuse across geo-cultural and geo-political contexts, the model shifts in its position on the model spectrum in comparison to single task models. Such a shift happens as some modelling choices draw it towards a more localised model. On the other hand other choices strongly push it towards a more globalised model, the result of which is that only a minor shift happens, in comparison to single-task models using the same data representations. The choices that pull the model towards being more localised is the use of sub-words computed using byte-pair embeddings. As such embeddings are computed outside of the dataset, they provide the data representation, and subsequently the model, with external knowledge on the composition of words. On the other hand, the use of using three datasets for abusive language detection for the main and auxiliary tasks, suggests that any given model will learn some cultural patterns of abuse, but those patterns are limited to two specific instances of abuse, rather than learning about situating the user in their own subjective embodiments. Moreover, as I do not train my models on any tasks that seek to make predictions on users, the model does not cross entirely into the space of localised models.

% \zw{Model choices: MLP}
%Similarly for the choices in developing the data and textual representations, my own subjective embodiments and experiences are a factor in the modelling decisions. This is particularly true for multi-task learning, where I specifically set the weights for how much each task is to contribute to the main task. Such a weighting relies on my own consideration of how important each task is to the overall task of identifying abuse. The specific architecture of the model is influenced by their usefulness in prior work \citep{Bingel:2018} and seeking to answer the question of how more complex models would influence the performance on the task.
%}

\section{Discussion}
Given that subjective choices and biases masquerading as disembodied `objective' positions permeate through the machine learning pipeline, the quest for objectivity and bias-free machine learning becomes redundant.
\ZTedit{This redundancy is made apparent as all choices in the machine learning development pipeline embody the subjective experiences of all people who are a part of the pipeline, from people whose data is gathered, to annotators and the designers of the pipelines.
As these experiences are embedded in the system, so slips away the illusion of `objectivity' and `neutrality' of the machine learning technologies.}
In fact, the search for objectivity in the pipeline creates a veneer of social progress that may cause further harm to already marginalised communities by obscuring and entrenching the dominance of certain bodies over others.
\ZTedit{Such harm is instituted by providing a veneer of more just, or fair, machine learning technologies that nonetheless perform institutional violences upon all who are externalised by the development process and the subjective experiences that lie at the heart of them.}
Without taking the unique embodiments of all data subjects into account, this imaginary of fair only serves as a justification of maintaining oppressive structures that are inherently harmful and reductive.

Considering the \ZTdelete{question}\ZTedit{task} of hate speech detection, developing automated tools that are applied to a general population makes inherent decisions on behalf of the user-group.
\ZTedit{The decisions made by the third party judge, i.e. content moderation technologies, embed subjective experiences of the data subjects whose data has been stripped from the context it was created in, the annotators, and the designers of the technologies.}
\ZTedit{However, as}\ZTdelete{As} such decisions are codified through the machine learning pipeline, they are presented as disembodied and objective decisions on what constitutes hate speech.
\ZTedit{In this way, machine learning technologies embed normative socio-political positions on respectability and acceptability as I argued in \cref{chap:dirt}.
These normative values come to be presented as `objective' through the disembodiments that occur in the machine learning pipeline.
However, such notions of objectivity merely provide a thin veil over the subjective embodiments of the designers and annotators.
With the vast majority of research on abusive language detection being developed for English in the global economic north \citep{Vidgen-Derczynski:2020}, the notions of respectability that are normative with respect white majoritarian countries and cultures.}
Through such codification of white perspectives on respectability \ZTedit{masqueraded as objective}, attempts to address `bias' \ZTedit{in machine learning technologies for content moderation} only serve to justify existing oppressive structures \ZTedit{by further obscuring the subjectivities embedded in the systems}.

As data permeates through the machine learning pipeline, a consideration of how data is embodied can empower the designer to answer specific questions that are embodied and mired in context.
Such considerations allow designers to interrogate the contexts within which data are created and meaning is made at each step in the dataset creation process.
It is through such recognition of context and embodiment that one can realise that as context change, so does the applicability data.
Further, only by such recognition of the deeply complex nature of embodiment and data can one hope to ask and ascertain which views the models privilege and which are subjugated.
For building content moderation systems for hate speech and abuse, the designers of machine learning pipelines can ask how their own embodiments prejudice them to selectively sanction some speech patterns.
Moreover, designers may want to ask themselves how such sanctions create downstream implications for the speech that is sanctioned.

Although there are methods with which we can move towards more localised machine learning models, what positions are given space remains a political question.
It is only through wholly representing the context and embodiments of the data creator and the datum that one can hope to arrive at sufficiently localised models.
Thus, rather than asking how to eliminate bias and subjective experiences from machine learning in the pursuit of objectivity, shifting the question to consider embodiments would ask us to reflect on the subjective experiences that are given voice.
For hate speech detection, such reflections would have designers ask which groups to base their understanding of abuse and the subsequent annotations and models that are derived from it.
Such a shift would then require us to ask and reflect upon which bodies' subjective experiences we need to account for to give voice to socially, and computationally, marginalised groups.

Only by recognising the positionality of the designers \ZTedit{and annotators} of machine learning models \ZTedit{and data} can one account for what (and whom) ones own position, and the models derived from it privilege and sanction, and the political ramifications of these.
\ZTedit{For these reasons, it is imperative that machine learning moves away from consolidating power in the designers and move towards development practices that are rooted in the participation of the people who will be subject to the models, i.e. the intended users of the models.
Participatory design practices however can quickly turn predatory if the turn to participatory design principles does not also provide for a redistribution of power \citep{Kelly_2019}.
Here, Sasha Costanza-Chock's \citeyearpar{Costanza-Chock_2018} work on design justice can provide a guide towards developing participatory design practices for machine learning systems.
\citet{Costanza-Chock_2018} argue for design practices that centre the experiences and needs of the communities for whom the design practice is taking place.
Specifically, \citet{Costanza-Chock_2018} argues that design practices should have ``sustainable, community-led and -controlled outcomes.''
By working towards such goals, machine learning research can come develop processes and technologies that specifically cater to the communities for whom we are developing our technologies.}

\ZTedit{
\section{Summary}}
\ZTedit{
  In this chapter, I have sought to examine how subjective embodiments permeate through the machine learning pipeline in efforts examine the machine learning infrastructures that underpin contemporary content moderation technologies.
  Thus, in this chapter I seek to examine \textit{RQ I} by examining how subjective embodiments are embedded into machine learning infrastructures and what the consequences of this are.
  In this chapter, I then provide a reading of machine learning against the grain by critically examining how subjective embodiments become embedded within the machine learning infrastructure.
  By performing this reading, I have used this chapter to examine the ways in which machine learning systems come to produce socially biased outcomes, such that machine learning research can move beyond the discriminatory practices that we have developed.}

  \ZTedit{
  Identifying processes for developing machine learning technologies that are not discriminatory is of particular importance to content moderation technologies, as these technologies currently produce discriminatory outputs in terms of censorship of marginalised communities (see \cref{chap:dirt} for further details).
  The work that I have performed in this chapter, identifies specific ways in which machine learning, and machine learning for content moderation encodes the subjectivities that are widely read as social biases.
  To address this concern, I propose that researches be aware of the specific ways in which they embed their subjectivities throughout the machine learning pipeline and consciously make decisions in the development process to ensure that the subjectivities that are embedded within the systems reflect the aims and the subjectivities of the people who that the content moderation systems are to be applied to.
  Specifically, I suggest that researchers are mindful of their own subjectivities and the desired outcomes of the technologies and that research engages in a genuine efforts for participatory design by collaborating with the communities that technologies are developed for.
  Moreover, I argue that the universalist notions applied in machine learning, and for the algorithmic detection of abusive language, contribute strongly to the ongoing marginalisation that machine learning systems perpetuate.
  Finally, I argue that it is only by taking steps away from such universalist notions and towards co-developing systems with communities that are community-led and community-controlled that we can hope to overcome the issues of discriminatory systems and, in the case of content moderation, systems that do not censor marginalised communities.
  systems that are community-led and -controlled that
}
%
% \section{Machine Learning as a Conservative Practice}
% \zw{Write about dominant and subjugated discourses for machine learning here; bring in that ML is a conservative practice}
%
% \zw{Citations needed: Foucault on dominant and subjugated discourses, Fraser on subaltern publics, some archival theory on marginalising effects of dominant discourses.i}

