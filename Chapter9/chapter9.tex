\ifpdf
    \graphicspath{{Chapter9/Figs/Raster/}{Chapter9/Figs/PDF/}{Chapter9/Figs/}}
\else
    \graphicspath{{Chapter9/Figs/Vector/}{Chapter9/Figs/}}
\fi

\chapter{The Politics of Toxicity in Content Moderation Infrastructures}\label{chap:filter}
\zw{In collaboration with Nanna Bonde Thylstrup (who's the first author)}

Throughout our process, we have worked with notions of ``toxicity'' and ``abuse'' without further examining their implications or the political constructs they live within. Here we then take a critical look at the implications of the political economies which these terms live within and how content moderation infrastructures define toxic content.

Through an examination of three content moderation tools, the Perspective API, Opt Out and our own abuse detection system from \autoref{chap:liwc}, we argue that content moderation's historical reliance on static categories, which are embedded in social systems of racism and patriarchy, embeds content moderation technologies in structures that risk reproducing social inequalities. We choose the three technologies to illustrate the differences between top-down and bottom-up approaches to content moderation and the distinct ways in which they engage in embed meaning to ``toxic'' and ``abuse'', and the cultural filtering work that content moderation has come to do. These three examples enable to us identify the challenges inherent in attempts to automate and scale content moderation and ask two fundamental questions of content moderation: Whom are content moderation systems for and who gets to define and enforce them?

By engaging with scholarly work that draw on and develop pollution and discard theory, we can better understand this discourse of ``toxicity'' and identify new avenues of research for content moderation studies. Moreover, by relying on theories of social pollution from anthropology \cite{Douglas:1966} and work on dirt and toxicity in the field of discard studies \cite{Libeoiron:2018,Lepawsky:2019}, we argue that content moderation should move beyond the question of merely removal of toxic content to a productive 're-ordering of our environment' through practices of classification and purification \cite{Douglas:1966}.

\section{Content Moderation as a Problem of Dirt}
If we consider content moderation technologies as `protective' filtering systems that reject and accept to ensure the `health' of communities, then we must also reckon with their inseparability from discourses of hygiene and pollution. By conceptualising content moderation systems deployed with the purpose of protecting platforms and their communities against existential threat that occurs through the through the existence of dirt \cite{Lepawsky:2019}, we can begin to develop an understanding of online abusive content as `toxic'. Thus, content moderation systems become complex processes which aid the communities, or platforms in their practice of self constitution.

By applying \citet{Douglas:1966} to content moderation technology and their classification schemes of harmful and abusive content, we find that content moderation requires constant efforts to classify, detect, and reorganise content online in every step of the process. From conceptual frameworks and annotation guidelines to computational models and from organisational systems to manual labour. Each of these elements of the content moderation system become different mechanism that allow the system to exert efforts to positively reorganise online environments through sanctioning content. In many online spaces, e.g. Facebook's familiar space of people you (have once) know(n) content moderators are hard at work, removing human rights abuses and system critiques alike. In such cases, removal is not only a negative act, but also a part of productive processes embedded in complex community formation that reconstitute the platforms they operate on.

Douglas' framework of dirt allows us to see and examine the cultural embeddings of content moderation. Considering such cultural embeddings, it is no surprise that operationalising terms such as `toxic', `hate speech', and `offensive' lead to negotiations between coders and designers of data \cite{Waseem:2016}. Such issues with operationalisation of the terms then also blur the decision boundaries learned by machine learning for content moderation. Moreover, in spite of such culture wars in the data annotation processes and their downstream effects on blurring machine-identified decision boundaries, many machine learning methods are presented with the result of the annotation process as objective truths. The machine learning methods applied to such would-be objective bodies of data codify the patterns and correlations with the associated labels. What was once subjective is then presented as objective, universally true rules, as machine learning methods play the God Trick.\vspace{5mm}

Faithful representations of collective negotiations of what constitutes `toxic' or `abusive' must then also have a degree of indeterminability to them. This indeterminability of labels is then an indication of the instability of the terms and their operationalisations. It follows then, that indeterminability can cause harm to social order \cite{Hall:1997} as multiple concurrent decoding processes may exist that deviate from the intended encoding. In the setting of content moderation, we must add an intermediary in Hall's \cite{Hall:1997} setting of encoding-decoding framework, as the content moderation system itself must decode and adjudicate a decision: Should this content be actioned or not? Through answering this question, the meaning made in the decoding process of the content moderation system then becomes the final decision on its meaning, regardless of its intended encoding or the decoding of the reader.

Understanding these meaning-making processes and positions of power allows us to recognise that some content might be flagged as problematic because of its `inability to be assimilated into existing socio-cultural categories and systems' \cite{Rafi:2015}. In handling content for which multiple concurrent and contradictory meanings are made, content moderation should be a relative practice that constantly oscillates among the meanings encoded and decoded according to the context in which the actions happens. Moreover, as cultural systems can change quickly, so can the meanings of symbols. What was once accepted practice e.g. racist jokes can suddenly be considered harmful and socially transgressive; similarly what was once taboo can become acceptable practice.
Many of the issues with content moderation systems can then be traced back to these dynamic meaning-making processes, how does one determine if the usage of the N-word is used as a slur or a `soul' word \cite{Rahman:2012}?\vspace{5mm}

These dynamic complexities stand in contradiction to automated content moderation systems that assign each token a weight internally and a probability externally. These weights and probabilities are unlikely to be zero, reinforcing the assertion made by \citet{Douglas:1966}, that there is no such thing as absolute dirt. In fact, many of the methods that seek to mitigate social biases in machine learning, and by extension automated content moderation seek to re-assign weights to minimise the social marginalisation that such systems cause on already-marginalised people \cite{Liu:2019}. These mitigation strategies frequently operate within positivist logics of optimisation. The aim of such re-ordering within automated content moderation systems is not to remove all traces of social biases within such systems, instead such works engage in a calculus of operating with minimal acceptable harms to marginalised people (see \autoref{chap:disembodied} for in-depth discussion of the impacts of such logics). However, such reordering does not take into account the unequal impacts of equal treatment (see \autoref{chap:socialscience} for a deeper discussion on the impacts of equality based systems in an unjust world). In fact, such work rarely takes into account that through their search for patterns to aid in prediction, automated systems may go beyond simply representing inequities and instead actively amplify them.\vspace{5mm}

The fact that human, machine, and hybrid content moderation systems reproduce such social inequities has been the object of both scholarly work \cite{Davidson:2019,Sap:2019,Dixon:2018,Gomes:2018} and public criticism \cite{Guynn:2019}. Indeed the excessive policing of marginalised communities has given rise to the use of POTs \cite{Kulynych:2020} in efforts to circumvent such policing through a number of tactics including phonetic spelling (e.g. the use of `wypipo' instead of `white people') \cite{Guynn:2019}. These methods of circumventing content moderation systems come from the experience of negative removals \cite{Guynn:2019}. Examples of such negative removals. Considering, for instance, content moderation of AAE, the content moderation filters may reproduce racialised logics and thus excessively reject content written in AAE as particularly dirt-like. At other times, the system may fail to capture the semantic richness of AAE. For many content moderation systems, the working assumption embedded in the systems  \cite{Davidson:2019} that is curated through labelled data \cite{Sap:2019} is that any mention of the n-word invokes a negative stereotyping \cite{Waseem:2018}. However, as \citet{Rahman:2012} reminds us, beyond the negative uses of the n-word as a slur, there is a rich and complex cultural history and meaning assigned to the word, when it used by in-group speakers. Such structural biases occur because in efforts to scale the size of data, working with `deep data' (Lori Kendall cited in \citet{Brock:2015}), i.e. working on data with methods that include deeper insights about the `cultural, moral, and social choices about technology use' found in different cultural communities \cite(Brock:2015}).

Thus, in particular for the moderation of language, many content moderation infrastructures reproduce the problems of respectability politics and its favouring of upper-middle class White ideals \cite{Kerrison:2017}.
